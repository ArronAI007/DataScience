{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"provenance":[]}},"cells":[{"cell_type":"code","source":["!wget https://mind201910small.blob.core.windows.net/release/MINDsmall_train.zip https://mind201910small.blob.core.windows.net/release/MINDsmall_dev.zip\n","!sudo apt install unzip\n","!unzip MINDsmall_train.zip -d train\n","!unzip MINDsmall_dev.zip -d test\n","!pip install torch"],"metadata":{"id":"22BxaCO_uiea"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import re\n","import json\n","import pandas as pd\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torch import nn\n","import torch.nn.functional as F\n","from ast import literal_eval\n","from tqdm import tqdm\n","import copy"],"metadata":{"id":"82CSt3jwwZFR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1) Deep Knowledge-Aware Network (DKN)\n","\n","DKN is a deep learning model which incorporates information from knowledge graph for better news recommendation. Specifically, DKN uses [TransX](https://towardsdatascience.com/introduction-to-knowledge-graph-embedding-with-dgl-ke-77ace6fb60ef) method for knowledge graph representation learning, then applies a CNN framework, named KCNN, to combine entity embedding with word embedding and generate a final embedding vector for a news article. CTR prediction is made via an attention-based neural scorer.\n","\n","<img src='https://camo.githubusercontent.com/a7331fdfe727b5012bd32f44444273ba43273f9d99f71d32f46e14c955935df4/68747470733a2f2f7265636f64617461736574732e7a32302e7765622e636f72652e77696e646f77732e6e65742f696d616765732f646b6e5f6172636869746563747572652e706e67'>\n","\n","DKN takes one piece of candidate news and one piece of a user’s clicked news as input. For each piece of news, a specially designed KCNN is used to process its title and generate an embedding vector. KCNN is an extension of traditional CNN that allows flexibility in incorporating symbolic knowledge from a knowledge graph into sentence representation learning.\n","\n","With the KCNN, we obtain a set of embedding vectors for a user’s clicked history. To get final embedding of the user with respect to the current candidate news, we use an attention-based method to automatically match the candidate news to each piece of his clicked news, and aggregate the user’s historical interests with different weights. The candidate news embedding and the user embedding are concatenated and fed into a deep neural network (DNN) to calculate the predicted probability that the user will click the candidate news."],"metadata":{"id":"1vRipHyDtYIB"}},{"cell_type":"markdown","source":["## 1.1 Pre-processing\n","\n","* **Impression** means that content was delivered to someone's feed. List of news displayed in this impression and user's click behaviors on them (1 for click and 0 for non-click). The orders of news in a impressions have been shuffled.\n","\n","* **clicked_news** is the history (ID list of clicked news) of this user before this impression. The clicked news articles are ordered by time.\n","\n","* **entity_embedding** and **relation_embedding** contain the 100-dimensional embeddings of the entities and relations learned from the subgraph (from WikiData knowledge graph) by TransE method. In both files, the first column is the ID of entity/relation, and the other columns are the embedding vector values.\n","\n","* **Entities**\n","\n","1. Label\tThe entity name in the Wikidata knwoledge graph\n","2. Type\tThe type of this entity in Wikidat\n","3. WikidataId\tThe entity ID in Wikidata\n","4. Confidence\tThe confidence of entity linking\n","5. OccurrenceOffsets\tThe character-level entity offset in the text of title or abstract\n","6. SurfaceForms\tThe raw entity names in the original text"],"metadata":{"id":"tmzDByWFv8rQ"}},{"cell_type":"code","source":["class Config():\n","    num_filters = 50\n","    window_sizes = [2, 3, 4]\n","    num_batches = 8000\n","    num_batches_batch_loss = 50  # Number of batchs to show loss\n","    num_batches_val_loss_and_acc = 300\n","    num_batches_save_checkpoint = 400\n","    batch_size = 256\n","    learning_rate = 0.001\n","    train_validation_split = (0.8, 0.2)\n","    num_workers = 4  \n","    num_clicked_news_a_user = 50  \n","    use_context = os.environ['CONTEXT'] == '1' if 'CONTEXT' in os.environ else False\n","    use_attention = os.environ['ATTENTION'] == '1' if 'ATTENTION' in os.environ else True\n","    load_checkpoint = os.environ['LOAD_CHECKPOINT'] == '1' if 'LOAD_CHECKPOINT' in os.environ else True\n","    num_words_a_news = 20\n","    entity_confidence_threshold = 0.5\n","    word_freq_threshold = 3\n","    entity_freq_threshold = 3\n","    num_word_tokens = 1 + 14760\n","    word_embedding_dim = 100\n","    entity_embedding_dim = 100\n","\n","model_config = Config()\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"Em9loHiHwx_9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### pre-process data\n","def clean_dataset(\n","    behaviors_source, behaviors_target, news_source, news_target):\n","    \n","    behaviors = pd.read_table(\n","        behaviors_source, header=None, usecols=[3, 4], names=['clicked_news', 'impressions'])\n","    \n","    behaviors.impressions = behaviors.impressions.str.split()\n","    behaviors = behaviors.explode('impressions').reset_index(drop=True)\n","    behaviors['candidate_news'], behaviors['clicked'] = behaviors.impressions.str.split('-').str\n","    behaviors.clicked_news.fillna('', inplace=True)\n","    behaviors.to_csv(\n","        behaviors_target, sep='\\t', index=False, columns=['clicked_news', 'candidate_news', 'clicked'])\n","\n","    news = pd.read_table(\n","        news_source, header=None, usecols=[0, 3, 6], names=['id', 'title', 'entities'])\n","    news.to_csv(news_target, sep='\\t', index=False)\n","    return behaviors[['clicked_news', 'candidate_news', 'clicked']], news[['id', 'title', 'entities']]\n","\n","df_behaviors, df_news = clean_dataset(\n","    './train/behaviors.tsv', \n","    './train/behaviors_cleaned.tsv',\n","    './train/news.tsv', \n","    './train/news_cleaned.tsv',\n","    )\n","\n","df_behaviors_test, df_news_test = clean_dataset(\n","    './test/behaviors.tsv', \n","    './test/behaviors_cleaned.tsv',\n","    './test/news.tsv', \n","    './test/news_cleaned.tsv',\n","    )\n","\n","pd.merge(df_behaviors, df_news, left_on='candidate_news', right_on='id').head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":372},"id":"OoLOnFID5-xL","executionInfo":{"status":"ok","timestamp":1671723954263,"user_tz":-480,"elapsed":128843,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"582b41ad-ecd2-455a-f5e2-aa325e6bfab9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-7-0daec949f978>:10: FutureWarning: Columnar iteration over characters will be deprecated in future releases.\n","  behaviors['candidate_news'], behaviors['clicked'] = behaviors.impressions.str.split('-').str\n"]},{"output_type":"execute_result","data":{"text/plain":["                                        clicked_news candidate_news clicked  \\\n","0  N55189 N42782 N34694 N45794 N18445 N63302 N104...         N55689       1   \n","1  N8419 N15771 N1431 N5888 N18663 N24123 N22130 ...         N55689       0   \n","2  N58936 N15919 N11917 N2153 N55312 N13008 N4142...         N55689       0   \n","3  N41089 N3577 N59496 N18086 N56175 N56630 N1389...         N55689       0   \n","4  N15415 N3680 N19638 N9155 N848 N16636 N1603 N1...         N55689       1   \n","\n","       id                                              title  \\\n","0  N55689  Charles Rogers, former Michigan State football...   \n","1  N55689  Charles Rogers, former Michigan State football...   \n","2  N55689  Charles Rogers, former Michigan State football...   \n","3  N55689  Charles Rogers, former Michigan State football...   \n","4  N55689  Charles Rogers, former Michigan State football...   \n","\n","                                            entities  \n","0  [{\"Label\": \"Charles Rogers (American football)...  \n","1  [{\"Label\": \"Charles Rogers (American football)...  \n","2  [{\"Label\": \"Charles Rogers (American football)...  \n","3  [{\"Label\": \"Charles Rogers (American football)...  \n","4  [{\"Label\": \"Charles Rogers (American football)...  "],"text/html":["\n","  <div id=\"df-f5f7788c-9c81-47fb-9ad5-d7dc9c676041\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>clicked_news</th>\n","      <th>candidate_news</th>\n","      <th>clicked</th>\n","      <th>id</th>\n","      <th>title</th>\n","      <th>entities</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>N55189 N42782 N34694 N45794 N18445 N63302 N104...</td>\n","      <td>N55689</td>\n","      <td>1</td>\n","      <td>N55689</td>\n","      <td>Charles Rogers, former Michigan State football...</td>\n","      <td>[{\"Label\": \"Charles Rogers (American football)...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>N8419 N15771 N1431 N5888 N18663 N24123 N22130 ...</td>\n","      <td>N55689</td>\n","      <td>0</td>\n","      <td>N55689</td>\n","      <td>Charles Rogers, former Michigan State football...</td>\n","      <td>[{\"Label\": \"Charles Rogers (American football)...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>N58936 N15919 N11917 N2153 N55312 N13008 N4142...</td>\n","      <td>N55689</td>\n","      <td>0</td>\n","      <td>N55689</td>\n","      <td>Charles Rogers, former Michigan State football...</td>\n","      <td>[{\"Label\": \"Charles Rogers (American football)...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>N41089 N3577 N59496 N18086 N56175 N56630 N1389...</td>\n","      <td>N55689</td>\n","      <td>0</td>\n","      <td>N55689</td>\n","      <td>Charles Rogers, former Michigan State football...</td>\n","      <td>[{\"Label\": \"Charles Rogers (American football)...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>N15415 N3680 N19638 N9155 N848 N16636 N1603 N1...</td>\n","      <td>N55689</td>\n","      <td>1</td>\n","      <td>N55689</td>\n","      <td>Charles Rogers, former Michigan State football...</td>\n","      <td>[{\"Label\": \"Charles Rogers (American football)...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f5f7788c-9c81-47fb-9ad5-d7dc9c676041')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f5f7788c-9c81-47fb-9ad5-d7dc9c676041 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f5f7788c-9c81-47fb-9ad5-d7dc9c676041');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["for i in df_news['entities'].sample(5).values:\n","    print(i, '\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7bwXVrOZ3j7c","executionInfo":{"status":"ok","timestamp":1671729994277,"user_tz":-480,"elapsed":384,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"fb392688-2ecb-4c5d-f9ee-520470ff2852"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["[{\"Label\": \"Hawthorn Woods, Illinois\", \"Type\": \"G\", \"WikidataId\": \"Q2288376\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [0], \"SurfaceForms\": [\"Hawthorn Woods\"]}] \n","\n","[{\"Label\": \"Audible (store)\", \"Type\": \"O\", \"WikidataId\": \"Q366651\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [46], \"SurfaceForms\": [\"Audible\"]}] \n","\n","[{\"Label\": \"Philadelphia City Council\", \"Type\": \"B\", \"WikidataId\": \"Q7182649\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [0], \"SurfaceForms\": [\"Philadelphia City Council\"]}] \n","\n","[{\"Label\": \"Washington Redskins\", \"Type\": \"O\", \"WikidataId\": \"Q212654\", \"Confidence\": 0.998, \"OccurrenceOffsets\": [0], \"SurfaceForms\": [\"Redskins\"]}] \n","\n","[{\"Label\": \"Queens\", \"Type\": \"G\", \"WikidataId\": \"Q18424\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [58], \"SurfaceForms\": [\"Queens\"]}] \n","\n"]}]},{"cell_type":"code","source":["### balance negative and positive data\n","def balance(source, target, true_false_division_range):\n","    low = true_false_division_range[0]\n","    high = true_false_division_range[1]\n","\n","    original = pd.read_table(source)\n","    true_part = original[original['clicked'] == 1]\n","    false_part = original[original['clicked'] == 0]\n","\n","    if len(true_part) / len(false_part) < low:\n","        print(f'Drop {len(false_part) - int(len(true_part) / low)} from false part')\n","        false_part = false_part.sample(n=int(len(true_part) / low))\n","\n","    elif len(true_part) / len(false_part) > high:\n","        print(f'Drop {len(true_part) - int(len(false_part) * high)} from true part')\n","        true_part = true_part.sample(n=int(len(false_part) * high))\n","\n","    balanced = pd.concat([true_part, false_part]).sample(frac=1).reset_index(drop=True)\n","    balanced.to_csv(target, sep='\\t', index=False)\n","    return balanced\n","\n","df_balance = balance(\n","    './train/behaviors_cleaned.tsv',\n","    './train/behaviors_cleaned_balanced.tsv',\n","    [1 / 2, 2],\n",")\n","\n","df_balance.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":223},"id":"yTzfvW4G6cz5","executionInfo":{"status":"ok","timestamp":1671723975795,"user_tz":-480,"elapsed":21550,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"1d74c7da-e81e-4fd9-fdff-5ac7d2a4693b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drop 5134412 from false part\n"]},{"output_type":"execute_result","data":{"text/plain":["                                        clicked_news candidate_news  clicked\n","0  N26168 N45395 N27402 N55846 N27927 N3046 N6228...          N4663        0\n","1  N51238 N13995 N61277 N36030 N36312 N40704 N465...         N40381        0\n","2  N10779 N61864 N14538 N44018 N14939 N22570 N558...         N47652        1\n","3  N60615 N28533 N15185 N31686 N12393 N30145 N298...         N38869        1\n","4  N8748 N23505 N61864 N11071 N29177 N16695 N1809...         N24180        1"],"text/html":["\n","  <div id=\"df-0ab94a96-bdd4-46bc-bc58-601bbb706cd9\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>clicked_news</th>\n","      <th>candidate_news</th>\n","      <th>clicked</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>N26168 N45395 N27402 N55846 N27927 N3046 N6228...</td>\n","      <td>N4663</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>N51238 N13995 N61277 N36030 N36312 N40704 N465...</td>\n","      <td>N40381</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>N10779 N61864 N14538 N44018 N14939 N22570 N558...</td>\n","      <td>N47652</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>N60615 N28533 N15185 N31686 N12393 N30145 N298...</td>\n","      <td>N38869</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>N8748 N23505 N61864 N11071 N29177 N16695 N1809...</td>\n","      <td>N24180</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0ab94a96-bdd4-46bc-bc58-601bbb706cd9')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-0ab94a96-bdd4-46bc-bc58-601bbb706cd9 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-0ab94a96-bdd4-46bc-bc58-601bbb706cd9');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["def parse_news(source, target, word2int_path, entity2int_path, mode):\n","\n","    def clean_text(text):\n","        return re.sub(r'[^a-zA-Z ]', '', text).lower().strip()\n","\n","    if mode == 'train':\n","        word2int, word2freq, entity2int, entity2freq = [{} for _ in range(4)]\n","\n","        news = pd.read_table(source)\n","        news.entities.fillna('[]', inplace=True)\n","        parsed_news = pd.DataFrame(columns=['id', 'title', 'entities'])\n","\n","        with tqdm(total=len(news), desc=\"Counting words and entities\") as pbar:\n","            for row in news.itertuples(index=False):\n","                for w in clean_text(row.title).split():\n","                    if w not in word2freq:\n","                        word2freq[w] = 1\n","                    else:\n","                        word2freq[w] += 1\n","\n","                for e in json.loads(row.entities):\n","                    # Count occurrence time within title\n","                    occur_lis = list(filter(lambda x: x < len(row.title), e['OccurrenceOffsets']))\n","                    times = len(occur_lis) * e['Confidence']\n","\n","                    if times > 0:\n","                        if e['WikidataId'] not in entity2freq:\n","                            entity2freq[e['WikidataId']] = times\n","                        else:\n","                            entity2freq[e['WikidataId']] += times\n","                pbar.update(1)\n","\n","        for k, v in word2freq.items():\n","            if v >= Config.word_freq_threshold:\n","                word2int[k] = len(word2int) + 1\n","\n","        for k, v in entity2freq.items():\n","            if v >= Config.entity_freq_threshold:\n","                entity2int[k] = len(entity2int) + 1\n","\n","        with tqdm(total=len(news), desc=\"Parsing words and entities\") as pbar:\n","            for row in news.itertuples(index=False):\n","                new_row = [\n","                    row.id, [0] * Config.num_words_a_news,\n","                    [0] * Config.num_words_a_news\n","                ]\n","\n","                # Calculate local entity map (map lower single word to entity)\n","                local_entity_map = {}\n","                for e in json.loads(row.entities):\n","                    if e['Confidence'] > Config.entity_confidence_threshold and e[\n","                            'WikidataId'] in entity2int:\n","                        for x in ' '.join(e['SurfaceForms']).lower().split():\n","                            local_entity_map[x] = entity2int[e['WikidataId']]\n","                try:\n","                    for i, w in enumerate(clean_text(row.title).split()):\n","                        if w in word2int:\n","                            new_row[1][i] = word2int[w]\n","                            if w in local_entity_map:\n","                                new_row[2][i] = local_entity_map[w]\n","                except IndexError:\n","                    pass\n","                parsed_news.loc[len(parsed_news)] = new_row\n","\n","                pbar.update(1)\n","\n","        parsed_news.to_csv(target, sep='\\t', index=False)\n","\n","        word2int = pd.DataFrame(word2int.items(), columns=['word','int'])\n","        word2int.to_csv(word2int_path, sep='\\t', index=False)\n","\n","        entity2int = pd.DataFrame(entity2int.items(), columns=['entity', 'int'])\n","        entity2int.to_csv(entity2int_path, sep='\\t', index=False)\n","        return parsed_news, word2int, entity2int\n","\n","    elif mode == 'test':\n","        news = pd.read_table(source)\n","        news.entities.fillna('[]', inplace=True)\n","        parsed_news = pd.DataFrame(columns=['id', 'title', 'entities'])\n","\n","        word2int = dict(pd.read_table(word2int_path).values.tolist())\n","        entity2int = dict(pd.read_table(entity2int_path).values.tolist())\n","\n","        word_total = 0\n","        word_missed = 0\n","\n","        with tqdm(total=len(news), desc=\"Parsing words and entities\") as pbar:\n","            for row in news.itertuples(index=False):\n","                new_row = [\n","                    row.id, [0] * Config.num_words_a_news,\n","                    [0] * Config.num_words_a_news\n","                ]\n","\n","                # Calculate local entity map (map lower single word to entity)\n","                local_entity_map = {}\n","                for e in json.loads(row.entities):\n","                    if e['Confidence'] > Config.entity_confidence_threshold and e[\n","                            'WikidataId'] in entity2int:\n","                        for x in ' '.join(e['SurfaceForms']).lower().split():\n","                            local_entity_map[x] = entity2int[e['WikidataId']]\n","                try:\n","                    for i, w in enumerate(clean_text(row.title).split()):\n","                        word_total += 1\n","                        if w in word2int:\n","                            new_row[1][i] = word2int[w]\n","                            if w in local_entity_map:\n","                                new_row[2][i] = local_entity_map[w]\n","                        else:\n","                            word_missed += 1\n","                except IndexError:\n","                    pass\n","\n","                parsed_news.loc[len(parsed_news)] = new_row\n","                pbar.update(1)\n","        print(f'Out-of-Vocabulary rate: {word_missed/word_total:.4f}')\n","        parsed_news.to_csv(target, sep='\\t', index=False)\n","        return parsed_news\n","\n","    else:\n","        print('Wrong mode!')\n","        return 0\n","\n","df_parsed_news, df_word2int, df_entity2int = parse_news(\n","    './train/news_cleaned.tsv',\n","    './train/news_with_entity.tsv',\n","    './train/word2int.tsv',\n","    './train/entity2int.tsv',\n","    mode='train'\n",")\n","\n","df_parsed_news_test = parse_news(\n","    './test/news_cleaned.tsv',\n","    './test/news_with_entity.tsv',\n","    './train/word2int.tsv',\n","    './train/entity2int.tsv',\n","    mode='test'\n",")\n","\n","print(df_entity2int.head())\n","print(df_word2int.head())\n","df_parsed_news.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":484},"id":"37Da54Lm7tyD","executionInfo":{"status":"ok","timestamp":1671724688487,"user_tz":-480,"elapsed":712697,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"9c96b393-e794-4e1f-8226-106727bc17bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Counting words and entities: 100%|██████████| 51282/51282 [00:01<00:00, 48194.92it/s]\n","Parsing words and entities: 100%|██████████| 51282/51282 [06:47<00:00, 125.94it/s]\n","Parsing words and entities: 100%|██████████| 42416/42416 [05:02<00:00, 140.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Out-of-Vocabulary rate: 0.0506\n","     entity  int\n","0    Q43274    1\n","1     Q9682    2\n","2   Q193583    3\n","3  Q1215884    4\n","4    Q49233    5\n","        word  int\n","0        the    1\n","1     brands    2\n","2      queen    3\n","3  elizabeth    4\n","4     prince    5\n"]},{"output_type":"execute_result","data":{"text/plain":["       id                                              title  \\\n","0  N55528  [1, 2, 3, 4, 5, 6, 7, 5, 8, 9, 10, 0, 0, 0, 0,...   \n","1  N19639  [11, 12, 13, 14, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n","2  N61837  [1, 16, 17, 18, 19, 20, 21, 1, 22, 17, 23, 24,...   \n","3  N53526  [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 3...   \n","4  N38324  [31, 37, 38, 39, 17, 40, 41, 42, 37, 43, 44, 0...   \n","\n","                                            entities  \n","0  [0, 0, 2, 2, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n","1  [0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "],"text/html":["\n","  <div id=\"df-a3702f0b-463c-4b77-9a04-b9d6def2d0b7\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>title</th>\n","      <th>entities</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>N55528</td>\n","      <td>[1, 2, 3, 4, 5, 6, 7, 5, 8, 9, 10, 0, 0, 0, 0,...</td>\n","      <td>[0, 0, 2, 2, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>N19639</td>\n","      <td>[11, 12, 13, 14, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n","      <td>[0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>N61837</td>\n","      <td>[1, 16, 17, 18, 19, 20, 21, 1, 22, 17, 23, 24,...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>N53526</td>\n","      <td>[25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 3...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>N38324</td>\n","      <td>[31, 37, 38, 39, 17, 40, 41, 42, 37, 43, 44, 0...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a3702f0b-463c-4b77-9a04-b9d6def2d0b7')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a3702f0b-463c-4b77-9a04-b9d6def2d0b7 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a3702f0b-463c-4b77-9a04-b9d6def2d0b7');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["def transform_entity_embedding(source, target, entity2int_path):\n","\n","    entity_embedding = pd.read_table(source, header=None)\n","    entity_embedding['vector'] = entity_embedding.iloc[:, 1:101].values.tolist()\n","    entity_embedding = entity_embedding[[0, 'vector']].rename(columns={0: \"entity\"})\n","    entity2int = pd.read_table(entity2int_path)\n","    merged_df = pd.merge(entity_embedding, entity2int, on='entity').sort_values('int')\n","    entity_embedding_transformed = np.zeros(\n","        (len(entity2int) + 1, Config.entity_embedding_dim))\n","    for row in merged_df.itertuples(index=False):\n","        entity_embedding_transformed[row.int] = row.vector\n","    np.save(target, entity_embedding_transformed)\n","    return entity_embedding_transformed\n","\n","entity_transformed = transform_entity_embedding(\n","    \"./train/entity_embedding.vec\",\n","    \"./train/entity_embedding.npy\",\n","    \"./train/entity2int.tsv\",\n",")\n","\n","print(entity_transformed.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oZCvgwKTAnQH","executionInfo":{"status":"ok","timestamp":1671724689721,"user_tz":-480,"elapsed":1249,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"d1951f0a-101f-4df4-88c2-81b48e5ad990"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(3072, 100)\n"]}]},{"cell_type":"code","source":["def transform2json(source, target):\n","    behaviors = pd.read_table(\n","        source, header=None, names=['uid', 'time', 'clicked_news', 'impression'])\n","    \n","    f = open(target, \"w\")\n","    with tqdm(total=len(behaviors), desc=\"Transforming tsv to json\") as pbar:\n","        for row in behaviors.itertuples(index=False):\n","            item = {}\n","            item['uid'] = row.uid[1:]\n","            item['time'] = row.time\n","            item['impression'] = {\n","                x.split('-')[0][1:]: int(x.split('-')[1])\n","                for x in row.impression.split()\n","            }\n","            f.write(json.dumps(item) + '\\n')\n","            pbar.update(1)\n","    f.close()\n","\n","transform2json('./test/behaviors.tsv', './test/truth.json')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"61sJgdAhAnGR","executionInfo":{"status":"ok","timestamp":1671724694329,"user_tz":-480,"elapsed":4610,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"f3087c92-2c7e-48cf-ba6f-89922290d07e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Transforming tsv to json: 100%|██████████| 73152/73152 [00:03<00:00, 18766.81it/s]\n"]}]},{"cell_type":"markdown","source":["## 1.2 DataLoader"],"metadata":{"id":"h00lhKCCB86X"}},{"cell_type":"code","source":["class DKNDataset(Dataset):\n","    def __init__(self, behaviors_path, news_with_entity_path, config):\n","\n","        super(Dataset, self).__init__()\n","        self.behaviors = pd.read_table(behaviors_path)\n","        self.behaviors['clicked_news'].fillna('', inplace=True)\n","        self.news_with_entity = pd.read_table(\n","            news_with_entity_path,\n","            index_col='id',\n","            converters={'title': literal_eval,'entities': literal_eval}\n","            )\n","        self.config = config\n","\n","    def __len__(self):\n","        return len(self.behaviors)\n","\n","    def __getitem__(self, idx):\n","\n","        def news2dict(news, df):\n","            if news in df.index:\n","                news_dict = {\"word\": df.loc[news].title, \"entity\": df.loc[news].entities}  \n","            else: \n","                # if cant find the ID in entity list\n","                news_dict = {\n","                    \"word\": [0] * Config.num_words_a_news, \n","                    \"entity\": [0] * Config.num_words_a_news}\n","            return news_dict\n","\n","        item = {}\n","        row = self.behaviors.iloc[idx]\n","        item[\"clicked\"] = row['clicked']\n","        item[\"candidate_news\"] = news2dict(row['candidate_news'], self.news_with_entity)\n","\n","        item[\"clicked_news\"] = [\n","            news2dict(x, self.news_with_entity)\n","            for x in row['clicked_news'].split()[:self.config.num_clicked_news_a_user]\n","        ]\n","\n","        # if item[\"clicked_news\"] dont have num_clicked_news_a_user history\n","        padding = {\n","            \"word\": [0] * self.config.num_words_a_news,\n","            \"entity\": [0] * self.config.num_words_a_news\n","        }\n","\n","        repeated_times = self.config.num_clicked_news_a_user - len(item[\"clicked_news\"])\n","        item[\"clicked_news\"].extend([padding] * repeated_times)\n","        return item\n","\n","### testing\n","dataset = DKNDataset(\n","    './train/behaviors_cleaned.tsv',\n","    './train/news_with_entity.tsv',\n","    model_config\n",")\n","\n","## train test split\n","train_size = int(model_config.train_validation_split[0] / sum(Config.train_validation_split) * len(dataset))\n","validation_size = len(dataset) - train_size\n","train_dataset, val_dataset = random_split(dataset, (train_size, validation_size))\n","\n","train_dataloader = DataLoader(\n","    train_dataset, batch_size=model_config.batch_size, shuffle=True,\n","    num_workers=model_config.num_workers, drop_last=True)\n","\n","val_dataloader = DataLoader(\n","    val_dataset, batch_size=model_config.batch_size, shuffle=False,\n","    num_workers=model_config.num_workers, drop_last=False)\n","\n","sample_dataset = next(iter(train_dataset))\n","entity_embedding = np.load('./train/entity_embedding.npy')\n","context_embedding = np.load('./train/entity_embedding.npy')\n","\n","print(entity_embedding.shape)\n","print(context_embedding.shape)\n","print(sample_dataset['clicked'])\n","print(sample_dataset['candidate_news'])\n","print(sample_dataset['clicked_news'][0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wyl2fh7mtZxq","executionInfo":{"status":"ok","timestamp":1671724713291,"user_tz":-480,"elapsed":18977,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"72f2a572-ddbe-440e-dd52-da660e9ccecd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(3072, 100)\n","(3072, 100)\n","0\n","{'word': [768, 4468, 37, 2723, 1079, 146, 9561, 12829, 53, 1989, 4644, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'entity': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n","{'word': [43, 1519, 260, 136, 1040, 37, 2478, 299, 1060, 719, 1637, 120, 2882, 2757, 11743, 0, 0, 0, 0, 0], 'entity': [0, 172, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}]},{"cell_type":"markdown","source":["## 1.3 Model Construction"],"metadata":{"id":"Epk4lwhQXnLw"}},{"cell_type":"code","source":["class Attention(torch.nn.Module):\n","    \"\"\"\n","    Attention Net\n","    Input embedding vectors (produced by KCNN) of a candidate news and all of user's clicked news,\n","    produce final user embedding vectors with respect to the candidate news.\n","    \"\"\"\n","\n","    def __init__(self, config):\n","        super(Attention, self).__init__()\n","        self.config = config\n","        self.dnn = nn.Sequential(\n","            nn.Linear(len(self.config.window_sizes) * 2 * self.config.num_filters, 16), \n","            nn.Linear(16, 1)\n","        )\n","\n","    def forward(self, candidate_news_vector, clicked_news_vector):\n","        \"\"\"\n","        Args:\n","          candidate_news_vector: batch_size, len(window_sizes) * num_filters\n","          clicked_news_vector: num_clicked_news_a_user, batch_size, len(window_sizes) * num_filters\n","        Returns:\n","          user_vector: batch_size, len(window_sizes) * num_filters\n","        \"\"\"\n","\n","        # [num_clicked_news_a_user, m, len(window_sizes) * num_filters]\n","        candidate_expanded = candidate_news_vector.expand(\n","            self.config.num_clicked_news_a_user, -1, -1)\n","        \n","        # [num_clicked_news_a_user, m, 1]\n","        clicked_news_weights = self.dnn(\n","            torch.cat((clicked_news_vector, candidate_expanded), dim=-1)\n","            )\n","\n","        # [m, num_clicked_news_a_user]\n","        clicked_news_weights = F.softmax(\n","            clicked_news_weights.squeeze(-1).transpose(0, 1),\n","            dim=1)        \n","\n","        # [m, num_clicked_news_a_user, 1] x [m, num_clicked_news_a_user, len(window_sizes) * num_filters]\n","        # [batch_size, len(window_sizes) * num_filters]\n","        user_vector = torch.bmm(\n","            clicked_news_weights.unsqueeze(1), clicked_news_vector.transpose(0, 1)).squeeze(1)\n","        return user_vector\n","\n","class KCNN(torch.nn.Module):\n","    \"\"\"\n","    Knowledge-aware CNN (KCNN) based on Kim CNN.\n","    Input a news sentence (e.g. its title), produce its embedding vector.\n","    \"\"\"\n","\n","    def __init__(self, config, entity_embedding, context_embedding):\n","        \n","        super(KCNN, self).__init__()\n","        self.config = config\n","        self.word_embedding = nn.Embedding(\n","            config.num_word_tokens, config.word_embedding_dim)\n","        \n","        self.entity_embedding = entity_embedding\n","        self.context_embedding = context_embedding\n","        self.transform_matrix = nn.Parameter(\n","            torch.empty(self.config.word_embedding_dim, self.config.entity_embedding_dim)\n","            )\n","        self.transform_bias = nn.Parameter(\n","            torch.empty(self.config.word_embedding_dim)\n","            )\n","\n","        self.conv_filters = nn.ModuleDict({\n","            str(x): nn.Conv2d(\n","                3 if self.config.use_context else 2,\n","                self.config.num_filters,\n","                (x, self.config.word_embedding_dim),\n","                ) for x in self.config.window_sizes\n","        })\n","\n","        self.transform_matrix.data.uniform_(-0.1, 0.1)\n","        self.transform_bias.data.uniform_(-0.1, 0.1)\n","\n","    def forward(self, news):\n","        \"\"\"\n","        Args:\n","          news:\n","            {\n","                \"word\": [Tensor(batch_size) * num_words_a_news],\n","                \"entity\":[Tensor(batch_size) * num_words_a_news]\n","            }\n","        Returns:\n","          final_vector: batch_size, len(window_sizes) * num_filters\n","        \"\"\"\n","\n","        # stack news[\"word\"](list) to be torch tensor\n","        # [m, num_words_a_news] --> [m, num_words_a_news, word_embedding_dim]\n","        word_vector = self.word_embedding(torch.stack(news[\"word\"], dim=1).to(device))\n","\n","        # entity_embedding.shape = [3072, 100]\n","        # [m, num_words_a_news, 100]\n","        entity_vector = F.embedding(\n","            torch.stack(news[\"entity\"], dim=1),\n","            torch.from_numpy(self.entity_embedding)).float().to(device)\n","\n","        if self.config.use_context:\n","            # [m, num_words_a_news, 100]\n","            context_vector = F.embedding(\n","                torch.stack(news[\"entity\"], dim=1),\n","                torch.from_numpy(self.context_embedding)).float().to(device)\n","\n","        # The abbreviations are the same as those in paper\n","        b = self.config.batch_size\n","        n = self.config.num_words_a_news\n","        d = self.config.word_embedding_dim\n","        k = self.config.entity_embedding_dim\n","        \n","        # transform_matrix [word_dim, entity_dim] --> (b * n, word_embedding_dim, entity_embedding_dim)\n","        transformed_entity_vector = torch.bmm(\n","            self.transform_matrix.expand(b * n, -1, -1), # (5120, 100, 100)\n","            entity_vector.view(b * n, k, 1) # (5120, 100, 1)\n","            )\n","        \n","        # [m, num_words_a_news, word_embedding_dim]\n","        transformed_entity_vector = torch.tanh(\n","            torch.add(\n","                transformed_entity_vector.view(b, n, d),\n","                self.transform_bias.expand(b, n, -1)))\n","\n","        if self.config.use_context:\n","            # [m, num_words_a_news, word_embedding_dim]\n","            transformed_context_vector = torch.tanh(\n","                torch.add(\n","                    torch.bmm(self.transform_matrix.expand(b * n, -1, -1),\n","                              context_vector.view(b * n, k, 1)).view(b, n, d),\n","                    self.transform_bias.expand(b, n, -1)))\n","\n","        if self.config.use_context:\n","            # [m, 3, num_words_a_news, word_embedding_dim]\n","            multi_channel_vector = torch.stack(\n","                [word_vector, transformed_entity_vector, transformed_context_vector], dim=1)\n","            \n","        else:\n","            # [m, 2, num_words_a_news, word_embedding_dim]\n","            multi_channel_vector = torch.stack(\n","                [word_vector, transformed_entity_vector], dim=1)\n","\n","        pooled_vectors = []\n","        for x in self.config.window_sizes:\n","            # [m, num_filters, num_words_a_news + 1 - x] \n","            convoluted = self.conv_filters[str(x)](multi_channel_vector).squeeze(dim=3)\n","            activated = F.relu(convoluted)\n","\n","            # [m, num_filters]\n","            # pooled = activated.max(dim=-1)[0]\n","            pooled = F.max_pool1d(activated, activated.size(2)).squeeze(dim=2)            \n","            pooled_vectors.append(pooled)\n","\n","        # [m, len(window_sizes) * num_filters]\n","        final_vector = torch.cat(pooled_vectors, dim=1)\n","        return final_vector\n","\n","class DKN(torch.nn.Module):\n","    \"\"\"\n","    Deep knowledge-aware network.\n","    Input a candidate news and a list of user clicked news, produce the click probability.\n","    \"\"\"\n","\n","    def __init__(self, config, entity_embedding, context_embedding):\n","        super(DKN, self).__init__()\n","        self.config = config\n","        self.kcnn = KCNN(config, entity_embedding, context_embedding)\n","\n","        if self.config.use_attention:\n","            self.attention = Attention(config)\n","\n","        self.dnn = nn.Sequential(\n","            nn.Linear(\n","                len(self.config.window_sizes) * 2 * self.config.num_filters, 16), \n","                nn.Linear(16, 1)\n","                )\n","\n","    def forward(self, candidate_news, clicked_news):\n","        \"\"\"\n","        Args:\n","          candidate_news:\n","            {\n","                \"word\": [Tensor(batch_size) * num_words_a_news],\n","                \"entity\":[Tensor(batch_size) * num_words_a_news]\n","            }\n","          clicked_news:\n","            [\n","                {\n","                    \"word\": [Tensor(batch_size) * num_words_a_news],\n","                    \"entity\":[Tensor(batch_size) * num_words_a_news]\n","                } * num_clicked_news_a_user\n","            ]\n","        Returns:\n","          click_probability: batch_size\n","        \"\"\"\n","        # [m, len(window_sizes) * num_filters]\n","        candidate_news_vector = self.kcnn(candidate_news)\n","\n","        # [num_clicked_news_a_user, m, len(window_sizes) * num_filters]\n","        clicked_news_vector = torch.stack([self.kcnn(x) for x in clicked_news])\n","\n","        # [m, len(window_sizes) * num_filters]\n","        if self.config.use_attention:\n","            user_vector = self.attention(candidate_news_vector, clicked_news_vector)\n","        else:\n","            user_vector = clicked_news_vector.mean(dim=0)\n","\n","        # Sigmoid is done with BCEWithLogitsLoss\n","        # batch_size\n","        click_probability = self.dnn(\n","            torch.cat((user_vector, candidate_news_vector), dim=1)).squeeze(dim=1)\n","        return click_probability\n","\n","### testing\n","# sample_dataset = next(iter(train_dataloader))\n","dkn = DKN(model_config, entity_embedding, context_embedding).to(device)\n","criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([23.7]).float().to(device))\n","optimizer = torch.optim.Adam(dkn.parameters(), lr=Config.learning_rate)\n","\n","y_pred = dkn(\n","    sample_dataset[\"candidate_news\"], sample_dataset[\"clicked_news\"])\n","y = sample_dataset[\"clicked\"].float().to(device)\n","loss = criterion(y_pred, y)\n","# optimizer.zero_grad()\n","# loss.backward()\n","# optimizer.step()\n","\n","print(y_pred.size())\n","print(loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TO0_QNnGWVhO","executionInfo":{"status":"ok","timestamp":1671728680439,"user_tz":-480,"elapsed":4675,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"dfff12b3-fbb0-4755-fffd-6fa6a97b4738"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([256])\n","tensor(1.4333, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"]}]},{"cell_type":"markdown","source":["## 1.4 Evaluation"],"metadata":{"id":"VPL51dPhaZHN"}},{"cell_type":"code","source":["### evaluate\n","from sklearn.metrics import roc_auc_score\n","\n","def mrr_score(y_true, y_score):\n","    order = np.argsort(y_score)[::-1]\n","    y_true = np.take(y_true, order)\n","    rr_score = y_true / (np.arange(len(y_true)) + 1)\n","    return np.sum(rr_score) / np.sum(y_true)\n","\n","def dcg_score(y_true, y_score, k=10):\n","    order = np.argsort(y_score)[::-1]\n","    y_true = np.take(y_true, order[:k])\n","    gains = 2 ** y_true - 1\n","    discounts = np.log2(np.arange(len(y_true)) + 2)\n","    return np.sum(gains / discounts)\n","\n","def ndcg_score(y_true, y_score, k=10):\n","    best = dcg_score(y_true, y_true, k)\n","    actual = dcg_score(y_true, y_score, k)\n","    return actual / best\n","\n","loss_full, aucs, mrrs, ndcg5s, ndcg10s = [[] for _ in range(5)]\n","\n","with tqdm(total=len(val_dataloader), desc=\"Checking loss and accuracy\") as pbar:\n","\n","    for minibatch in val_dataloader:\n","\n","        y_pred = dkn(minibatch[\"candidate_news\"], minibatch[\"clicked_news\"])\n","        y = minibatch[\"clicked\"].float().to(device)\n","        loss = criterion(y_pred, y)\n","        loss_full.append(loss.item())\n","        y_pred_list = y_pred.tolist()\n","        y_list = y.tolist()\n","\n","        auc = roc_auc_score(y_list, y_pred_list)\n","        mrr = mrr_score(y_list, y_pred_list)\n","        ndcg5 = ndcg_score(y_list, y_pred_list, 5)\n","        ndcg10 = ndcg_score(y_list, y_pred_list, 10)\n","\n","        aucs.append(auc)\n","        mrrs.append(mrr)\n","        ndcg5s.append(ndcg5)\n","        ndcg10s.append(ndcg10)\n","        pbar.update(1)\n","        # break\n","\n","print(\n","    f\"loss_full: {np.mean(loss_full)}\",\n","    f\"aucs: {np.mean(aucs)}\",\n","    f\"mrrs: {np.mean(mrrs)}\",\n","    f\"ndcg5s: {np.mean(ndcg5s)}\",\n","    f\"ndcg10s: {np.mean(ndcg10s)}\",\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oagnfIkHeJF6","executionInfo":{"status":"ok","timestamp":1671729327903,"user_tz":-480,"elapsed":30727,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"35693fa2-1aed-44c0-9f54-dd88e0dabfd4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\rChecking loss and accuracy:   0%|          | 0/4566 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Checking loss and accuracy:   0%|          | 1/4566 [00:31<39:35:36, 31.22s/it]"]},{"output_type":"stream","name":"stdout","text":["loss_full: 1.4227112531661987 aucs: 0.6055327868852459 mrrs: 0.016584527987922888 ndcg5s: 0.0 ndcg10s: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["### inference\n","y_pred = []\n","y = []\n","count = 0\n","\n","with tqdm(total=len(val_dataloader), desc=\"Inferering\") as pbar:\n","    for minibatch in val_dataloader:\n","        y_pred.extend(dkn(minibatch[\"candidate_news\"], minibatch[\"clicked_news\"]).tolist())\n","        y.extend(minibatch[\"clicked\"].float().tolist())\n","        pbar.update(1)\n","        count += 1\n","        if count == 500:\n","            break\n","\n","y_pred = iter(y_pred)\n","y = iter(y)\n","\n","truth_file = open('./test/truth.json', 'r')\n","submission_answer_file = open('./data/test/answer.json', 'w')\n","for line in truth_file.readlines():\n","    user_truth = json.loads(line)\n","    user_inference = copy.deepcopy(user_truth)\n","    for k in user_truth['impression'].keys():\n","        assert next(y) == user_truth['impression'][k]\n","        user_inference['impression'][k] = next(y_pred)\n","    submission_answer_file.write(json.dumps(user_inference) + '\\n')"],"metadata":{"id":"TGg6nc7_dwX6"},"execution_count":null,"outputs":[]}]}