{"cells":[{"cell_type":"markdown","source":["Refer [Vision Transformer from Scratch](https://medium.com/mlearning-ai/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c#:~:text=Save-,Vision%20Transformers%20from%20Scratch%20(PyTorch)%3A%20A%20step%2Dby%2D,in%20other%20tasks%20as%20well.)\n","\n","<img src=\"https://github.com/BrianPulfer/PapersReimplementations/blob/master/vit/architecture.png?raw=true\">"],"metadata":{"id":"oFv5BOWkNqBN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UsQw4t7ZWobd"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","from tqdm import tqdm\n","import torch\n","import torch.nn as nn\n","from torch.nn import CrossEntropyLoss\n","from torch.optim import Adam\n","from torch.utils.data import DataLoader\n","from torchvision.datasets.mnist import MNIST\n","from torchvision.transforms import ToTensor\n","\n","np.random.seed(0)\n","torch.manual_seed(0)\n","\n","\n","def patchify(images, n_patches):\n","    n, c, h, w = images.shape\n","\n","    assert h == w, \"Patchify method is implemented for square images only\"\n","\n","    patches = torch.zeros(n, n_patches ** 2, h * w // n_patches ** 2)\n","    patch_size = h // n_patches\n","\n","    for idx, image in enumerate(images):\n","        for i in range(h // n_patches):\n","            for j in range(w // n_patches):\n","                patch = image[:, i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size]\n","                patches[idx, i * n_patches + j] = patch.flatten()\n","    return patches\n","\n","\n","class MyViT(nn.Module):\n","    def __init__(self, input_shape, n_patches=7, hidden_d=8, n_heads=2, out_d=10, device=None):\n","        # Super constructor\n","        super(MyViT, self).__init__()\n","        self.device = device\n","\n","        # Input and patches sizes\n","        self.input_shape = input_shape\n","        self.n_patches = n_patches\n","        self.n_heads = n_heads\n","        assert input_shape[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n","        assert input_shape[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n","        self.patch_size = (input_shape[1] / n_patches, input_shape[2] / n_patches)\n","        self.hidden_d = hidden_d\n","\n","        # 1) Linear mapper\n","        self.input_d = int(input_shape[0] * self.patch_size[0] * self.patch_size[1])\n","        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n","\n","        # 2) Classification token\n","        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n","\n","        # 3) Positional embedding\n","        # (In forward method)\n","\n","        # 4a) Layer normalization 1\n","        self.ln1 = nn.LayerNorm((self.n_patches ** 2 + 1, self.hidden_d))\n","\n","        # 4b) Multi-head Self Attention (MSA) and classification token\n","        self.msa = MyMSA(self.hidden_d, n_heads)\n","\n","        # 5a) Layer normalization 2\n","        self.ln2 = nn.LayerNorm((self.n_patches ** 2 + 1, self.hidden_d))\n","\n","        # 5b) Encoder MLP\n","        self.enc_mlp = nn.Sequential(\n","            nn.Linear(self.hidden_d, self.hidden_d),\n","            nn.ReLU()\n","        )\n","\n","        # 6) Classification MLP\n","        self.mlp = nn.Sequential(\n","            nn.Linear(self.hidden_d, out_d),\n","            nn.Softmax(dim=-1)\n","        )\n","\n","    def forward(self, images):\n","        # Dividing images into patches\n","        n, c, w, h = images.shape\n","        patches = patchify(images, self.n_patches)\n","\n","        # Running linear layer for tokenization\n","        tokens = self.linear_mapper(patches)\n","\n","        # Adding classification token to the tokens\n","        tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])\n","\n","        # Adding positional embedding\n","        tokens += get_positional_embeddings(self.n_patches ** 2 + 1, self.hidden_d).repeat(n, 1, 1).to(self.device)\n","\n","        # TRANSFORMER ENCODER BEGINS ###################################\n","        # NOTICE: MULTIPLE ENCODER BLOCKS CAN BE STACKED TOGETHER ######\n","        # Running Layer Normalization, MSA and residual connection\n","        out = tokens + self.msa(self.ln1(tokens))\n","\n","        # Running Layer Normalization, MLP and residual connection\n","        out = out + self.enc_mlp(self.ln2(out))\n","        # TRANSFORMER ENCODER ENDS   ###################################\n","\n","        # Getting the classification token only\n","        out = out[:, 0]\n","\n","        return self.mlp(out)\n","\n","\n","class MyMSA(nn.Module):\n","    def __init__(self, d, n_heads=2):\n","        super(MyMSA, self).__init__()\n","        self.d = d\n","        self.n_heads = n_heads\n","\n","        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n","\n","        d_head = int(d / n_heads)\n","        self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n","        self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n","        self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n","        self.d_head = d_head\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, sequences):\n","        # Sequences has shape (N, seq_length, token_dim)\n","        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n","        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n","        result = []\n","        for sequence in sequences:\n","            seq_result = []\n","            for head in range(self.n_heads):\n","                q_mapping = self.q_mappings[head]\n","                k_mapping = self.k_mappings[head]\n","                v_mapping = self.v_mappings[head]\n","\n","                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n","                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n","\n","                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n","                seq_result.append(attention @ v)\n","            result.append(torch.hstack(seq_result))\n","        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])\n","\n","\n","def get_positional_embeddings(sequence_length, d):\n","    result = torch.ones(sequence_length, d)\n","    for i in range(sequence_length):\n","        for j in range(d):\n","            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n","    return result\n","\n","\n","def main():\n","    # Loading data\n","    transform = ToTensor()\n","\n","    train_set = MNIST(root='./../datasets', train=True, download=True, transform=transform)\n","    test_set = MNIST(root='./../datasets', train=False, download=True, transform=transform)\n","\n","    train_loader = DataLoader(train_set, shuffle=True, batch_size=16)\n","    test_loader = DataLoader(test_set, shuffle=False, batch_size=16)\n","\n","    # Defining model and training options\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model = MyViT((1, 28, 28), n_patches=7, hidden_d=20, n_heads=2, out_d=10, device=device).to(device)\n","    N_EPOCHS = 5\n","    LR = 0.01\n","\n","    # Training loop\n","    optimizer = Adam(model.parameters(), lr=LR)\n","    criterion = CrossEntropyLoss()\n","    for epoch in tqdm(range(N_EPOCHS), desc=\"Training\"):\n","        train_loss = 0.0\n","        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\", leave=False):\n","            x, y = batch\n","            x, y = x.to(device), y.to(device)\n","            y_hat = model(x)\n","            loss = criterion(y_hat, y) / len(x)\n","\n","            train_loss += loss.detach().cpu().item()\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}\")\n","\n","    # Test loop\n","    correct, total = 0, 0\n","    test_loss = 0.0\n","    for batch in tqdm(test_loader, desc=\"Testing\"):\n","        x, y = batch\n","        x, y = x.to(device), y.to(device)\n","        y_hat = model(x)\n","        loss = criterion(y_hat, y) / len(x)\n","        test_loss += loss.detach().cpu().item()\n","\n","        correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n","        total += len(x)\n","    print(f\"Test loss: {test_loss:.2f}\")\n","    print(f\"Test accuracy: {correct / total * 100:.2f}%\")\n","\n","\n","if __name__ == '__main__':\n","    main()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"name":"Vision Transformers.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}