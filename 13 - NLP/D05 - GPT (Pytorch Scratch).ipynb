{"cells":[{"cell_type":"code","source":["pip install transformers"],"metadata":{"id":"LXO8RiR8Nznq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import json\n","import regex as re\n","import requests\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from torch.utils.data import Dataset\n","from torch.utils.data.dataloader import DataLoader\n","import pickle\n","import math\n","import time\n","from collections import defaultdict"],"metadata":{"id":"xMbzP_33BrcT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 0) Generative Pre-trained Transformer (GPT)\n","\n","GPT model only has decoder block.\n","While encoder block only generate the output with similar length as input,\n","decoder is generative in nature.\n","\n","\n","GPT-2 does not require the encoder part of the transformer architecture because the model uses a masked self-attention that can only look at prior tokens. The encoder is not needed because the model does not need to learn the representation of the input sequence.\n","\n","\n","It produces estimates for the probability of the next word as outputs but it is auto-regressive as each token in the sentence has the context of the previous words. Thus GPT-2 works one token at a time.\n","\n","\n","BERT, by contrast, is not auto-regressive. It uses the entire surrounding context all-at-once. GPT-2 the context vector is zero-initialized for the first word embedding.\n","**"],"metadata":{"id":"BejQteEwrCz8"}},{"cell_type":"markdown","source":["# 1) Tokenization (Byte Pair Encoding)\n","\n","The GPT-2 and RoBERTa tokenizers (which are pretty similar) have a clever way to deal with this: they donâ€™t look at words as being written with Unicode characters, but with bytes. This way the base vocabulary has a small size (256), but every character you can think of will still be included and not end up being converted to the unknown token. This trick is called byte-level BPE.\n","\n","Description: https://huggingface.co/course/chapter6/5?fw=pt\n","\n","Code: https://github.com/karpathy/minGPT/blob/master/mingpt/bpe.py"],"metadata":{"id":"lCwRiti8wjLY"}},{"cell_type":"markdown","source":["# 2) Dataset"],"metadata":{"id":"SRzhU2ZQCYdk"}},{"cell_type":"code","source":["class SortDataset(Dataset):\n","    \"\"\" \n","    Dataset for the Sort problem. E.g. for problem length 6:\n","    Input: 0 0 2 1 0 1 -> Output: 0 0 0 1 1 2\n","    \n","    Since GPT does not have encoder block, the input and output is concentenate for teacher forcing\n","    input:  0 0 2 1 0 1 0 0 0 1 1\n","    output: -1 -1 -1 -1 -1 0 0 0 1 1 2\n","    where -1 is \"ignore\", as the transformer is reading the input sequence\n","    \"\"\"\n","\n","    def __init__(self, split, length=6, num_digits=3):\n","        assert split in {'train', 'test'}\n","        self.split = split\n","        self.length = length\n","        self.num_digits = num_digits\n","    \n","    def __len__(self):\n","        return 10000\n","    \n","    def get_vocab_size(self):\n","        return self.num_digits\n","    \n","    def get_block_size(self):\n","        # the length of the sequence that will feed into transformer, \n","        # containing concatenated input and the output, \n","        # -1 because the transformer starts making predictions at the last input element\n","        return self.length * 2 - 1\n","\n","    def __getitem__(self, idx):\n","        \n","        # use rejection sampling to generate an input example from the desired split\n","        while True:\n","            # generate some random integers\n","            inp = torch.randint(self.num_digits, size=(self.length,), dtype=torch.long)\n","            # half of the time let's try to boost the number of examples that \n","            # have a large number of repeats, as this is what the model seems to struggle\n","            # with later in training, and they are kind of rate\n","            if torch.rand(1).item() < 0.5:\n","                if inp.unique().nelement() > self.length // 2:\n","                    # too many unqiue digits, re-sample\n","                    continue\n","            # figure out if this generated example is train or test based on its hash\n","            h = hash(pickle.dumps(inp.tolist()))\n","            inp_split = 'test' if h % 4 == 0 else 'train' # designate 25% of examples as test\n","            if inp_split == self.split:\n","                break\n","        \n","        # solve the task: i.e. sort\n","        sol = torch.sort(inp)[0]\n","\n","        # concatenate the problem specification and the solution\n","        cat = torch.cat((inp, sol), dim=0)\n","\n","        # the inputs to the transformer will be the offset sequence\n","        x = cat[:-1].clone()\n","        y = cat[1:].clone()\n","        # we only want to predict at output locations, mask out the loss at the input locations\n","        y[:self.length-1] = -1\n","        return x, y\n","\n","class GPTConfig:\n","    def __init__(self, vocab_size, block_size, **kwargs):\n","        self.vocab_size = vocab_size\n","        self.block_size = block_size\n","        for key, value in kwargs.items():\n","            setattr(self, key, value)\n","\n","class GPT1Config(GPTConfig):\n","\n","    # model\n","    n_layer = 3\n","    n_head = 3\n","    n_embd = 48\n","    embd_pdrop = 0.1\n","    resid_pdrop = 0.1\n","    attn_pdrop = 0.1\n","\n","    # data\n","    device = 'auto'\n","    num_workers = 4\n","\n","    # optimizer parameters\n","    max_iters = 2000\n","    batch_size = 64\n","    learning_rate = 5e-4\n","    betas = (0.9, 0.95)\n","    weight_decay = 0.1\n","    grad_norm_clip = 1.0"],"metadata":{"id":"mxZcWIE1CXvW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = SortDataset('train')\n","test_dataset = SortDataset('test')\n","\n","x, y = train_dataset[0]\n","print(\"x:\", x)\n","print(\"y:\", y)\n","print(f'\\nPrediction Sequence:')\n","for a, b in zip(x, y):\n","    print(int(a), int(b))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sN25wNYIDZ9H","executionInfo":{"status":"ok","timestamp":1668520206910,"user_tz":-480,"elapsed":6,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"71c237e8-d4fe-4bf8-9202-ce82882380b6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x: tensor([1, 0, 1, 1, 2, 2, 0, 1, 1, 1, 2])\n","y: tensor([-1, -1, -1, -1, -1,  0,  1,  1,  1,  2,  2])\n","\n","Prediction Sequence:\n","1 -1\n","0 -1\n","1 -1\n","1 -1\n","2 -1\n","2 0\n","0 1\n","1 1\n","1 1\n","1 2\n","2 2\n"]}]},{"cell_type":"code","source":["# config\n","vocab_size = train_dataset.get_vocab_size()\n","block_size = train_dataset.get_block_size()\n","config = GPT1Config(vocab_size=vocab_size, block_size=block_size)"],"metadata":{"id":"L5rKDpNMIRPp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3) Modeling"],"metadata":{"id":"PKwwp8HewuOO"}},{"cell_type":"code","source":["class NewGELU(nn.Module):\n","    \"\"\"\n","    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n","    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n","    \"\"\"\n","    def forward(self, x):\n","        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n","\n","\n","class CausalSelfAttention(nn.Module):\n","    \"\"\"\n","    A vanilla multi-head masked self-attention layer with a projection at the end.\n","    It's important in decoder block to have diagonal mask\n","    It is also possible to use torch.nn.MultiheadAttention.\n","    \"\"\"\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        assert config.n_embd % config.n_head == 0\n","        # key, query, value projections for all heads, but in a batch\n","        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n","        # output projection\n","        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n","        # regularization\n","        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n","        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n","\n","        # causal mask to ensure that attention is only applied to the left in the input sequence\n","        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size))\n","                                     .view(1, 1, config.block_size, config.block_size))\n","        self.n_head = config.n_head\n","        self.n_embd = config.n_embd\n","\n","    def forward(self, x):\n","        # batch_size, seq_len, emb_dim\n","        B, T, C = x.size() \n","\n","        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n","        # (b, seq_len, emb_dim) --> (b, seq_len, emb_dim * 3) --> (b, seq_len, emb_dim)\n","        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n","        \n","        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (b, h, seq_len, d_k)\n","        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (b, h, seq_len, d_k)\n","        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (b, h, seq_len, d_k)\n","\n","        # (b, h, seq_len, d_k) matmul (b, h, d_k, seq_len) --> (b, h, seq_len, seq_len)\n","        att = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(k.size(-1))\n","\n","        # diagonal mask\n","        # fill 0 mask with super small number so it wont affect the softmax weight\n","        # (batch_size, h, seq_len, seq_len)\n","\n","        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n","        att = F.softmax(att, dim=-1)\n","        att = self.attn_dropout(att)\n","\n","        # (b, h, seq_len, seq_len) matmul (b, h, seq_len, d_k) --> (b, h, seq_len, d_k)\n","        y = att @ v \n","\n","        # (b, h, seq_len, d_k) --> (b, seq_len, h, d_k) --> (b, seq_len, d_model)\n","        y = y.transpose(1, 2).contiguous().view(B, T, C)\n","\n","        # output projection\n","        y = self.resid_dropout(self.c_proj(y))\n","        return y\n","\n","\n","class Block(nn.Module):\n","    \"\"\" GPT only contain decode block\n","    \"\"\"\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.ln_1 = nn.LayerNorm(config.n_embd)\n","        self.attn = CausalSelfAttention(config)\n","        self.ln_2 = nn.LayerNorm(config.n_embd)\n","        self.mlp = nn.ModuleDict(dict(\n","            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n","            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n","            act     = NewGELU(),\n","            dropout = nn.Dropout(config.resid_pdrop),\n","        ))\n","        m = self.mlp\n","        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x)))) # MLP forward\n","\n","    def forward(self, x):\n","        \n","        # (batch_size, seq_len, emb_dim)\n","        x = x + self.attn(self.ln_1(x))\n","        x = x + self.mlpf(self.ln_2(x))\n","        return x\n","\n","### testing\n","wte = nn.Embedding(config.vocab_size, config.n_embd)\n","block = Block(config)\n","\n","# sample dataset from data loader\n","train_loader = DataLoader(train_dataset, batch_size=1, num_workers=0, drop_last=False)\n","sample_data = next(iter(train_loader))\n","x, y = sample_data\n","\n","tok_emb = wte(x)\n","print('Token Embedding Size:', tok_emb.size())\n","\n","block_out = block(tok_emb)\n","print('Block Output Size:', block_out.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-gy9OcsmMXfD","executionInfo":{"status":"ok","timestamp":1668521734654,"user_tz":-480,"elapsed":1237,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"33a20201-b802-4c07-d69f-5ff1e56acf82"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Token Embedding Size: torch.Size([1, 11, 48])\n","Block Output Size: torch.Size([1, 11, 48])\n"]}]},{"cell_type":"code","source":["class GPT(nn.Module):\n","    \"\"\" GPT Language Model \"\"\"\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.block_size = config.block_size\n","\n","        self.transformer = nn.ModuleDict(dict(\n","            wte = nn.Embedding(config.vocab_size, config.n_embd),\n","            wpe = nn.Embedding(config.block_size, config.n_embd),\n","            drop = nn.Dropout(config.embd_pdrop),\n","            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n","            ln_f = nn.LayerNorm(config.n_embd),\n","        ))\n","        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n","\n","        # init all weights, and apply a speci al scaled init to the residual projections, per GPT-2 paper\n","        self.apply(self._init_weights)\n","        for pn, p in self.named_parameters():\n","            if pn.endswith('c_proj.weight'):\n","                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n","\n","        # report number of parameters (note we don't count the decoder parameters in lm_head)\n","        n_params = sum(p.numel() for p in self.transformer.parameters())\n","        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, nn.Embedding):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","        elif isinstance(module, nn.LayerNorm):\n","            torch.nn.init.zeros_(module.bias)\n","            torch.nn.init.ones_(module.weight)\n","\n","    def configure_optimizers(self, train_config):\n","\n","        # separate out all parameters to those that will and won't experience regularizing weight decay\n","        decay = set()\n","        no_decay = set()\n","        whitelist_weight_modules = (torch.nn.Linear, )\n","        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n","        for mn, m in self.named_modules():\n","            for pn, p in m.named_parameters():\n","                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n","                # random note: because named_modules and named_parameters are recursive\n","                # we will see the same tensors p many many times. but doing it this way\n","                # allows us to know which parent module any tensor p belongs to...\n","                if pn.endswith('bias'):\n","                    # all biases will not be decayed\n","                    no_decay.add(fpn)\n","                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n","                    # weights of whitelist modules will be weight decayed\n","                    decay.add(fpn)\n","                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n","                    # weights of blacklist modules will NOT be weight decayed\n","                    no_decay.add(fpn)\n","\n","        # validate that we considered every parameter\n","        param_dict = {pn: p for pn, p in self.named_parameters()}\n","        inter_params = decay & no_decay\n","        union_params = decay | no_decay\n","        \n","        # create the pytorch optimizer object\n","        optim_groups = [\n","            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n","            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n","        ]\n","        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n","        return optimizer\n","\n","    def forward(self, idx, targets=None):\n","        device = idx.device\n","        b, t = idx.size()\n","        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n","\n","        # positional token, shape (1, t)\n","        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) \n","\n","        # forward the GPT model itself\n","        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n","        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n","        x = self.transformer.drop(tok_emb + pos_emb)\n","        for block in self.transformer.h:\n","            x = block(x)\n","        \n","        x = self.transformer.ln_f(x)\n","        # (b, t, n_embd) -- > # (b, t, vocab_size)\n","        logits = self.lm_head(x)\n","\n","        # if we are given some desired targets also calculate the loss\n","        # -1 at output will be ignored\n","        loss = None\n","        if targets is not None:\n","            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n","        return logits, loss\n","\n","    @torch.no_grad()\n","    def generate(self, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n","        \"\"\"\n","        Take a conditioning sequence of indices idx (LongTensor of shape (b, t)) and complete\n","        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n","        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n","        \"\"\"\n","        for _ in range(max_new_tokens):\n","            # if the sequence context is growing too long we must crop it at block_size\n","            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n","            # forward the model to get the logits for the index in the sequence\n","            logits, _ = self(idx_cond)\n","            # pluck the logits at the final step and scale by desired temperature\n","            logits = logits[:, -1, :] / temperature\n","            # optionally crop the logits to only the top k options\n","            if top_k is not None:\n","                v, _ = torch.topk(logits, top_k)\n","                logits[logits < v[:, [-1]]] = -float('Inf')\n","            # apply softmax to convert logits to (normalized) probabilities\n","            probs = F.softmax(logits, dim=-1)\n","            # either sample from the distribution or take the most likely element\n","            if do_sample:\n","                idx_next = torch.multinomial(probs, num_samples=1)\n","            else:\n","                _, idx_next = torch.topk(probs, k=1, dim=-1)\n","            # append sampled index to the running sequence and continue\n","            idx = torch.cat((idx, idx_next), dim=1)\n","\n","        return idx\n","\n","\n","### testing\n","wte = nn.Embedding(config.vocab_size, config.n_embd)\n","model = GPT(config)\n","\n","# sample dataset from data loader\n","train_loader = DataLoader(train_dataset, batch_size=64, num_workers=0, drop_last=False)\n","sample_data = next(iter(train_loader))\n","x, y = sample_data\n","logits, loss = model.forward(x, y)\n","print('logits: ', logits.size())\n","print('loss: ', loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"13LrwU6NR9bx","executionInfo":{"status":"ok","timestamp":1668520716855,"user_tz":-480,"elapsed":6,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"3ab2414e-9da4-4979-e8b5-b9d142c577a1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["number of parameters: 0.09M\n","torch.Size([64, 11, 48])\n","torch.Size([64, 11, 48])\n","logits:  torch.Size([64, 11, 3])\n","loss:  tensor(1.1490, grad_fn=<NllLossBackward0>)\n"]}]},{"cell_type":"code","source":["class Trainer:\n","\n","    def __init__(self, config, model, train_dataset):\n","        self.config = config\n","        self.model = model\n","        self.optimizer = None\n","        self.train_dataset = train_dataset\n","        self.callbacks = defaultdict(list)\n","\n","        # determine the device we'll train on\n","        if config.device == 'auto':\n","            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","        else:\n","            self.device = config.device\n","        self.model = self.model.to(self.device)\n","        print(\"running on device\", self.device)\n","\n","        # variables that will be assigned to trainer class later for logging and etc\n","        self.iter_num = 0\n","        self.iter_time = 0.0\n","        self.iter_dt = 0.0\n","\n","    def add_callback(self, onevent: str, callback):\n","        self.callbacks[onevent].append(callback)\n","\n","    def set_callback(self, onevent: str, callback):\n","        self.callbacks[onevent] = [callback]\n","\n","    def trigger_callbacks(self, onevent: str):\n","        for callback in self.callbacks.get(onevent, []):\n","            callback(self)\n","\n","    def run(self):\n","        model, config = self.model, self.config\n","\n","        # setup the optimizer\n","        self.optimizer = model.configure_optimizers(config)\n","\n","        # setup the dataloader\n","        train_loader = DataLoader(\n","            self.train_dataset,\n","            sampler=torch.utils.data.RandomSampler(self.train_dataset, replacement=True, num_samples=int(1e10)),\n","            shuffle=False,\n","            pin_memory=True,\n","            batch_size=config.batch_size,\n","            num_workers=config.num_workers,\n","        )\n","\n","        model.train()\n","        self.iter_num = 0\n","        self.iter_time = time.time()\n","        data_iter = iter(train_loader)\n","        while True:\n","\n","            # fetch the next batch (x, y) and re-init iterator if needed\n","            try:\n","                batch = next(data_iter)\n","            except StopIteration:\n","                data_iter = iter(train_loader)\n","                batch = next(data_iter)\n","            batch = [t.to(self.device) for t in batch]\n","            x, y = batch\n","\n","            # forward the model\n","            logits, self.loss = model(x, y)\n","\n","            # backprop and update the parameters\n","            model.zero_grad(set_to_none=True)\n","            self.loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n","            self.optimizer.step()\n","\n","            self.trigger_callbacks('on_batch_end')\n","            self.iter_num += 1\n","            tnow = time.time()\n","            self.iter_dt = tnow - self.iter_time\n","            self.iter_time = tnow\n","\n","            # termination conditions\n","            if config.max_iters is not None and self.iter_num >= config.max_iters:\n","                break\n","\n","model = GPT(config)\n","trainer = Trainer(config, model, train_dataset)\n","trainer = Trainer(config, model, train_dataset)\n","\n","def batch_end_callback(trainer):\n","    if trainer.iter_num % 100 == 0:\n","        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n","trainer.set_callback('on_batch_end', batch_end_callback)\n","trainer.run()"],"metadata":{"id":"YqK1wjjLOhkW"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}