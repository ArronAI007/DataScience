{"cells":[{"cell_type":"code","source":["pip install transformers tiktoken"],"metadata":{"id":"LXO8RiR8Nznq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import json\n","import regex as re\n","import requests\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from torch.utils.data import Dataset\n","from torch.utils.data.dataloader import DataLoader\n","import pickle\n","import math\n","import time\n","from collections import defaultdict\n","import tiktoken"],"metadata":{"id":"xMbzP_33BrcT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 0) Generative Pre-trained Transformer (GPT)\n","\n","GPT model only has decoder block.\n","While encoder block only generate the output with similar length as input,\n","decoder is generative in nature.\n","\n","\n","GPT-2 does not require the encoder part of the transformer architecture because the model uses a masked self-attention that can only look at prior tokens. The encoder is not needed because the model does not need to learn the representation of the input sequence.\n","\n","\n","It produces estimates for the probability of the next word as outputs but it is auto-regressive as each token in the sentence has the context of the previous words. Thus GPT-2 works one token at a time.\n","\n","\n","BERT, by contrast, is not auto-regressive. It uses the entire surrounding context all-at-once. GPT-2 the context vector is zero-initialized for the first word embedding.\n","**"],"metadata":{"id":"BejQteEwrCz8"}},{"cell_type":"markdown","source":["# 1) Tokenization (Byte Pair Encoding)\n","\n","The GPT-2 and RoBERTa tokenizers (which are pretty similar) have a clever way to deal with this: they donâ€™t look at words as being written with Unicode characters, but with bytes. This way the base vocabulary has a small size (256), but every character you can think of will still be included and not end up being converted to the unknown token. This trick is called byte-level BPE.\n","\n","Description: https://huggingface.co/course/chapter6/5?fw=pt\n","\n","Code: https://github.com/karpathy/minGPT/blob/master/mingpt/bpe.py"],"metadata":{"id":"lCwRiti8wjLY"}},{"cell_type":"markdown","source":["# 2) Dataset"],"metadata":{"id":"SRzhU2ZQCYdk"}},{"cell_type":"code","source":["# download the tiny shakespeare dataset\n","data_dir = os.path.join('data', 'tinyshakespeare')\n","input_file_path = os.path.join(data_dir, 'input.txt')\n","if not os.path.exists(input_file_path):\n","    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n","    os.makedirs(data_dir)\n","    with open(input_file_path, 'w') as f:\n","        f.write(requests.get(data_url).text)\n","\n","with open(input_file_path, 'r') as f:\n","    data = f.read()\n","n = len(data)\n","train_data = data[:int(n*0.9)]\n","val_data = data[int(n*0.9):]\n","\n","# encode with tiktoken gpt2 bpe\n","enc = tiktoken.get_encoding(\"gpt2\")\n","train_ids = enc.encode_ordinary(train_data)\n","val_ids = enc.encode_ordinary(val_data)\n","print(f\"train has {len(train_ids):,} tokens\")\n","print(f\"val has {len(val_ids):,} tokens\")\n","\n","# export to bin files\n","train_ids = np.array(train_ids, dtype=np.uint16)\n","val_ids = np.array(val_ids, dtype=np.uint16)\n","train_ids.tofile(os.path.join(data_dir, 'train.bin'))\n","val_ids.tofile(os.path.join(data_dir, 'val.bin'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gFsw898QYE58","executionInfo":{"status":"ok","timestamp":1682129525711,"user_tz":-480,"elapsed":3939,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"6cfabfe6-a59f-4903-a696-ad19c63db762"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train has 301,966 tokens\n","val has 36,059 tokens\n"]}]},{"cell_type":"code","source":["class GPTConfig:\n","    def __init__(self, vocab_size, **kwargs):\n","        self.vocab_size = vocab_size\n","        for key, value in kwargs.items():\n","            setattr(self, key, value)\n","\n","class CustomConfig(GPTConfig):\n","    # model\n","    n_layer = 8\n","    n_head = 8\n","    n_embd = 256\n","    embd_pdrop = 0.1\n","    resid_pdrop = 0.1\n","    attn_pdrop = 0.1\n","    dropout = 0.1\n","    compile = True\n","\n","    # data\n","    device = 'cuda'\n","    num_workers = 0\n","\n","    # optimizer parameters\n","    max_iters = 2e4\n","    batch_size = 4\n","    block_size = 64\n","    learning_rate = 6e-4\n","    betas = (0.9, 0.95)\n","    weight_decay = 1e-1\n","    grad_norm_clip = 1.0\n","\n","# config\n","vocab_size = len(train_ids)\n","config = CustomConfig(vocab_size=vocab_size)"],"metadata":{"id":"2iyOZwp6hfjF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# read data from .bin\n","data_dir = os.path.join('data', 'tinyshakespeare')\n","train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n","val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n","\n","class ShakespeareDataset(Dataset):\n","    def __init__(self, split, block_size=128, device_type='cuda'):\n","        assert split in {'train', 'test'}\n","        self.split = split\n","        self.block_size = block_size\n","        self.device_type = device_type\n","        self.data = train_data if split == 'train' else val_data\n","\n","    def __len__(self):\n","        return len(self.data) - self.block_size\n","\n","    def __getitem__(self, idx):\n","        # ix = torch.randint(len(data) - block_size, (batch_size,))\n","        # x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n","        # y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n","        x = torch.from_numpy(self.data[idx : idx + self.block_size].astype(np.int64))\n","        y = torch.from_numpy(self.data[idx + 1 : idx + 1 + self.block_size].astype(np.int64))\n","\n","        if self.device_type == 'cuda':\n","            # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n","            x, y = x.pin_memory().to('cuda', non_blocking=True), y.pin_memory().to('cuda', non_blocking=True)\n","        else:\n","            x, y = x.to('cpu'), y.to('cpu')\n","        return x, y\n","\n","train_dataset = ShakespeareDataset('train', config.block_size, config.device)\n","train_loader = DataLoader(train_dataset, batch_size=config.batch_size, num_workers=config.num_workers, drop_last=False)\n","test_dataset = ShakespeareDataset('test', config.block_size, config.device)\n","test_loader = DataLoader(test_dataset, batch_size=config.batch_size, num_workers=config.num_workers, drop_last=False)\n","sample_data = next(iter(train_loader))\n","x, y = sample_data\n","print(\"x:\", x.size())\n","print(\"y:\", y.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J3LHo61xbFAb","executionInfo":{"status":"ok","timestamp":1682129533312,"user_tz":-480,"elapsed":7606,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"e4337de5-a364-421f-86a2-cc460362650b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x: torch.Size([4, 64])\n","y: torch.Size([4, 64])\n"]}]},{"cell_type":"markdown","source":["# 3) Modeling"],"metadata":{"id":"PKwwp8HewuOO"}},{"cell_type":"code","source":["class NewGELU(nn.Module):\n","    \"\"\"\n","    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n","    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n","    \"\"\"\n","    def forward(self, x):\n","        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n","\n","\n","class CausalSelfAttention(nn.Module):\n","    \"\"\"\n","    A vanilla multi-head masked self-attention layer with a projection at the end.\n","    It's important in decoder block to have diagonal mask\n","    It is also possible to use torch.nn.MultiheadAttention.\n","    \"\"\"\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        assert config.n_embd % config.n_head == 0\n","        # key, query, value projections for all heads, but in a batch\n","        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n","        # output projection\n","        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n","        # regularization\n","        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n","        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n","        self.dropout = config.dropout\n","        self.n_head = config.n_head\n","        self.n_embd = config.n_embd\n","\n","        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n","        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n","        if not self.flash:\n","            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n","            # causal mask to ensure that attention is only applied to the left in the input sequence\n","            self.register_buffer(\n","                \"mask\",\n","                torch.tril(torch.ones(config.block_size, config.block_size)\n","            ).view(1, 1, config.block_size, config.block_size))\n","\n","    def forward(self, x):\n","        # batch_size, seq_len, emb_dim\n","        B, T, C = x.size()\n","\n","        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n","        # (b, seq_len, emb_dim) --> (b, seq_len, emb_dim * 3) --> (b, seq_len, emb_dim)\n","        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n","\n","        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (b, h, seq_len, d_k)\n","        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (b, h, seq_len, d_k)\n","        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (b, h, seq_len, d_k)\n","\n","        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n","        if self.flash:\n","            # efficient attention using Flash Attention CUDA kernels\n","            y = torch.nn.functional.scaled_dot_product_attention(\n","                q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True\n","            )\n","        else:\n","            # (b, h, seq_len, d_k) matmul (b, h, d_k, seq_len) --> (b, h, seq_len, seq_len)\n","            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n","            # diagonal mask\n","            # fill 0 mask with super small number so it wont affect the softmax weight\n","            # (batch_size, h, seq_len, seq_len)\n","            att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n","            att = F.softmax(att, dim=-1)\n","            att = self.attn_dropout(att)\n","\n","            # (b, h, seq_len, seq_len) matmul (b, h, seq_len, d_k) --> (b, h, seq_len, d_k)\n","            y = att @ v\n","\n","        # (b, h, seq_len, d_k) --> (b, seq_len, h, d_k) --> (b, seq_len, d_model)\n","        y = y.transpose(1, 2).contiguous().view(B, T, C)\n","\n","        # output projection\n","        y = self.resid_dropout(self.c_proj(y))\n","        return y\n","\n","\n","class Block(nn.Module):\n","    \"\"\" GPT only contain decode block\"\"\"\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.ln_1 = nn.LayerNorm(config.n_embd)\n","        self.attn = CausalSelfAttention(config)\n","        self.ln_2 = nn.LayerNorm(config.n_embd)\n","\n","        self.mlp = nn.ModuleDict(dict(\n","            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n","            act     = NewGELU(),\n","            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n","            dropout = nn.Dropout(config.resid_pdrop),\n","        ))\n","        m = self.mlp\n","        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x))))\n","\n","    def forward(self, x):\n","\n","        # (batch_size, seq_len, emb_dim)\n","        x = x + self.attn(self.ln_1(x))\n","        x = x + self.mlpf(self.ln_2(x))\n","        return x\n","\n","### testing\n","wte = nn.Embedding(config.vocab_size, config.n_embd).to(config.device)\n","block = Block(config).to(config.device)\n","\n","tok_emb = wte(x)\n","print('Token Embedding Size:', tok_emb.size())\n","\n","block_out = block(tok_emb)\n","print('Block Output Size:', block_out.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-gy9OcsmMXfD","executionInfo":{"status":"ok","timestamp":1682129537981,"user_tz":-480,"elapsed":4684,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"3a9b94ce-0467-49bf-8154-b25a4f34ad39"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Token Embedding Size: torch.Size([4, 64, 256])\n","Block Output Size: torch.Size([4, 64, 256])\n"]}]},{"cell_type":"markdown","source":["anothe"],"metadata":{"id":"kOOgD3qGGkpC"}},{"cell_type":"code","source":["class GPT(nn.Module):\n","    \"\"\" GPT Language Model \"\"\"\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.block_size = config.block_size\n","\n","        self.positional_encoding = self._get_positional_encoding(config.block_size, config.n_embd)\n","        self.transformer = nn.ModuleDict(dict(\n","            # wte = nn.Embedding(config.vocab_size, config.n_embd),\n","            # wpe = nn.Embedding(config.block_size, config.n_embd),\n","            drop = nn.Dropout(config.embd_pdrop),\n","            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n","            ln_f = nn.LayerNorm(config.n_embd),\n","        ))\n","        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n","\n","        # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper\n","        self.apply(self._init_weights)\n","        for pn, p in self.named_parameters():\n","            if pn.endswith('c_proj.weight'):\n","                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n","\n","        # report number of parameters (note we don't count the decoder parameters in lm_head)\n","        n_params = sum(p.numel() for p in self.transformer.parameters())\n","        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, nn.Embedding):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","        elif isinstance(module, nn.LayerNorm):\n","            torch.nn.init.zeros_(module.bias)\n","            torch.nn.init.ones_(module.weight)\n","\n","    def _get_positional_encoding(self, max_seq_len, d_model):\n","        positional_encoding = torch.zeros(max_length, embed_size)\n","        position = torch.arange(0, max_length).unsqueeze(1).float()\n","        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * -(math.log(10000.0) / embed_size))\n","        positional_encoding[:, 0::2] = torch.sin(position * div_term)\n","        positional_encoding[:, 1::2] = torch.cos(position * div_term)\n","        return positional_encoding\n","\n","    def configure_optimizers(self, train_config):\n","\n","        # separate out all parameters to those that will and won't experience regularizing weight decay\n","        decay = set()\n","        no_decay = set()\n","        whitelist_weight_modules = (torch.nn.Linear, )\n","        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n","        for mn, m in self.named_modules():\n","            for pn, p in m.named_parameters():\n","                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n","                # random note: because named_modules and named_parameters are recursive\n","                # we will see the same tensors p many many times. but doing it this way\n","                # allows us to know which parent module any tensor p belongs to...\n","                if pn.endswith('bias'):\n","                    # all biases will not be decayed\n","                    no_decay.add(fpn)\n","                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n","                    # weights of whitelist modules will be weight decayed\n","                    decay.add(fpn)\n","                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n","                    # weights of blacklist modules will NOT be weight decayed\n","                    no_decay.add(fpn)\n","\n","        # validate that we considered every parameter\n","        param_dict = {pn: p for pn, p in self.named_parameters()}\n","        inter_params = decay & no_decay\n","        union_params = decay | no_decay\n","\n","        # create the pytorch optimizer object\n","        optim_groups = [\n","            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n","            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n","        ]\n","        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n","        return optimizer\n","\n","    def forward(self, idx, targets=None):\n","        device = idx.device\n","        b, t = idx.size()\n","        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n","        tok_emb = self.transformer.wte(idx) # (b, t, n_embd)\n","        pos_emb = self.positional_encoding[:t, :].unsqueeze(0) # (1, t, n_embd)\n","\n","        # positional token, shape (1, t)\n","        # pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)\n","        # pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n","\n","        x = self.transformer.drop(tok_emb + pos_emb)\n","        for block in self.transformer.h:\n","            x = block(x)\n","\n","        x = self.transformer.ln_f(x)\n","        # (b, t, n_embd) -- > # (b, t, vocab_size)\n","        logits = self.lm_head(x)\n","\n","        # if we are given some desired targets also calculate the loss\n","        # -1 at output will be ignored\n","        loss = None\n","        if targets is not None:\n","            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n","        return logits, loss\n","\n","    @torch.no_grad()\n","    def generate(self, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n","        \"\"\"\n","        Take a conditioning sequence of indices idx (LongTensor of shape (b, t)) and complete\n","        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n","        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n","        \"\"\"\n","        for _ in range(max_new_tokens):\n","            # if the sequence context is growing too long we must crop it at block_size\n","            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n","            # forward the model to get the logits for the index in the sequence\n","            logits, _ = self(idx_cond)\n","            # pluck the logits at the final step and scale by desired temperature\n","            logits = logits[:, -1, :] / temperature\n","            # optionally crop the logits to only the top k options\n","            if top_k is not None:\n","                v, _ = torch.topk(logits, top_k)\n","                logits[logits < v[:, [-1]]] = -float('Inf')\n","            # apply softmax to convert logits to (normalized) probabilities\n","            probs = F.softmax(logits, dim=-1)\n","            # either sample from the distribution or take the most likely element\n","            if do_sample:\n","                idx_next = torch.multinomial(probs, num_samples=1)\n","            else:\n","                _, idx_next = torch.topk(probs, k=1, dim=-1)\n","            # append sampled index to the running sequence and continue\n","            idx = torch.cat((idx, idx_next), dim=1)\n","        return idx\n","\n","### testing\n","wte = nn.Embedding(config.vocab_size, config.n_embd).to(config.device)\n","model = GPT(config).to(config.device)\n","model = torch.compile(model)\n","\n","# sample dataset from data loader\n","logits, loss = model.forward(x, y)\n","print('logits: ', logits.size())\n","print('loss: ', loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"13LrwU6NR9bx","executionInfo":{"status":"ok","timestamp":1682129585500,"user_tz":-480,"elapsed":47534,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"4fc3d13d-2d5f-4a40-8c3a-131b25bd50df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["number of parameters: 83.64M\n"]},{"output_type":"stream","name":"stderr","text":["[2023-04-22 02:12:32,380] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n"]},{"output_type":"stream","name":"stdout","text":["logits:  torch.Size([4, 64, 301966])\n","loss:  tensor(12.6203, device='cuda:0', grad_fn=<CompiledFunctionBackward>)\n"]}]},{"cell_type":"code","source":["class Trainer:\n","\n","    def __init__(self, config, model, train_dataset):\n","        self.config = config\n","        self.model = model\n","        self.optimizer = None\n","        self.train_dataset = train_dataset\n","        self.callbacks = defaultdict(list)\n","        self.device = config.device\n","        self.model = self.model.to(self.device)\n","\n","        # variables that will be assigned to trainer class later for logging and etc\n","        self.iter_num = 0\n","        self.iter_time = 0.0\n","        self.iter_dt = 0.0\n","\n","    def add_callback(self, onevent: str, callback):\n","        self.callbacks[onevent].append(callback)\n","\n","    def set_callback(self, onevent: str, callback):\n","        self.callbacks[onevent] = [callback]\n","\n","    def trigger_callbacks(self, onevent: str):\n","        for callback in self.callbacks.get(onevent, []):\n","            callback(self)\n","\n","    def run(self):\n","        model, config = self.model, self.config\n","\n","        # setup the optimizer\n","        self.optimizer = model.configure_optimizers(config)\n","\n","        # setup the dataloader\n","        train_loader = DataLoader(\n","            self.train_dataset,\n","            sampler=torch.utils.data.RandomSampler(self.train_dataset, replacement=True, num_samples=int(1e10)),\n","            shuffle=False,\n","            # pin_memory=True,\n","            batch_size=config.batch_size,\n","            num_workers=config.num_workers,\n","        )\n","\n","        model.train()\n","        self.iter_num = 0\n","        self.iter_time = time.time()\n","        data_iter = iter(train_loader)\n","        while True:\n","\n","            # fetch the next batch (x, y) and re-init iterator if needed\n","            try:\n","                batch = next(data_iter)\n","            except StopIteration:\n","                data_iter = iter(train_loader)\n","                batch = next(data_iter)\n","            batch = [t.to(self.device) for t in batch]\n","            x, y = batch\n","\n","            # forward the model\n","            logits, self.loss = model(x, y)\n","\n","            # backprop and update the parameters\n","            model.zero_grad(set_to_none=True)\n","            self.loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n","            self.optimizer.step()\n","\n","            self.trigger_callbacks('on_batch_end')\n","            self.iter_num += 1\n","            tnow = time.time()\n","            self.iter_dt = tnow - self.iter_time\n","            self.iter_time = tnow\n","\n","            # termination conditions\n","            if config.max_iters is not None and self.iter_num >= config.max_iters:\n","                break\n","\n","model = GPT(config)\n","trainer = Trainer(config, model, train_dataset)\n","trainer = Trainer(config, model, train_dataset)\n","\n","def batch_end_callback(trainer):\n","    if trainer.iter_num % 500 == 0:\n","        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n","trainer.set_callback('on_batch_end', batch_end_callback)\n","\n","trainer.run()"],"metadata":{"id":"YqK1wjjLOhkW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1e5de401-4d95-41d0-8070-d6307eccece1","executionInfo":{"status":"ok","timestamp":1682131770427,"user_tz":-480,"elapsed":2184942,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["number of parameters: 83.64M\n","iter_dt 0.00ms; iter 0: train loss 12.63450\n","iter_dt 107.20ms; iter 500: train loss 4.48716\n","iter_dt 105.59ms; iter 1000: train loss 4.52789\n","iter_dt 108.12ms; iter 1500: train loss 4.98708\n","iter_dt 110.25ms; iter 2000: train loss 4.63459\n","iter_dt 107.60ms; iter 2500: train loss 4.00061\n","iter_dt 107.23ms; iter 3000: train loss 4.97294\n","iter_dt 108.26ms; iter 3500: train loss 5.00927\n","iter_dt 107.98ms; iter 4000: train loss 4.54547\n","iter_dt 111.64ms; iter 4500: train loss 4.49295\n","iter_dt 111.27ms; iter 5000: train loss 4.24072\n","iter_dt 107.57ms; iter 5500: train loss 4.49713\n","iter_dt 108.68ms; iter 6000: train loss 4.44291\n","iter_dt 111.30ms; iter 6500: train loss 5.33070\n","iter_dt 109.34ms; iter 7000: train loss 3.61869\n","iter_dt 107.99ms; iter 7500: train loss 4.67334\n","iter_dt 112.82ms; iter 8000: train loss 3.57174\n","iter_dt 107.69ms; iter 8500: train loss 3.86336\n","iter_dt 109.40ms; iter 9000: train loss 3.27506\n","iter_dt 109.84ms; iter 9500: train loss 4.14696\n","iter_dt 108.70ms; iter 10000: train loss 4.08218\n","iter_dt 110.11ms; iter 10500: train loss 3.54298\n","iter_dt 111.87ms; iter 11000: train loss 3.74974\n","iter_dt 108.32ms; iter 11500: train loss 3.55141\n","iter_dt 108.01ms; iter 12000: train loss 4.48247\n","iter_dt 109.48ms; iter 12500: train loss 3.81247\n","iter_dt 106.79ms; iter 13000: train loss 4.20017\n","iter_dt 110.27ms; iter 13500: train loss 4.63286\n","iter_dt 109.94ms; iter 14000: train loss 3.73029\n","iter_dt 108.09ms; iter 14500: train loss 3.91646\n","iter_dt 108.11ms; iter 15000: train loss 3.75826\n","iter_dt 109.84ms; iter 15500: train loss 4.00236\n","iter_dt 109.69ms; iter 16000: train loss 4.97849\n","iter_dt 111.63ms; iter 16500: train loss 3.79926\n","iter_dt 108.72ms; iter 17000: train loss 4.28578\n","iter_dt 107.01ms; iter 17500: train loss 3.31687\n","iter_dt 107.41ms; iter 18000: train loss 3.00550\n","iter_dt 110.36ms; iter 18500: train loss 3.55539\n","iter_dt 110.68ms; iter 19000: train loss 4.26288\n","iter_dt 108.18ms; iter 19500: train loss 3.47491\n"]}]},{"cell_type":"code","source":["text = 'Lord:\\nRise! My people, conquer the north!'\n","sample_ids = torch.Tensor(enc.encode_ordinary(text)).long()\n","sample_ids = torch.unsqueeze(sample_ids, 0).to(config.device)\n","result = model.generate(sample_ids, max_new_tokens=50, temperature=1, do_sample=False, top_k=None)\n","print(enc.decode(result.detach().cpu().tolist()[0]))"],"metadata":{"id":"duAm60cLoT_f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682133243016,"user_tz":-480,"elapsed":1394,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"b436e7f9-b4f3-4c5b-cbe5-430c4ba4d856"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Lord:\n","Rise! My people, conquer the north!\n","And, in the same blood of death,\n","Is my poor heart-blood, like a very heart,\n","To see his head in the same fair king's face.\n","\n","GLOUCESTER:\n","I'll have his uncle\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}