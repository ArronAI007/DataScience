{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"kYrE7gINKHDl"},"source":["!pip install transformers\n","!pip install tensorflow_addons\n","!curl -LO https://raw.githubusercontent.com/MohamadMerchant/SNLI/master/data.tar.gz\n","!tar -xvzf data.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s50Fc-Bxe7VS"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","import transformers\n","from sklearn.manifold import TSNE\n","from tensorflow.keras.utils import plot_model\n","import logging\n","logging.getLogger('tensorflow').disabled = True\n","pd.set_option('max_colwidth', 400)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eh9IhQKeNZDu","executionInfo":{"status":"ok","timestamp":1620062580540,"user_tz":-480,"elapsed":61298,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"2896c54e-6131-4ff0-f5af-cf42daaf92de"},"source":["# Define the strategy to use and print the number of devices found\n","strategy = tf.distribute.MirroredStrategy()\n","print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of devices: 1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Xe_VxTVCNWf_"},"source":["# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n","# tf.config.experimental_connect_to_cluster(resolver)\n","# # This is the TPU initialization code that has to be at the beginning.\n","# tf.tpu.experimental.initialize_tpu_system(resolver)\n","# print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n","# strategy = tf.distribute.TPUStrategy(resolver)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ExQOqQRx-vm7"},"source":["# 1) Text Classification"]},{"cell_type":"markdown","metadata":{"id":"9P9qEFOT-zLi"},"source":["## 1.1 Data Import"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":238},"id":"3wdjxyP9-lxA","executionInfo":{"status":"ok","timestamp":1619189573688,"user_tz":-480,"elapsed":4214,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"a526d3b4-6ce7-4942-eff0-557a4c42fad4"},"source":["train_df = pd.read_csv(\"./data/SNLI_Corpus/snli_1.0_train.csv\", nrows=100000)\n","valid_df = pd.read_csv(\"./data/SNLI_Corpus/snli_1.0_dev.csv\")\n","test_df = pd.read_csv(\"./data/SNLI_Corpus/snli_1.0_test.csv\")\n","\n","train_df = train_df[train_df.similarity != \"-\"].sample(frac=1.0, random_state=42).reset_index(drop=True)\n","valid_df = valid_df[valid_df.similarity != \"-\"].sample(frac=1.0, random_state=42).reset_index(drop=True)\n","train_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>similarity</th>\n","      <th>sentence1</th>\n","      <th>sentence2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>contradiction</td>\n","      <td>A woman is using toy which blows giant bubbles.</td>\n","      <td>A little girl is playing with chalk on a driveway.</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>neutral</td>\n","      <td>A young Asian girl holds a stuffed cat toy in a classroom.</td>\n","      <td>A young Asian girl sits in class with a stuffed cat toy, the only surviving possession remaining after the tsunami.</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>entailment</td>\n","      <td>A young woman with an afro and an electronic device in her hands walks next to an orange bike.</td>\n","      <td>A young woman walks next to an orange bike.</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>neutral</td>\n","      <td>A young asian girl is sliding down a pole on outdoor playground equipment.</td>\n","      <td>The girl has yellow skin</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>entailment</td>\n","      <td>a man is walking with a cane.</td>\n","      <td>The man is walking.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      similarity  ...                                                                                                            sentence2\n","0  contradiction  ...                                                                   A little girl is playing with chalk on a driveway.\n","1        neutral  ...  A young Asian girl sits in class with a stuffed cat toy, the only surviving possession remaining after the tsunami.\n","2     entailment  ...                                                                          A young woman walks next to an orange bike.\n","3        neutral  ...                                                                                             The girl has yellow skin\n","4     entailment  ...                                                                                                  The man is walking.\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"WtZfqUvxBnlx"},"source":["# label encoding\n","label_map = dict(enumerate(train_df['similarity'].astype('category').cat.categories))\n","y_train = train_df['similarity'].map({v:k for k, v in label_map.items()}).values\n","y_val = valid_df['similarity'].map({v:k for k, v in label_map.items()}).values\n","y_test = test_df['similarity'].map({v:k for k, v in label_map.items()}).values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AmKQOygICQ74"},"source":["## 1.2 Pre-processing"]},{"cell_type":"code","metadata":{"id":"LKtbploBnuWS"},"source":["max_length = 64\n","batch_size = 32"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nkPrtg5RDaLH"},"source":["tokenizer = transformers.BertTokenizer.from_pretrained( \"bert-base-uncased\", do_lower_case=True)\n","print(len(tokenizer.get_vocab()))\n","tokenizer.encode('Hello Tensorflow')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fiWf1kBnJp7q","executionInfo":{"status":"ok","timestamp":1619063830546,"user_tz":-480,"elapsed":12151,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"6d0620d3-ab1e-4ad7-fa0d-daa756ea8431"},"source":["sentence_pairs = train_df[[\"sentence1\", \"sentence2\"]].values[:5]\n","encoded = tokenizer.batch_encode_plus(\n","    sentence_pairs.tolist(),\n","    add_special_tokens=True,\n","    max_length=max_length,\n","    return_attention_mask=True,\n","    return_token_type_ids=True,\n","    padding='max_length',\n","    return_tensors=\"tf\")\n","\n","print(encoded.keys())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uHm47JjILO6-","executionInfo":{"status":"ok","timestamp":1619063830547,"user_tz":-480,"elapsed":11899,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"0185e0c5-8c60-48a1-e8a4-9aa07b18eabc"},"source":["print(encoded['input_ids'][0][:32])\n","print(encoded['token_type_ids'][0][:32])\n","print(encoded['attention_mask'][0][:32])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor(\n","[  101  1037  2450  2003  2478  9121  2029 13783  5016 17255  1012   102\n","  1037  2210  2611  2003  2652  2007 16833  2006  1037 11202  1012   102\n","     0     0     0     0     0     0     0     0], shape=(32,), dtype=int32)\n","tf.Tensor([0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0], shape=(32,), dtype=int32)\n","tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0], shape=(32,), dtype=int32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"KyI0-QBbJpBJ","executionInfo":{"status":"ok","timestamp":1619063830548,"user_tz":-480,"elapsed":11659,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"401529cf-3589-411b-b7c9-8cb1dcbc86d2"},"source":["tokenizer.decode(encoded['input_ids'][0][:32])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'[CLS] a woman is using toy which blows giant bubbles. [SEP] a little girl is playing with chalk on a driveway. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"iWVSSZ86_pmV"},"source":["class BertDataGenerator(tf.keras.utils.Sequence):\n","    \"\"\"Generates batches of data.\n","\n","    Args:\n","        sentence_pairs: Array of premise and hypothesis input sentences.\n","        labels: Array of labels.\n","        batch_size: Integer batch size.\n","        shuffle: boolean, whether to shuffle the data.\n","        include_targets: boolean, whether to incude the labels.\n","\n","    Returns:\n","        Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n","        (or just `[input_ids, attention_mask, `token_type_ids]`\n","         if `include_targets=False`)\n","    \"\"\"\n","\n","    def __init__(self, sentence_pairs, labels, batch_size=batch_size, shuffle=True, include_targets=True):\n","\n","        self.sentence_pairs = sentence_pairs\n","        self.labels = labels\n","        self.shuffle = shuffle\n","        self.batch_size = batch_size\n","        self.include_targets = include_targets\n","        self.tokenizer = transformers.BertTokenizer.from_pretrained( \"bert-base-uncased\", do_lower_case=True)\n","        self.indexes = np.arange(len(self.sentence_pairs))\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        # Denotes the number of batches per epoch.\n","        return len(self.sentence_pairs) // self.batch_size\n","\n","    def __getitem__(self, idx):\n","        # Retrieves the batch of index.\n","        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n","        sentence_pairs = self.sentence_pairs[indexes]\n","\n","        # With BERT tokenizer's batch_encode_plus batch of both the sentences are, encoded together and separated by [SEP] token.\n","        encoded = self.tokenizer.batch_encode_plus(\n","            sentence_pairs.tolist(),\n","            add_special_tokens=True,\n","            max_length=max_length,\n","            return_attention_mask=True,\n","            return_token_type_ids=True,\n","            padding='max_length',\n","            return_tensors=\"tf\",\n","        )\n","\n","        # Convert batch of encoded features to numpy array.\n","        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n","        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n","        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n","\n","        # Set to true if data generator is used for training/validation.\n","        if self.include_targets:\n","            labels = np.array(self.labels[indexes], dtype=\"int32\")\n","            return [input_ids, attention_masks, token_type_ids], labels\n","        else:\n","            return [input_ids, attention_masks, token_type_ids]\n","\n","    def on_epoch_end(self):\n","        # Shuffle indexes after each epoch if shuffle is set to True.\n","        if self.shuffle:\n","            np.random.RandomState(42).shuffle(self.indexes)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VVeARI6mHDta"},"source":["## 1.3 Model Building "]},{"cell_type":"markdown","metadata":{"id":"_pgfi34deBZt"},"source":["Outputs of Bert Model comprised of:\n","\n","- ***last_hidden_state*** with shape=(m, seq_len, embed_dim)\n","- ***pooler_output*** with shape=(m, emb_dim)\n","- ***hidden_states*** which generate the hidden state for all transformer layers. Only when set *output_hidden_states=True*, shape=(num_layers, m, seq_len, emb_dim)\n","- ***attentions*** attention weights from each layer.  Only when set *output_attentions=True*, shape=(num_layers, m, seq_len, emb_dim)\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"ic7tTYRbT4Uf"},"source":["def build_model():\n","\n","  # Encoded token ids from BERT tokenizer\n","  input_ids = tf.keras.layers.Input(shape=(max_length, ), dtype=tf.int32, name=\"input_ids\")\n","  # Attention masks indicates to the model which tokens should be attended to\n","  attention_masks = tf.keras.layers.Input(shape=(max_length, ), dtype=tf.int32, name=\"attention_masks\")\n","  # Token type ids are binary masks identifying different sequences in the model\n","  token_type_ids = tf.keras.layers.Input(shape=(max_length, ), dtype=tf.int32, name=\"token_type_ids\")\n","\n","  # Loading pretrained BERT model, freeze the weight, check bert_model.config to configure\n","  bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n","  bert_model.trainable = False\n","\n","  bert_output = bert_model(input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids, output_attentions=False, output_hidden_states=False)\n","  sequence_output = bert_output[\"last_hidden_state\"] # (m, seq_len, emb_dim)\n","  pooled_output = bert_output[\"pooler_output\"] # (m, emb_dim)\n","\n","  # Add trainable layers on top of Bert to adapt the pretrained features on the new data.\n","  bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(2, return_sequences=True))(sequence_output) # (m, emb_dim, hidden_unit * 2)\n","  \n","  # Applying hybrid pooling approach to bi_lstm sequence output.\n","  avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm) # (m, hidden_unit)\n","  max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm) # (m, hidden_unit)\n","  concat = tf.keras.layers.concatenate([avg_pool, max_pool]) # (m, hidden_unit)\n","  dropout = tf.keras.layers.Dropout(0.3)(concat)\n","  output = tf.keras.layers.Dense(3, activation=\"softmax\")(dropout) #(m, 3)\n","\n","  model = tf.keras.models.Model(inputs=[input_ids, attention_masks, token_type_ids], outputs=output)\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E6-a6ZpahyEy"},"source":["bert_encoder = build_model()\n","bert_encoder.compile(optimizer=tf.keras.optimizers.Adam(), loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n","bert_encoder.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D_BilrsSlp2d"},"source":["## 1.4 Model Training (Frozen Pre-trained Bert)"]},{"cell_type":"code","metadata":{"id":"zfpkxlWKlxAh"},"source":["# generate batch data\n","train_data = BertDataGenerator(train_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"), y_train, batch_size=batch_size, shuffle=True)\n","valid_data = BertDataGenerator(valid_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"), y_val, batch_size=batch_size, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"90kThjWYl4mk"},"source":["epochs = 2\n","history = bert_encoder.fit(train_data, validation_data=valid_data, epochs=epochs, use_multiprocessing=True, workers=-1, steps_per_epoch=10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hedVSC45nKuv"},"source":["## 1.5 Fine Tuning Bert\n","\n","This step must only be performed after the feature extraction model has been trained to convergence on the new data.\n","\n","This is an optional last step where bert_model is unfreezed and retrained with a very low learning rate. This can deliver meaningful improvement by incrementally adapting the pretrained features to the new data."]},{"cell_type":"code","metadata":{"id":"DTOFmOgpmj1r"},"source":["bert_encoder.layers[3].trainable = True\n","# Recompile the model to make the change effective.\n","bert_encoder.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n","bert_encoder.summary()\n","\n","# train entire model\n","history = bert_encoder.fit(train_data, validation_data=valid_data, epochs=epochs, use_multiprocessing=True, workers=-1,)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YlJSwbZ9pgrR"},"source":["## 1.6 Evaluation"]},{"cell_type":"code","metadata":{"id":"NQL6zvYLph6t"},"source":["test_data = BertDataGenerator(test_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"), y_test, batch_size=batch_size, shuffle=False)\n","model.evaluate(test_data, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-34EWG73qfS_"},"source":["def check_similarity(sentence1, sentence2):\n","    sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n","    test_data = BertDataGenerator(sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False)\n","\n","    proba = model.predict(test_data)[0]\n","    idx = np.argmax(proba)\n","    proba = f\"{proba[idx]: .2f}%\"\n","    pred = label_map.get(idx)\n","    return pred, proba"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EFq7v5J6rG_7"},"source":["sentence1 = \"Two women are observing something together.\"\n","sentence2 = \"Two women are standing with their eyes closed.\"\n","check_similarity(sentence1, sentence2)"],"execution_count":null,"outputs":[]}]}