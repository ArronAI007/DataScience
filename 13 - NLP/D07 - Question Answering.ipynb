{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"provenance":[]},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"d2f9fc86977c4e8eb006f9f3f72db99e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e0d262ddf3b24126b3853f1b84bc9ba1","IPY_MODEL_0fcfcc76eac2485d9e9f3aa93f65bc6b","IPY_MODEL_359c7128b7ba4d89bfbb5c2d8c17af95"],"layout":"IPY_MODEL_a7bd71c014554e7ca8a50a19e8ae4d03"}},"e0d262ddf3b24126b3853f1b84bc9ba1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_78fac23c2833477fa86a361f7e897633","placeholder":"​","style":"IPY_MODEL_8120d64d831a41ecac29d3471b1f9a75","value":"Epoch 0: 100%"}},"0fcfcc76eac2485d9e9f3aa93f65bc6b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c88cac8a2e345cfb72f47ccb535a9f5","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dd2e060f8e76428196d88f2b2e215549","value":2}},"359c7128b7ba4d89bfbb5c2d8c17af95":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f71927617b084ed7ae86afab9b66a8ae","placeholder":"​","style":"IPY_MODEL_e5bc44594e124aef874f207d3e4ff57a","value":" 2/2 [00:03&lt;00:00,  1.61s/it, loss=6.02, v_num=, train_loss_step=6.020, val_loss=5.530, train_loss_epoch=6.020]"}},"a7bd71c014554e7ca8a50a19e8ae4d03":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"78fac23c2833477fa86a361f7e897633":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8120d64d831a41ecac29d3471b1f9a75":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5c88cac8a2e345cfb72f47ccb535a9f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd2e060f8e76428196d88f2b2e215549":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f71927617b084ed7ae86afab9b66a8ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5bc44594e124aef874f207d3e4ff57a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bba3eee9173f46fca5658767dc9ed171":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0a33dcf8fca24dec806eb81839d43b24","IPY_MODEL_cc0c00dd64c44f8f803221e83c0c62e6","IPY_MODEL_ffe20aa4c1f244ef8949d22fd7d49d7d"],"layout":"IPY_MODEL_23a685ebbc6e466194a2c2a9e668f1b2"}},"0a33dcf8fca24dec806eb81839d43b24":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_35b59d0f8bda4957bc31d49044fbb07d","placeholder":"​","style":"IPY_MODEL_601e54d02a0d4894847b92b0483c49bc","value":"Validation DataLoader 0: 100%"}},"cc0c00dd64c44f8f803221e83c0c62e6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_770ecabebb964e09aa71dc85c0243484","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a9d2931410cf44d6ade364e88e8a1024","value":1}},"ffe20aa4c1f244ef8949d22fd7d49d7d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33afc44d5a3a43968392b0ef6574b705","placeholder":"​","style":"IPY_MODEL_35efdd1a226046f7bbaf7849404b9548","value":" 1/1 [00:00&lt;00:00, 26.77it/s]"}},"23a685ebbc6e466194a2c2a9e668f1b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"35b59d0f8bda4957bc31d49044fbb07d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"601e54d02a0d4894847b92b0483c49bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"770ecabebb964e09aa71dc85c0243484":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9d2931410cf44d6ade364e88e8a1024":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"33afc44d5a3a43968392b0ef6574b705":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35efdd1a226046f7bbaf7849404b9548":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["pip install datasets pytorch_lightning transformers"],"metadata":{"id":"PY62SlQdBzlP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset, list_datasets\n","import datasets\n","from torch.utils.data import Dataset, DataLoader\n","from argparse import Namespace\n","import torch\n","import pandas as pd\n","import pytorch_lightning as pl\n","import transformers\n","import torch.nn.functional as F\n","from typing import Union\n","from pytorch_lightning.callbacks import LearningRateMonitor\n","from pytorch_lightning.loggers import CSVLogger\n","from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n","import numpy as np\n","\n","squad_train = load_dataset('squad', split='train[:1%]')\n","squad_valid = load_dataset('squad', split='validation[:1%]')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wTXsq6G7BcuZ","executionInfo":{"status":"ok","timestamp":1659112163259,"user_tz":-480,"elapsed":5623,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"90da1976-ad7c-4a21-dc28-230f869f2de4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n","Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"]}]},{"cell_type":"markdown","source":["# 1) Data Loading\n","Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n","\n","[Kaggle Reference](https://www.kaggle.com/code/karthikrangasai/chaii-q-a-with-pytorch-lightining)"],"metadata":{"id":"uoTdjj9CBUEV"}},{"cell_type":"code","source":["config = Namespace(\n","    seed = 7,\n","    trainer = Namespace(\n","        precision = 16,\n","        accumulate_grad_batches = 2,\n","        max_epochs = 3,\n","        weights_summary='top',\n","        num_sanity_val_steps = 0,\n","        gpus = 1,\n","        fast_dev_run=True,\n","        # stochastic_weight_avg=True,\n","    ),\n","\n","    model = Namespace(\n","        model_name_or_path = \"deepset/tinyroberta-squad2\",\n","        config_name = \"deepset/tinyroberta-squad2\",\n","        optimizer_type = 'AdamW',\n","        learning_rate = 3e-5,\n","        weight_decay = 1e-2,\n","        epsilon = 1e-8,\n","        max_grad_norm = 1.0,\n","        lr_scheduler = 'cosine',\n","        warmup_ratio = 0.1,\n","    ),\n","\n","    data = Namespace(\n","        train_batch_size = 4,\n","        eval_batch_size = 4,\n","        max_seq_length = 512,\n","        doc_stride = 128,\n","        valid_split = 0.25,\n","        tokenizer_name = \"deepset/bert-base-uncased-squad2\",\n","    ),\n",")"],"metadata":{"id":"G9d4Zz8PC8QL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DatasetRetriever(Dataset):\n","    def __init__(self, features, mode='train'):\n","        super(DatasetRetriever, self).__init__()\n","        self.features = features\n","        self.mode = mode\n","\n","    def __len__(self):\n","        return len(self.features)\n","\n","    def __getitem__(self, item):\n","        feature = self.features[item]\n","\n","        res = {\n","            'input_ids': torch.tensor(feature['input_ids'], dtype=torch.long),\n","            'attention_mask': torch.tensor(feature['attention_mask'], dtype=torch.long),\n","        }\n","\n","        if self.mode == 'train':\n","            res.update({\n","                'start_position': torch.tensor(feature['start_position'], dtype=torch.long),\n","                'end_position': torch.tensor(feature['end_position'], dtype=torch.long)\n","            })\n","\n","        else:\n","            res.update({\n","                'id': feature['example_id'],\n","                'context': feature['context'],\n","                'question': feature['question']\n","            })\n","\n","        return res\n","\n","class DataModuleFit(pl.LightningDataModule):\n","    def __init__(self, config, train_set, valid_set, **kwargs):\n","        super().__init__()\n","\n","        # built in method to extract config and save as self.hparams\n","        self.save_hyperparameters(config)\n","        self._tokenizer = transformers.AutoTokenizer.from_pretrained(self.hparams.tokenizer_name)\n","        self.train_set = train_set\n","        self.valid_set = valid_set\n","\n","    def _prepare_features(self, example):\n","\n","        # stride = # of overlapping tokens from the end of the truncated sequence\n","        # the overlap between truncated and overflowing sequences\n","        # return overflow will make the truncated text as next data line\n","        tokenized_example = self._tokenizer(\n","            [x.lstrip() for x in example[\"question\"]],\n","            example[\"context\"],\n","            truncation=\"only_second\",\n","            max_length=self.hparams.max_seq_length,\n","            stride=self.hparams.doc_stride,\n","            return_overflowing_tokens=True,\n","            return_offsets_mapping=True,\n","            padding=\"max_length\",\n","        )\n","\n","        # the index of overflowing sample\n","        sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n","\n","        # offset mapping is the span of text corresponding to each token\n","        # (33, 35) --> {'word': 'Hu', 'start': 33, 'end': 35},\n","        # (35, 40) --> 'word': '##gging', 'start': 35, 'end': 40,\n","        offset_mapping = tokenized_example.pop(\"offset_mapping\")\n","\n","        features = []\n","        for i, offsets in enumerate(offset_mapping):\n","            feature = {}\n","            input_ids = tokenized_example[\"input_ids\"][i]\n","            attention_mask = tokenized_example[\"attention_mask\"][i]\n","            feature['input_ids'] = input_ids\n","            feature['attention_mask'] = attention_mask\n","            feature['offset_mapping'] = offsets\n","\n","            # cls_index = 0\n","            cls_index = input_ids.index(self._tokenizer.cls_token_id)\n","\n","            # sequence_ids = [None, 0 * 15, 1 * 158]\n","            sequence_ids = tokenized_example.sequence_ids(i)\n","            sample_index = sample_mapping[i]\n","            answers = example[\"answers\"][i]\n","\n","            ### converting the answer start char position to token position\n","            if len(answers[\"answer_start\"]) == 0:\n","                feature[\"start_position\"] = cls_index\n","                feature[\"end_position\"] = cls_index\n","            else:\n","                start_char = answers[\"answer_start\"][0]\n","                end_char = start_char + len(answers[\"text\"][0])\n","\n","                # make pred token start at context\n","                token_start_index = 0\n","                while sequence_ids[token_start_index] != 1:\n","                    token_start_index += 1\n","\n","                # make pred token end at padding\n","                token_end_index = len(input_ids) - 1\n","                while sequence_ids[token_end_index] != 1:\n","                    token_end_index -= 1\n","\n","                # offsets[17] = (0, 0)\n","                # offsets [174] = (694, 695)\n","                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n","                    feature[\"start_position\"] = cls_index\n","                    feature[\"end_position\"] = cls_index\n","                else:\n","\n","                    # to make token_start_index equal to start char but not bigger than whole length\n","                    # 17 < 384 & 0 <= 505\n","                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n","                        token_start_index += 1\n","                    feature[\"start_position\"] = token_start_index - 1\n","                    while offsets[token_end_index][1] >= end_char:\n","                        token_end_index -= 1\n","                    feature[\"end_position\"] = token_end_index + 1\n","\n","            features.append(feature)\n","        return features\n","\n","    def prepare_data(self):\n","\n","        self._train_features = self._prepare_features(self.train_set)\n","        self._valid_features = self._prepare_features(self.valid_set)\n","\n","    def setup(self, stage=None):\n","        self._train_dset = DatasetRetriever(self._train_features)\n","        self._valid_dset = DatasetRetriever(self._valid_features)\n","\n","    def train_dataloader(self):\n","        return DataLoader(\n","            self._train_dset,\n","            batch_size=self.hparams.train_batch_size,\n","            num_workers=4,\n","            pin_memory=True,\n","            drop_last=False,\n","            shuffle=True\n","        )\n","\n","    def val_dataloader(self):\n","        return DataLoader(\n","            self._valid_dset,\n","            batch_size=self.hparams.eval_batch_size,\n","            num_workers=4,\n","            pin_memory=True,\n","            drop_last=False,\n","            shuffle=False,\n","    )"],"metadata":{"id":"iK24TIv7QQ_s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["module = DataModuleFit(config.data, squad_train, squad_valid)\n","module.prepare_data()\n","module.setup()\n","train_dataloader = module.train_dataloader()\n","val_dataloader = module.val_dataloader()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qqkg0Kl9OxAe","executionInfo":{"status":"ok","timestamp":1659112175581,"user_tz":-480,"elapsed":12325,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"ab9b3db6-241e-4891-dfa3-34690f73c0f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}]},{"cell_type":"markdown","source":["# 2) Model Training"],"metadata":{"id":"0IEXezQbDL6m"}},{"cell_type":"code","source":["class Model(pl.LightningModule):\n","\n","    def __init__(self, config, **kwargs):\n","        super().__init__()\n","        self.save_hyperparameters(config)\n","        self.model_config = transformers.AutoConfig.from_pretrained(self.hparams.config_name)\n","        self.model = transformers.AutoModel.from_pretrained(self.hparams.model_name_or_path, config=self.model_config)\n","        self.qa_outputs = torch.nn.Linear(self.model_config.hidden_size, 2)\n","        self.dropout = torch.nn.Dropout(self.model_config.hidden_dropout_prob)\n","        self._init_weights(self.qa_outputs)\n","\n","    def forward(self, input_ids, attention_mask):\n","        \"\"\"The forward step performs the next step for the model while training.\"\"\"\n","\n","        # sequence_output['last_hidden_state'].size() == [m, 512, 768]\n","        # sequence_output['pooler_output'].size() = [m, 768]\n","        sequence_output = self.model(input_ids, attention_mask=attention_mask)[0]\n","\n","        # [m, 512, 2]\n","        qa_logits = self.qa_outputs(sequence_output)\n","\n","        # [m, 512, 1], [m, 512, 1]\n","        start_logits, end_logits = qa_logits.split(1, dim=-1)\n","\n","        # [m, 512]\n","        start_logits = start_logits.squeeze(-1)\n","        end_logits = end_logits.squeeze(-1)\n","\n","        return start_logits, end_logits\n","\n","    def predict(self, batch):\n","        input_ids = batch[\"input_ids\"]\n","        attention_mask = batch[\"attention_mask\"]\n","        pred_start, pred_end = self(input_ids, attention_mask=attention_mask)\n","        return {\n","            'pred_start': pred_start,\n","            'pred_end': pred_end,\n","        }\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, torch.nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","\n","    def get_num_training_steps(self) -> int:\n","        \"\"\"Total training steps inferred from datamodule and devices.\"\"\"\n","        if isinstance(self.trainer.limit_train_batches, int) and self.trainer.limit_train_batches != 0:\n","            num_batches = self.trainer.limit_train_batches\n","        elif isinstance(self.trainer.limit_train_batches, float):\n","            # limit_train_batches is a percentage of batches\n","            dataset_size = len(self.train_dataloader())\n","            num_batches = int(dataset_size * self.trainer.limit_train_batches)\n","        else:\n","            num_batches = len(self.train_dataloader())\n","\n","        num_devices = max(1, self.trainer.num_gpus, self.trainer.num_processes)\n","        if self.trainer.tpu_cores:\n","            num_devices = max(num_devices, self.trainer.tpu_cores)\n","\n","        effective_batch_size = self.trainer.accumulate_grad_batches * num_devices\n","        max_estimated_steps = (num_batches // effective_batch_size) * self.trainer.max_epochs\n","\n","        if self.trainer.max_steps and self.trainer.max_steps < max_estimated_steps:\n","            return self.trainer.max_steps\n","        return max_estimated_steps\n","\n","    @staticmethod\n","    def _compute_warmup(num_training_steps: int, num_warmup_steps: Union[int, float]) -> int:\n","        if isinstance(num_warmup_steps, float) and (num_warmup_steps > 1 or num_warmup_steps < 0):\n","            raise Exception(\"`num_warmup_steps` as float should be provided between 0 and 1.\")\n","\n","        if isinstance(num_warmup_steps, int):\n","            if num_warmup_steps > num_training_steps:\n","                raise Exception(\"`num_warmup_steps` as int should be less than `num_training_steps`.\")\n","            return num_warmup_steps\n","\n","\n","        if isinstance(num_warmup_steps, float):\n","            # Convert float values to percentage of training steps to use as warmup\n","            num_warmup_steps *= num_training_steps\n","        return round(num_warmup_steps)\n","\n","    def configure_optimizers(self):\n","        param_optimizer = list(self.model.named_parameters())\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","        optimizer_grouped_parameters = [\n","            {\n","                \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","                \"weight_decay_rate\": self.hparams.weight_decay\n","            },\n","            {\n","                \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","                \"weight_decay_rate\": 0.0\n","            },\n","        ]\n","        optimizer = transformers.AdamW(\n","            optimizer_grouped_parameters,\n","            lr=self.hparams.learning_rate,\n","            eps=self.hparams.epsilon,\n","            correct_bias=True)\n","\n","        if self.hparams.lr_scheduler is not None:\n","            num_training_steps = self.get_num_training_steps()\n","            lr_scheduler = transformers.get_scheduler(\n","                name=self.hparams.lr_scheduler,\n","                optimizer=optimizer,\n","                num_warmup_steps=self._compute_warmup(num_training_steps, self.hparams.warmup_ratio),\n","                num_training_steps=num_training_steps,\n","            )\n","            lr_scheduler_config = {\n","                \"scheduler\": lr_scheduler,\n","                \"interval\": \"step\",\n","                \"frequency\": 1,\n","            }\n","            return [optimizer], [lr_scheduler_config]\n","        return optimizer\n","\n","    def _compute_loss(self, preds, labels):\n","        start_preds, end_preds = preds\n","        start_labels, end_labels = labels\n","        start_loss = F.cross_entropy(start_preds, start_labels, ignore_index=-1)\n","        end_loss = F.cross_entropy(end_preds, end_labels, ignore_index=-1)\n","        total_loss = (start_loss + end_loss) / 2\n","        return total_loss\n","\n","    def training_step(self, batch, batch_idx):\n","        input_ids = batch[\"input_ids\"]\n","        attention_mask = batch[\"attention_mask\"]\n","        targets_start = batch[\"start_position\"]\n","        targets_end = batch['end_position']\n","\n","        outputs_start, outputs_end = self(input_ids, attention_mask=attention_mask)\n","        loss = self._compute_loss((outputs_start, outputs_end), (targets_start, targets_end))\n","        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        input_ids = batch[\"input_ids\"]\n","        attention_mask = batch[\"attention_mask\"]\n","        targets_start = batch[\"start_position\"]\n","        targets_end = batch['end_position']\n","\n","        outputs_start, outputs_end = self(input_ids, attention_mask=attention_mask)\n","        loss = self._compute_loss((outputs_start, outputs_end), (targets_start, targets_end))\n","        self.log('val_loss', loss, prog_bar=True)"],"metadata":{"id":"s8OhYy0tjZnD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Model(config.model)\n","for batch in train_dataloader:\n","    break\n","\n","res = model.forward(batch['input_ids'], batch['attention_mask'])\n","print(res[0].size(), res[0].size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PYuv5kIHlvW0","executionInfo":{"status":"ok","timestamp":1659112187715,"user_tz":-480,"elapsed":12138,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"eab80a0e-4134-4cac-c730-386125137097"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at deepset/tinyroberta-squad2 were not used when initializing RobertaModel: ['qa_outputs.bias', 'qa_outputs.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at deepset/tinyroberta-squad2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([4, 512]) torch.Size([4, 512])\n"]}]},{"cell_type":"code","source":["# training\n","pl.seed_everything(config.seed)\n","lr_monitor = LearningRateMonitor(logging_interval='step')\n","logger = CSVLogger(save_dir='logs/')\n","\n","# Checkpoint\n","ckpt = ModelCheckpoint(\n","    monitor=f'val_loss',\n","    save_top_k=1,\n","    save_last=False,\n","    save_weights_only=True,\n","    dirpath='checkpoints',\n","    filename='{epoch:02d}-{val_loss:.4f}',\n","    verbose=False,\n","    mode='min',\n",")\n","\n","trainer = pl.Trainer(\n","    logger=logger,\n","    callbacks=[ckpt, lr_monitor],\n","    **vars(config.trainer)\n",")\n","\n","trainer.fit(model, datamodule=module)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":733,"referenced_widgets":["d2f9fc86977c4e8eb006f9f3f72db99e","e0d262ddf3b24126b3853f1b84bc9ba1","0fcfcc76eac2485d9e9f3aa93f65bc6b","359c7128b7ba4d89bfbb5c2d8c17af95","a7bd71c014554e7ca8a50a19e8ae4d03","78fac23c2833477fa86a361f7e897633","8120d64d831a41ecac29d3471b1f9a75","5c88cac8a2e345cfb72f47ccb535a9f5","dd2e060f8e76428196d88f2b2e215549","f71927617b084ed7ae86afab9b66a8ae","e5bc44594e124aef874f207d3e4ff57a","bba3eee9173f46fca5658767dc9ed171","0a33dcf8fca24dec806eb81839d43b24","cc0c00dd64c44f8f803221e83c0c62e6","ffe20aa4c1f244ef8949d22fd7d49d7d","23a685ebbc6e466194a2c2a9e668f1b2","35b59d0f8bda4957bc31d49044fbb07d","601e54d02a0d4894847b92b0483c49bc","770ecabebb964e09aa71dc85c0243484","a9d2931410cf44d6ade364e88e8a1024","33afc44d5a3a43968392b0ef6574b705","35efdd1a226046f7bbaf7849404b9548"]},"id":"CvCLSXONrp2T","executionInfo":{"status":"ok","timestamp":1659112207371,"user_tz":-480,"elapsed":19670,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"64d74649-5ac4-461f-f2d2-34bacd348ac5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Global seed set to 7\n","Using 16bit native Automatic Mixed Precision (AMP)\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","Running in fast_dev_run mode: will run a full train, val, test and prediction loop using 1 batch(es).\n","`Trainer(limit_train_batches=1)` was configured so 1 batch per epoch will be used.\n","`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n","`Trainer(limit_test_batches=1)` was configured so 1 batch will be used.\n","`Trainer(limit_predict_batches=1)` was configured so 1 batch will be used.\n","`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py:2127: LightningDeprecationWarning: `Trainer.num_gpus` was deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.num_devices` instead.\n","  \"`Trainer.num_gpus` was deprecated in v1.6 and will be removed in v1.8.\"\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py:2095: LightningDeprecationWarning: `Trainer.num_processes` is deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.num_devices` instead.\n","  \"`Trainer.num_processes` is deprecated in v1.6 and will be removed in v1.8. \"\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py:2111: LightningDeprecationWarning: `Trainer.tpu_cores` is deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.num_devices` instead.\n","  \"`Trainer.tpu_cores` is deprecated in v1.6 and will be removed in v1.8. \"\n","\n","  | Name       | Type         | Params\n","--------------------------------------------\n","0 | model      | RobertaModel | 82.1 M\n","1 | qa_outputs | Linear       | 1.5 K \n","2 | dropout    | Dropout      | 0     \n","--------------------------------------------\n","82.1 M    Trainable params\n","0         Non-trainable params\n","82.1 M    Total params\n","164.240   Total estimated model params size (MB)\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py:1937: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n","  category=PossibleUserWarning,\n"]},{"output_type":"display_data","data":{"text/plain":["Training: 0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2f9fc86977c4e8eb006f9f3f72db99e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Validation: 0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bba3eee9173f46fca5658767dc9ed171"}},"metadata":{}}]},{"cell_type":"code","source":["# prediction\n","for batch in val_dataloader:\n","    break\n","\n","preds = model.predict(batch)\n","sub_pred_start = preds['pred_start'].argmax(dim=-1)\n","sub_pred_end = preds['pred_end'].argmax(dim=-1)\n","print(sub_pred_start)\n","print(sub_pred_end)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rL-5IuNOv08H","executionInfo":{"status":"ok","timestamp":1659112285595,"user_tz":-480,"elapsed":5473,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"c09ba402-94c7-49d0-97eb-79331f8aad49"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["tensor([  6,  34,  44, 146])\n","tensor([ 2, 74, 15,  6])\n"]}]}]}