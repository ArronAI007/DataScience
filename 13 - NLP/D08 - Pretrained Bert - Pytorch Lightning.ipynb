{"cells":[{"cell_type":"code","source":["!pip install iterative_train_test_split tqdm mlflow"],"metadata":{"id":"BtZJ9VTZB59S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import sys, os\n","import yaml, logging\n","import mlflow\n","from mlflow.tracking import MlflowClient\n","import mlflow.pytorch\n","from skmultilearn.model_selection import iterative_train_test_split\n","import transformers\n","import torch\n","import pytorch_lightning as pl  \n","from torch.utils.data import DataLoader, Dataset, random_split\n","from sklearn.metrics import classification_report, f1_score\n","from tqdm import tqdm\n","from typing import Optional, Union"],"metadata":{"id":"uB-TtWxP_mPz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1) Data Processing"],"metadata":{"id":"Sm6_qKwqD6CU"}},{"cell_type":"code","source":["def split_data(\n","    df:pd.DataFrame, \n","    aspect_classes:list, \n","    x_col:list=['text', 'label'], \n","    test_size:float=0.2, seed:int=0\n","    ):\n","    '''\n","    Split data into test train set\n","    '''\n","    np.random.seed(seed)\n","    x = df[x_col].values\n","    y = df[aspect_classes].values\n","    X_train, y_train, X_test, y_test = iterative_train_test_split(x, y, test_size=test_size)\n","    return pd.DataFrame(X_train, columns=x_col), pd.DataFrame(X_test, columns=x_col)    \n","\n","\n","class DataModule(pl.LightningDataModule):\n","    def __init__(\n","        self, df_train:pd.DataFrame, df_test:pd.DataFrame, max_len:int, batch_size:int, \n","        tokenizer:str=\"distilbert-base-uncased\", text_col:str='text'\n","        ):\n","        '''Picking Up Raw Data and Processing'''\n","        \n","        super().__init__()\n","        self.train_df = df_train\n","        self.test_df = df_test\n","        self.max_len = max_len\n","        self.batch_size = batch_size\n","        self.text_col = text_col\n","        \n","        if tokenizer == \"distilbert-base-uncased\":\n","            logger.info(\"Applying Distillbert Tokenizer\")\n","            self.tokenizer = transformers.DistilBertTokenizer.from_pretrained(\n","                \"distilbert-base-uncased\")\n","        else:\n","            logger.info(\"Applying Bertweet Tokenizer\")\n","            self.tokenizer = transformers.BertweetTokenizer.from_pretrained(\n","                \"vinai/bertweet-base\", normalization=True)\n","        \n","    class Dataset(Dataset):\n","        def __init__(self, encodings, labels):\n","            self.encodings = encodings\n","            self.labels = labels\n","\n","        def __getitem__(self, idx):\n","            item = {\n","                key: torch.tensor(val[idx]).clone().detach() \n","                for key, val in self.encodings.items()\n","                }\n","            item['labels'] = torch.tensor(self.labels[idx])\n","            return item\n","\n","        def __len__(self):\n","            return len(self.labels)\n","    \n","    def train_dataloader(self):\n","        '''Return DataLoader for train tokens and labels'''\n","        \n","        features = self.tokenizer(\n","            self.train_df[self.text_col].tolist(), \n","            max_length=self.max_len, \n","            truncation=True, \n","            padding='max_length',  \n","            return_tensors='pt')\n","        \n","        labels = self.train_df['label'].tolist()\n","        dataset = self.Dataset(features, labels)\n","        return DataLoader(dataset, batch_size=self.batch_size, shuffle=True, num_workers=10)\n","    \n","    def val_dataloader(self):\n","        '''Return DataLoader for test tokens and labels'''\n","        \n","        # only pad to longest length of the current batch\n","        features = self.tokenizer(\n","            self.test_df[self.text_col].tolist(), \n","            max_length=self.max_len, \n","            truncation=True, \n","            padding='longest',  \n","            return_tensors='pt')\n","        \n","        labels = self.test_df['label'].tolist()\n","        dataset = self.Dataset(features, labels)\n","        return DataLoader(dataset, batch_size=self.batch_size, shuffle=False, num_workers=10)\n","    \n","    def calculate_pos_weights(self, class_counts, len_data):\n","        pos_weights = np.ones_like(class_counts)\n","        neg_counts = [len_data - pos_count for pos_count in class_counts]\n","        for cdx, (pos_count, neg_count) in enumerate(zip(class_counts,  neg_counts)):\n","            pos_weights[cdx] = neg_count / (pos_count + 1e-5)\n","            # pos_weights[cdx] = 1. if pos_weights[cdx] == 0 else pos_weights[cdx]\n","        return torch.as_tensor(pos_weights, dtype=torch.float)\n","    \n","    def get_weight(self, df, aspect_classes):\n","        return self.calculate_pos_weights(\n","             df[aspect_classes].sum().values, \n","             len(df)\n","         )"],"metadata":{"id":"gR7gSqulBcwi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["INPUT = 'sample.csv'\n","X_COL = ['text']\n","Y_COL = ['ability', 'dependability', 'purpose', 'integrity']\n","\n","BATCH_SIZE = 32\n","INPUT_MAX_LEN = 128\n","STANDARD_LR = 5e-5\n","FINE_LR = 5e-7\n","EPOCHS = 20\n","LIMIT_STEP = 500\n","MODEL = \"vinai/bertweet-base\"\n","TEXT_COL = 'text'\n","\n","df = pd.read_parquet(INPUT)\n","df_train, df_val = split_data(df, Y_COL, X_COL, test_size=0.2, seed=0)\n","\n","data_module = DataModule(\n","    df_train,\n","    df_val,\n","    max_len=INPUT_MAX_LEN, \n","    batch_size=BATCH_SIZE,\n","    tokenizer=MODEL,\n","    text_col=TEXT_COL\n",")"],"metadata":{"id":"N77y-Nu6A6I6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2) Model Training"],"metadata":{"id":"8l-BaDZYEf_r"}},{"cell_type":"code","source":["class LightningArticleClassifier(pl.LightningModule):\n","    def __init__(\n","        self, output_class_len, learning_rate, \n","        max_len=64, hidden_dim=64, pos_weight=None, bert_model=\"distilbert-base-uncased\"\n","        ):\n","\n","        super().__init__()\n","        self.max_len = max_len\n","        self.lr = learning_rate\n","        self.emb_dim = 768\n","        self.hidden_dim = hidden_dim\n","        self.drop_out = torch.nn.Dropout(0.1)\n","        self.fc1 = torch.nn.Linear(self.emb_dim, self.emb_dim // 2)\n","        self.fc2 = torch.nn.Linear(self.emb_dim // 2, self.hidden_dim * 4)\n","        self.fc3 = torch.nn.Linear(self.hidden_dim * 4, self.hidden_dim)\n","        self.fc4 = torch.nn.Linear(self.hidden_dim, output_class_len)\n","        self.tanh = torch.nn.Tanh()\n","        self.gelu = torch.nn.GELU()\n","        self.softmax = torch.nn.LogSoftmax(dim=1)\n","        \n","        if pos_weight is not None:\n","            self.pos_weight = torch.tensor(pos_weight, dtype=torch.float)\n","        else:\n","            self.pos_weight = None\n","        \n","        if bert_model == \"distilbert-base-uncased\":\n","            logger.info(\"Importing Distillbert Model\")\n","            self.bert_model = transformers.DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n","        else:\n","            logger.info(\"Importing Bertweet Model\")\n","            self.bert_model = transformers.AutoModel.from_pretrained(\"vinai/bertweet-base\")\n","        \n","        # metrics\n","        self.val_loss, self.val_corrects, self.val_len = 0., 0., 0.\n","        self.train_loss, self.train_corrects, self.train_len = 0., 0., 0.\n","        # self.train_f1, self.val_f1 = 0., 0.\n","        self.train_step, self.val_step = 0, 0\n","        self.epoch_loss_train, self.epoch_acc_train,  self.epoch_f1_train = [], [], []\n","        self.epoch_loss_val, self.epoch_acc_val, self.epoch_f1_val = [], [], []\n","            \n","    def forward(self, input_ids, attention_mask):\n","        bert_output = self.bert_model(\n","            input_ids=input_ids, \n","            attention_mask=attention_mask, \n","            output_attentions=False, \n","            output_hidden_states=False)\n","        \n","        # feed forward layer\n","        output = bert_output['last_hidden_state'][:, 0, :]\n","        output = self.fc1(output)\n","        output = self.tanh(output)\n","        output = self.drop_out(output)\n","        output = self.fc2(output)\n","        output = self.gelu(output)\n","        output = self.drop_out(output)\n","        output = self.fc3(output)\n","        output = self.gelu(output)\n","        output = self.drop_out(output)\n","        output = self.fc4(output)\n","        # output = torch.sigmoid(output)\n","        # output = self.softmax(output)\n","        return output\n","    \n","    def criterion(self, y_pred, y_true):\n","        \n","        if self.pos_weight != None:\n","            # criterion = torch.nn.CrossEntropyLoss(weight=self.pos_weight.cuda())\n","            criterion = torch.nn.BCEWithLogitsLoss(pos_weight=self.pos_weight.cuda())\n","        else:\n","            # criterion = torch.nn.CrossEntropyLoss()\n","            criterion = torch.nn.BCEWithLogitsLoss()\n","        return criterion(y_pred, y_true.float())\n","    \n","    def training_step(self, train_batch, batch_idx):\n","        input_ids = train_batch['input_ids']\n","        attention_mask = train_batch['attention_mask']\n","        labels = train_batch['labels']\n","        y_pred = self.forward(input_ids, attention_mask)\n","        loss = self.criterion(y_pred, labels)\n","        # _, preds = torch.max(y_pred, 1)\n","        preds = (y_pred >= 0.5).int()\n","                \n","        self.train_loss += loss\n","        # self.train_corrects += torch.sum(torch.sum(preds == labels.data))\n","        self.train_corrects += torch.sum(torch.all(torch.eq(preds, labels), dim=1).int())\n","        # self.train_f1 += f1_score(preds.cpu(), labels.data.cpu(), average='macro')\n","\n","        self.train_len += len(labels)\n","        self.train_step += 1\n","        self.log('train_loss', loss)\n","        \n","        return loss\n","    \n","    def training_epoch_end(self, out):\n","        self.epoch_acc_train.append(self.train_corrects / (self.train_len + 1))\n","        self.epoch_loss_train.append(self.train_loss / (self.train_len + 1))\n","        # self.epoch_f1_train.append(self.train_f1 / self.train_step)\n","        \n","        if self.current_epoch % 2 == 0:\n","            logger.info(f'\\nEpoch: {self.current_epoch}')\n","            logger.info(f'Training: loss: {self.epoch_loss_train[-1]}')\n","            logger.info(f'Training: Accuracy: {self.epoch_acc_train[-1]}')\n","            # print(f'Training: Macro F1: {self.epoch_f1_train[-1]}')\n","            \n","        self.train_loss, self.train_corrects = 0., 0.\n","        self.train_step, self.train_len = 0., 0.\n","            \n","    def validation_step(self, val_batch, batch_idx):\n","        input_ids = val_batch['input_ids']\n","        attention_mask = val_batch['attention_mask']\n","        labels = val_batch['labels']\n","        y_pred = self.forward(input_ids, attention_mask)\n","        loss = self.criterion(y_pred, labels)\n","        preds = (y_pred >= 0.5).int()\n","        # _, preds = torch.max(y_pred, 1)\n","        \n","        self.val_loss += loss\n","        # self.val_corrects += torch.sum(torch.sum(preds == labels.data))   \n","        self.val_corrects += torch.sum(torch.all(torch.eq(preds, labels), dim=1).int())\n","        \n","        # self.val_f1 += f1_score(preds.cpu(), labels.data.cpu(), average='macro')\n","        \n","        self.val_len += len(labels)\n","        self.val_step += 1\n","        self.log('val_loss', loss)\n","        return loss\n","    \n","    def validation_epoch_end(self, out):\n","        self.epoch_acc_val.append(self.val_corrects / (self.val_len + 1))\n","        self.epoch_loss_val.append(self.val_loss / (self.val_len + 1))\n","        # self.epoch_f1_val.append(self.val_f1 / self.val_step)\n","        \n","        if self.current_epoch % 2 == 0:\n","            logger.info(f'Validation: loss: {self.epoch_loss_val[-1]}')\n","            logger.info(f'Validation: Accuracy: {self.epoch_acc_val[-1]}')\n","            # print(f'Validation: Macro F1: {self.epoch_f1_val[-1]}')\n","            \n","        self.val_loss, self.val_corrects = 0., 0.\n","        self.val_step, self.val_len = 0., 0.\n","            \n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n","        '''\n","        lr_scheduler = {'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min'),\n","                        \"monitor\": \"train_loss\",\n","                       }\n","        \n","        return [optimizer], [lr_scheduler]\n","        '''\n","        return optimizer"],"metadata":{"id":"xWzuUUmbEkCp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = LightningArticleClassifier(\n","    output_class_len=len(aspect_classes),\n","    learning_rate=STANDARD_LR,\n","    max_len=INPUT_MAX_LEN,\n","    # pos_weight=[1, 5, 1, 1, 1]\n","    bert_model=MODEL\n",")\n","\n","trainer = pl.Trainer(\n","    max_epochs=EPOCHS,\n","    limit_train_batches=LIMIT_STEP,\n","    accelerator=\"gpu\",\n","    strategy=\"dp\",\n","    gpus=-1,\n","    # accelerator='ddp',\n","    # default_root_dir='/dbfs/FileStore/temp/kean_temp/logs'\n",")\n","\n","for param in model.bert_model.parameters():\n","    param.requires_grad = False\n","trainer.fit(model, data_module)\n","\n","### fine tuning\n","if FINE_LR is not None:\n","    trainer = pl.Trainer(\n","        max_epochs=5, \n","        limit_train_batches=LIMIT_STEP,\n","        accelerator=\"gpu\", \n","        strategy=\"dp\",\n","        gpus=-1,\n","        # accelerator='ddp',\n","        # default_root_dir='/dbfs/FileStore/temp/kean_temp/logs'\n","    )\n","\n","    model.lr = FINE_LR\n","    for param in model.bert_model.parameters():\n","        param.requires_grad = True\n","    trainer.fit(model, data_module)"],"metadata":{"id":"3oYkxNlYBb7Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3) Model Logging"],"metadata":{"id":"qADYvyFbFsTD"}},{"cell_type":"code","source":["def log_model(model_name, model, params, metrics, artifacts=None, experiment_uri=''):\n","    '''Log Model in MLFlow'''\n","    \n","    ### define experiment uri for mlflow\n","    if experiment_uri != '':\n","        if not mlflow.get_experiment_by_name(experiment_uri):\n","            mlflow.create_experiment(experiment_uri)\n","        mlflow.set_experiment(experiment_uri)\n","    \n","    with mlflow.start_run(run_name=model_name) as run:\n","        experimentID = run.info.experiment_id\n","        print(\"Experiment ID\", experimentID)\n","        mlflow.pytorch.log_model(model, model_name)\n","        mlflow.pytorch.log_state_dict(model.state_dict(), model_name)\n","        \n","        for k,v in params.items():\n","            mlflow.log_param(k, v)\n","        for k,v in metrics.items():\n","            mlflow.log_metric(k, v)\n","        if artifacts is not None:\n","            for artifact in artifacts:\n","                mlflow.log_artifact(artifact)\n","        mlflow.end_run()"],"metadata":{"id":"zP-Y0SGDFzYH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["experiment_uri = f\"/Users/{cfg['mlflow_email']}/{cfg['model_name']}\"\n","logger.info(f\"Logging to MlFlow in : {experiment_uri}\")\n","model_name = cfg['model_name']\n","\n","params = {\n","    'description': cfg['model_desc'],\n","    'epochs': EPOCHS,\n","    'max_sequence_length': INPUT_MAX_LEN,\n","    'batch_size': BATCH_SIZE,\n","    'max_step_per_epoch': LIMIT_STEP,\n","    'lr': STANDARD_LR,\n","}\n","\n","model_metrics = {\n","  \"train_loss\" : round(model.epoch_loss_train[-1].item(), 3),\n","  \"train_acc\" : round(model.epoch_acc_train[-1].item(), 3),\n","  \"eval_loss\" : round(model.epoch_loss_val[-1].item(), 3),\n","  \"eval_acc\" : round(model.epoch_acc_val[-1].item(), 3),\n","}\n","\n","log_model(model_name, model, params, model_metrics, experiment_uri=experiment_uri)"],"metadata":{"id":"Kk1Q3GBWFr7U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4) Validation"],"metadata":{"id":"N6DicHxTGnwz"}},{"cell_type":"code","source":["### model validation\n","data_module = DataModule(\n","    df_train,\n","    df_val,\n","    max_len=INPUT_MAX_LEN, \n","    batch_size=BATCH_SIZE,\n","    tokenizer=MODEL,\n","    text_col=text_col\n",")\n","\n","df_test = data_module.test_df\n","test_set = data_module.val_dataloader()\n","batch = test_set.dataset.encodings\n","total_len = len(batch['input_ids'])\n","step = 512\n","\n","probas = []\n","model.eval()\n","with torch.no_grad():\n","    for i in tqdm(range(0, total_len, step)):\n","        outputs = model.forward(\n","            input_ids=batch['input_ids'][i : i + step], \n","            attention_mask=batch['attention_mask'][i : i + step]\n","        )\n","        probas.append(torch.sigmoid(outputs).cpu().detach().numpy())\n","\n","thres = 0.5\n","result = np.vstack(probas)\n","result = (result >= thres).astype(int)\n","print(\n","    classification_report(np.array(test_set.dataset.labels), result, target_names=aspect_classes)\n",")    "],"metadata":{"id":"nyWrPXLXFDzi"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}