{"cells":[{"cell_type":"code","source":["# This code is from the repository https://github.com/databrickslabs/dolly\n","# Copyright (c) 2023 databrickslabs"],"metadata":{"id":"GCMCfQqEjQfr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers accelerate"],"metadata":{"id":"A3Npg25KL-1N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1) Training"],"metadata":{"id":"cbwWhwFGfSn8"}},{"cell_type":"markdown","source":["## 1.1 Initialization"],"metadata":{"id":"i-jE091VfxSp"}},{"cell_type":"code","source":["from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    DataCollatorForLanguageModeling,\n","    PreTrainedTokenizer,\n","    Trainer,\n","    TrainingArguments\n",")\n","from torch.nn.parallel import DistributedDataParallel as DDP\n","from datasets import Dataset, load_dataset\n","from functools import partial\n","import logging\n","import torch\n","import numpy as np \n","import re\n","\n","logger = logging.getLogger(\"logger\")\n","\n","### to be added as special tokens\n","INSTRUCTION_KEY = \"### Instruction:\"\n","INPUT_KEY = \"Input:\"\n","RESPONSE_KEY = \"### Response:\"\n","END_KEY = \"### End\"\n","RESPONSE_KEY_NL = f\"{RESPONSE_KEY}\\n\""],"metadata":{"id":"BcYBG3e8f0WC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1.2 Model Loading"],"metadata":{"id":"6YjlzTk8f2V0"}},{"cell_type":"code","source":["### Model Loading\n","INPUT_MODEL = \"EleutherAI/pythia-2.8b\"\n","def load_tokenizer(pretrained_model_name_or_path):\n","    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)\n","    tokenizer.pad_token = tokenizer.eos_token\n","    tokenizer.add_special_tokens({\"additional_special_tokens\": [END_KEY, INSTRUCTION_KEY, RESPONSE_KEY_NL]})\n","    return tokenizer\n","\n","def load_model(pretrained_model_name_or_path, gradient_checkpointing):\n","    model = AutoModelForCausalLM.from_pretrained(\n","        pretrained_model_name_or_path, trust_remote_code=True, use_cache=False if gradient_checkpointing else True)\n","    return model\n","\n","def get_model_tokenizer(\n","    pretrained_model_name_or_path, gradient_checkpointing):\n","    tokenizer = load_tokenizer(pretrained_model_name_or_path)\n","    model = load_model(pretrained_model_name_or_path, gradient_checkpointing=gradient_checkpointing)\n","    model.resize_token_embeddings(len(tokenizer))\n","    return model, tokenizer\n","\n","model, tokenizer = get_model_tokenizer(\n","    pretrained_model_name_or_path=INPUT_MODEL, \n","    gradient_checkpointing=True\n",")\n","\n","# find max length in model configuration\n","conf = model.config\n","max_length = getattr(model.config, \"max_position_embeddings\", None)"],"metadata":{"id":"TwKLbe8Df6AS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1.3 Data Processing"],"metadata":{"id":"qqJ5c0fMf6y-"}},{"cell_type":"code","source":["### Data Processing\n","INTRO_BLURB = (\n","    \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",")\n","PROMPT_NO_INPUT_FORMAT = \"\"\"{intro}\n","{instruction_key}\n","{instruction}\n","{response_key}\n","{response}\n","{end_key}\"\"\".format(\n","    intro=INTRO_BLURB,\n","    instruction_key=INSTRUCTION_KEY,\n","    instruction=\"{instruction}\",\n","    response_key=RESPONSE_KEY,\n","    response=\"{response}\",\n","    end_key=END_KEY,\n",")\n","\n","PROMPT_WITH_INPUT_FORMAT = \"\"\"{intro}\n","{instruction_key}\n","{instruction}\n","{input_key}\n","{input}\n","{response_key}\n","{response}\n","{end_key}\"\"\".format(\n","    intro=INTRO_BLURB,\n","    instruction_key=INSTRUCTION_KEY,\n","    instruction=\"{instruction}\",\n","    input_key=INPUT_KEY,\n","    input=\"{input}\",\n","    response_key=RESPONSE_KEY,\n","    response=\"{response}\",\n","    end_key=END_KEY,\n",")\n","\n","def load_training_dataset(path_or_dataset=\"databricks/databricks-dolly-15k\"):\n","    dataset = load_dataset(path_or_dataset)[\"train\"]\n","    def _add_text(rec):\n","        instruction = rec[\"instruction\"]\n","        response = rec[\"response\"]\n","        context = rec.get(\"context\")\n","        if context:\n","            rec[\"text\"] = PROMPT_WITH_INPUT_FORMAT.format(instruction=instruction, response=response, input=context)\n","        else:\n","            rec[\"text\"] = PROMPT_NO_INPUT_FORMAT.format(instruction=instruction, response=response)\n","        return rec\n","    dataset = dataset.map(_add_text)\n","    return dataset\n","\n","def preprocess_batch(batch, tokenizer, max_length):\n","    return tokenizer(\n","        batch[\"text\"],\n","        max_length=max_length,\n","        truncation=True,\n","    )\n","\n","def preprocess_dataset(tokenizer, max_length):\n","    dataset = load_training_dataset()\n","    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n","    dataset = dataset.map(\n","        _preprocessing_function,\n","        batched=True,\n","        remove_columns=[\"instruction\", \"context\", \"response\", \"text\", \"category\"],\n","    )\n","\n","    # Make sure we don't have any truncated records, as this would mean the end keyword is missing.\n","    dataset = dataset.filter(lambda rec: len(rec[\"input_ids\"]) < max_length)\n","    dataset = dataset.shuffle()\n","    return dataset\n","\n","class DataCollatorForCompletionOnlyLM(DataCollatorForLanguageModeling):\n","    def torch_call(self, examples):\n","        batch = super().torch_call(examples)\n","\n","        # The prompt ends with the response key plus a newline.  We encode this and then try to find it in the\n","        # sequence of tokens. This should just be a single token.\n","        response_token_ids = self.tokenizer.encode(RESPONSE_KEY_NL)\n","        labels = batch[\"labels\"].clone()\n","\n","        for i in range(len(examples)):\n","            response_token_ids_start_idx = None\n","            for idx in np.where(batch[\"labels\"][i] == response_token_ids[0])[0]:\n","                response_token_ids_start_idx = idx\n","                break\n","\n","            if response_token_ids_start_idx is None:\n","                raise RuntimeError(\n","                    f'Could not find response key {response_token_ids} in token IDs {batch[\"labels\"][i]}'\n","                )\n","\n","            response_token_ids_end_idx = response_token_ids_start_idx + 1\n","\n","            # Make pytorch loss function ignore all tokens up through the end of the response key\n","            labels[i, :response_token_ids_end_idx] = -100\n","\n","        batch[\"labels\"] = labels\n","\n","        return batch\n","\n","processed_dataset = preprocess_dataset(tokenizer=tokenizer, max_length=max_length)\n","split_dataset = processed_dataset.train_test_split(test_size=1000)\n","for k, v in next(iter(processed_dataset)).items():\n","    print(f\"{k}: {v} \\n\")\n","\n","data_collator = DataCollatorForCompletionOnlyLM(\n","        tokenizer=tokenizer, mlm=False, return_tensors=\"pt\", pad_to_multiple_of=8\n",")"],"metadata":{"id":"DzQSKkWtfR3p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1.4 Trainer"],"metadata":{"id":"sC5T9P5XgHaL"}},{"cell_type":"code","source":["local_output_dir = \"/logs/\"\n","\n","training_args = TrainingArguments(\n","        output_dir=local_output_dir,\n","        per_device_train_batch_size=4,\n","        per_device_eval_batch_size=4,\n","        fp16=False,\n","        bf16=False,\n","        learning_rate=1e-5,\n","        num_train_epochs=5,\n","        deepspeed=None,\n","        gradient_checkpointing=True,\n","        logging_dir=f\"{local_output_dir}/runs\",\n","        logging_strategy=\"steps\",\n","        logging_steps=10,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=500,\n","        save_strategy=\"steps\",\n","        save_steps=1000,\n","        save_total_limit=10,\n","        load_best_model_at_end=False,\n","        report_to=\"tensorboard\",\n","        disable_tqdm=True,\n","        remove_unused_columns=False,\n","        local_rank=2,\n","        warmup_steps=0,\n","    )\n","\n","trainer = Trainer(\n","    model=model,\n","    tokenizer=tokenizer,\n","    args=training_args,\n","    train_dataset=split_dataset[\"train\"],\n","    eval_dataset=split_dataset[\"test\"],\n","    data_collator=data_collator,\n",")\n","\n","trainer.train()\n","trainer.save_model(output_dir=local_output_dir)"],"metadata":{"id":"2vmvSq0ZfPYY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2) Generation"],"metadata":{"id":"0pqYHsxJgRWD"}},{"cell_type":"code","source":["PROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\n","{instruction_key}\n","{instruction}\n","{response_key}\n","\"\"\".format(\n","    intro=INTRO_BLURB,\n","    instruction_key=INSTRUCTION_KEY,\n","    instruction=\"{instruction}\",\n","    response_key=RESPONSE_KEY,\n",")\n","\n","def preprocess(tokenizer, instruction_text):\n","    prompt_text = PROMPT_FOR_GENERATION_FORMAT.format(instruction=instruction_text)\n","    inputs = tokenizer(prompt_text, return_tensors=\"pt\",)\n","    inputs[\"prompt_text\"] = prompt_text\n","    inputs[\"instruction_text\"] = instruction_text\n","    return inputs\n","\n","def forward(model, tokenizer, model_inputs, max_length=100):\n","    input_ids = model_inputs[\"input_ids\"]\n","    attention_mask = model_inputs.get(\"attention_mask\", None)\n","\n","    if input_ids.shape[1] == 0:\n","        input_ids = None\n","        attention_mask = None\n","        in_b = 1\n","    else:\n","        in_b = input_ids.shape[0]\n","\n","    generated_sequence = model.generate(\n","        input_ids=input_ids.to(model.device),\n","        attention_mask=attention_mask,\n","        pad_token_id=tokenizer.pad_token_id,\n","        max_length=max_length\n","    )\n","\n","    out_b = generated_sequence.shape[0]\n","    generated_sequence = generated_sequence.reshape(in_b, out_b // in_b, *generated_sequence.shape[1:])\n","    instruction_text = model_inputs.get(\"instruction_text\", None)\n","    return {\"generated_sequence\": generated_sequence, \"input_ids\": input_ids, \"instruction_text\": instruction_text}\n","\n","text = \"Give me best 30s advice\"\n","pre_process_result = preprocess(tokenizer, text)\n","print(pre_process_result[\"input_ids\"])\n","print(pre_process_result[\"prompt_text\"])\n","model_result = forward(model, tokenizer, pre_process_result)"],"metadata":{"id":"2olbg2_ofPjV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# post processing\n","def get_special_token_id(tokenizer, key):\n","    \"\"\"Gets the token ID for a given string that has been added to the tokenizer as a special token.\n","    When training, we configure the tokenizer so that the sequences like \"### Instruction:\" and \"### End\" are\n","    treated specially and converted to a single, new token.  This retrieves the token ID each of these keys map to.\n","    \"\"\"\n","    token_ids = tokenizer.encode(key)\n","    if len(token_ids) > 1:\n","        raise ValueError(f\"Expected only a single token for '{key}' but found {token_ids}\")\n","    return token_ids[0]\n","\n","def postprocess(tokenizer, model_outputs, return_full_text=False):\n","\n","    response_key_token_id = get_special_token_id(tokenizer, RESPONSE_KEY_NL)\n","    end_key_token_id = get_special_token_id(tokenizer, END_KEY)\n","    generated_sequence = model_outputs[\"generated_sequence\"][0]\n","    instruction_text = model_outputs[\"instruction_text\"]\n","    generated_sequence = generated_sequence.numpy().tolist()\n","    records = []\n","\n","    print(response_key_token_id, end_key_token_id)\n","\n","    for sequence in generated_sequence:\n","        # The response will be set to this variable if we can identify it.\n","        decoded = None\n","\n","        # Find where \"### Response:\" is first found in the generated tokens.  Considering this is part of the\n","        # prompt, we should definitely find it.  We will return the tokens found after this token.\n","        try:\n","            response_pos = sequence.index(response_key_token_id)\n","        except ValueError:\n","            logger.warn(f\"Could not find response key {response_key_token_id} in: {sequence}\")\n","            response_pos = None\n","\n","        if response_pos:\n","            # Next find where \"### End\" is located.  The model has been trained to end its responses with this\n","            # sequence (or actually, the token ID it maps to, since it is a special token).  We may not find\n","            # this token, as the response could be truncated.  If we don't find it then just return everything\n","            # to the end.  Note that even though we set eos_token_id, we still see the this token at the end.\n","            try:\n","                end_pos = sequence.index(end_key_token_id)\n","            except ValueError:\n","                logger.warning(f\"Could not find end key, the output is truncated!\")\n","                end_pos = None\n","            decoded = tokenizer.decode(sequence[response_pos + 1 : end_pos]).strip()\n","\n","        # If the full text is requested, then append the decoded text to the original instruction.\n","        if return_full_text:\n","            decoded = f\"{instruction_text}\\n{decoded}\"\n","        rec = {\"generated_text\": decoded}\n","        records.append(rec)\n","    return records\n","\n","final_output = postprocess(tokenizer, model_result, False)\n","print(final_output)"],"metadata":{"id":"HrIiJxVWgh5f"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}