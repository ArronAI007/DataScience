{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"CFuVkUrU5rO8"},"outputs":[],"source":["!pip install transformers datasets peft accelerate bitsandbytes safetensors sentencepiece --upgrade"]},{"cell_type":"markdown","source":["\"daryl149/llama-2-7b-chat-hf\"\n","p3.8xlarge\n","max_length: 128\n","\n","- vanila: 10.22s\n","\n","- tf32 instead of fp32: 10.38s\n","```python\n","torch.backends.cuda.matmul.allow_tf32 = True\n","```\n","\n","- half-precision: 10.87s (save memory)\n","```python\n","torch_dtype=torch.bfloat16,\n","```\n","\n","- load int 8: 33.92s\n","```python\n","load_in_8bit=True,\n","```"],"metadata":{"id":"edIjfMXH3f60"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"bDV8EOMnTaDf"},"outputs":[],"source":["import os, sys, time\n","import torch\n","import datasets\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForCausalLM,\n","    LlamaTokenizer,\n","    LlamaForCausalLM,\n","    BitsAndBytesConfig,\n","    DataCollatorForLanguageModeling,\n","    DataCollatorForSeq2Seq,\n","    Trainer,\n","    TrainingArguments,\n","    GenerationConfig\n",")\n","from peft import PeftModel, LoraConfig, prepare_model_for_kbit_training, get_peft_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CUcPgOPQOHv1"},"outputs":[],"source":["def print_number_of_trainable_model_parameters(model):\n","    trainable_model_params = 0\n","    all_model_params = 0\n","    for _, param in model.named_parameters():\n","        all_model_params += param.numel()\n","        if param.requires_grad:\n","            trainable_model_params += param.numel()\n","    print(f\"trainable model parameters: {trainable_model_params}\\n all model parameters: {all_model_params} \")\n","    return trainable_model_params"]},{"cell_type":"markdown","metadata":{"id":"2onbcr2-TPLG"},"source":["# 1) Model Loading\n","\n","[Fine-tuning a GPT â€” LoRA](https://dataman-ai.medium.com/fine-tune-a-gpt-lora-e9b72ad4ad3)\n","\n","[LLM-Trainer](https://github.com/mshumer/gpt-llm-trainer/blob/main/One_Prompt___Fine_Tuned_LLaMA_2.ipynb)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":85,"referenced_widgets":["26ad8ffcf92548f4a269fba254279516","0784e9eecfb44f668ff2c4b9480275e5","65d08ef5fa6245718a760cd0713c1eef","d1e97d72cf7b4e8eb28350efd077c2e4","83df6dc432a7403b8ad48d0fe78e7954","465a4c026f484310baca06d5dd1df7b9","53d5d3c50bef4bd7b1e849caf1f19427","8aaffadad0bb49169f91d53b61e6c560","de22628b7a6f4b28a62121f0c384ac61","21a989168dae462ba8d629dca3e43293","f03faf7671c14a47bf290bd460603196"]},"executionInfo":{"elapsed":87453,"status":"ok","timestamp":1691590300370,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"},"user_tz":-480},"id":"diAhkf7t_C_y","outputId":"77f738ef-3781-4438-9159-8f71b7b24e6f"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26ad8ffcf92548f4a269fba254279516"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["trainable model parameters: 262410240\n"," all model parameters: 3500412928 \n"]}],"source":["model_id = \"NousResearch/Llama-2-7b-hf\"\n","# model_id = \"daryl149/llama-2-7b-chat-hf\"\n","max_length = 256\n","\n","### device set up\n","device_map = \"auto\"\n","\n","batch_size = 64\n","micro_batch_size = 16\n","gradient_accumulation_steps = batch_size // micro_batch_size\n","world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n","ddp = world_size != 1\n","if ddp:\n","    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n","    gradient_accumulation_steps = gradient_accumulation_steps // world_size\n","\n","# nf4\" use a symmetric quantization scheme with 4 bits precision\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","model = LlamaForCausalLM.from_pretrained(\n","    model_id,\n","    torch_dtype=torch.float16,\n","    quantization_config=bnb_config,\n","    device_map=\"auto\"\n",")\n","\n","# value different than 1 will activate the more accurate but slower computation\n","model.config.pretraining_tp = 1\n","\n","if not ddp and torch.cuda.device_count() > 1:\n","    model.is_parallelizable = True\n","    model.model_parallel = True\n","\n","ori_p = print_number_of_trainable_model_parameters(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9FNn7C9oAgRk"},"outputs":[],"source":["### tokenizer\n","tokenizer = LlamaTokenizer.from_pretrained(model_id)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\""]},{"cell_type":"markdown","metadata":{"id":"fEjtE49Shcfj"},"source":["```python\n","for param in model.parameters():\n","  param.requires_grad = False  # freeze the model - train adapters later\n","  if param.ndim == 1:\n","    # cast the small parameters (e.g. layernorm) to fp32 for stability\n","    param.data = param.data.to(torch.float32)\n","\n","# reduce number of stored activations\n","model.gradient_checkpointing_enable()\n","model.enable_input_require_grads()\n","\n","class CastOutputToFloat(nn.Sequential):\n","  def forward(self, x): return super().forward(x).to(torch.float32)\n","model.lm_head = CastOutputToFloat(model.lm_head)\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12001,"status":"ok","timestamp":1691590318076,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"},"user_tz":-480},"id":"SB1cu2ZL_HzE","outputId":"c2e868a7-9f3b-44d2-a805-a48fb80a38ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["trainable model parameters: 4194304\n"," all model parameters: 3504607232 \n","# Trainable Parameter \n","Before: 262410240 \n","After: 4194304 \n","Percentage: 1.6\n"]}],"source":["# this line is similar to the block above\n","model = prepare_model_for_kbit_training(model)\n","# LoRA config based on QLoRA paper\n","peft_config = LoraConfig(\n","    r=8,\n","    lora_alpha=32,\n","    lora_dropout=0.1,\n","    target_modules=[\"q_proj\", \"v_proj\"],\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","model = get_peft_model(model, peft_config)\n","peft_p = print_number_of_trainable_model_parameters(model)\n","print(f\"# Trainable Parameter \\nBefore: {ori_p} \\nAfter: {peft_p} \\nPercentage: {round(peft_p / ori_p * 100, 2)}\")"]},{"cell_type":"markdown","metadata":{"id":"s1RGmVPHVDCY"},"source":["# 2) Data Loading\n","\n"]},{"cell_type":"code","source":["# Load datasets\n","# train_dataset = load_dataset('json', data_files='/content/train.jsonl', split=\"train\")\n","# valid_dataset = load_dataset('json', data_files='/content/test.jsonl', split=\"train\")\n","\n","# Preprocess datasets\n","# train_dataset_mapped = train_dataset.map(\n","#     lambda examples: {\n","#         'text': [f'[INST] <>\\n{system_message.strip()}\\n<>\\n\\n' + prompt + ' [/INST] ' + response \\\n","#                  for prompt, response in zip(examples['prompt'], examples['response'])]\n","#         },\n","#     batched=True\n","#     )"],"metadata":{"id":"ais6ZnDuBM5W"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"csckMzN7jOxL"},"outputs":[],"source":["### generate prompt based on template ###\n","prompt_template = {\n","    \"prompt_input\": \\\n","    \"Below is an instruction that describes a task, paired with an input that provides further context.\\\n","    Write a response that appropriately completes the request.\\\n","    \\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\",\n","\n","    \"prompt_no_input\": \\\n","    \"Below is an instruction that describes a task.\\\n","    Write a response that appropriately completes the request.\\\n","    \\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n\",\n","\n","    \"response_split\": \"### Response:\"\n","}\n","\n","def generate_prompt(instruction, input=None, label=None, prompt_template=prompt_template):\n","    if input:\n","        res = prompt_template[\"prompt_input\"].format(\n","            instruction=instruction, input=input)\n","    else:\n","        res = prompt_template[\"prompt_no_input\"].format(\n","            instruction=instruction)\n","    if label:\n","        res = f\"{res}{label}\"\n","    return res"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UHSOPz1pjI7w"},"outputs":[],"source":["def tokenize(tokenizer, prompt, max_length=max_length, add_eos_token=False):\n","    result = tokenizer(\n","        prompt,\n","        truncation=True,\n","        max_length=max_length,\n","        padding=False,\n","        return_tensors=None)\n","\n","    # result[\"input_ids\"].append(tokenizer.eos_token_id)\n","    # result[\"attention_mask\"].append(1)\n","    result[\"labels\"] = result[\"input_ids\"].copy()\n","    return result\n","\n","def generate_and_tokenize_prompt(data_point):\n","    full_prompt = generate_prompt(\n","        data_point[\"instruction\"],\n","        data_point[\"context\"],\n","        data_point[\"response\"],\n","    )\n","    tokenized_full_prompt = tokenize(tokenizer, full_prompt)\n","    user_prompt = generate_prompt(data_point[\"instruction\"], data_point[\"context\"])\n","    tokenized_user_prompt = tokenize(tokenizer, user_prompt)\n","    user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n","    mask_token = [-100] * user_prompt_len\n","    tokenized_full_prompt[\"labels\"] = mask_token + tokenized_full_prompt[\"labels\"][user_prompt_len:]\n","\n","    return tokenized_full_prompt"]},{"cell_type":"markdown","metadata":{"id":"S9QFhyuIhRMk"},"source":["```python\n","class DataCollatorForSeq2Seq:\n","    def __init__(self, tokenizer, pad_to_multiple_of=None):\n","        self.tokenizer = tokenizer\n","        self.pad_to_multiple_of = pad_to_multiple_of\n","\n","    def __call__(self, examples):\n","        input_ids = [example[\"input_ids\"] for example in examples]\n","        attention_mask = [example[\"attention_mask\"] for example in examples]\n","        labels = [example[\"labels\"] for example in examples]\n","\n","        # Pad the sequences to the maximum length in the batch\n","        max_length = max(len(seq) for seq in input_ids)\n","        input_ids = [seq + [self.tokenizer.pad_token_id] * (max_length - len(seq)) for seq in input_ids]\n","        attention_mask = [seq + [0] * (max_length - len(seq)) for seq in attention_mask]\n","        labels = [seq + [-100] * (max_length - len(seq)) for seq in labels]\n","\n","        # Convert the lists to PyTorch tensors\n","        input_ids = torch.tensor(input_ids)\n","        attention_mask = torch.tensor(attention_mask)\n","        labels = torch.tensor(labels)\n","\n","        # Pad the sequences to the multiple of `pad_to_multiple_of` if specified\n","        if self.pad_to_multiple_of is not None:\n","            pad_length = self.pad_to_multiple_of - (input_ids.size(1) % self.pad_to_multiple_of)\n","            if pad_length != self.pad_to_multiple_of:\n","                input_ids = F.pad(input_ids, (0, pad_length), value=self.tokenizer.pad_token_id)\n","                attention_mask = F.pad(attention_mask, (0, pad_length), value=0)\n","                labels = F.pad(labels, (0, pad_length), value=-100)\n","\n","        return {\n","            \"input_ids\": input_ids,\n","            \"attention_mask\": attention_mask,\n","            \"labels\": labels,\n","        }\n","\n","```"]},{"cell_type":"markdown","metadata":{"id":"aY7OYi5fhmLh"},"source":["```python\n","class DataCollatorForLanguageModeling:\n","    def __init__(self, tokenizer, mlm_probability=0.15):\n","        self.tokenizer = tokenizer\n","        self.mlm_probability = mlm_probability\n","\n","    def __call__(self, examples):\n","        input_ids = [example[\"input_ids\"] for example in examples]\n","        attention_mask = [example[\"attention_mask\"] for example in examples]\n","        labels = [example[\"labels\"] for example in examples]\n","\n","        # Mask tokens for masked language modeling\n","        for i in range(len(input_ids)):\n","            for j in range(len(input_ids[i])):\n","                if random.random() < self.mlm_probability:\n","                    # 80% of the time, replace with [MASK] token\n","                    if random.random() < 0.8:\n","                        input_ids[i][j] = self.tokenizer.mask_token_id\n","                    # 10% of the time, replace with a random token\n","                    elif random.random() < 0.5:\n","                        input_ids[i][j] = random.randint(0, len(self.tokenizer) - 1)\n","                    # 10% of the time, keep the original token\n","\n","        # Convert the lists to PyTorch tensors\n","        input_ids = torch.tensor(input_ids)\n","        attention_mask = torch.tensor(attention_mask)\n","        labels = torch.tensor(labels)\n","\n","        return {\n","            \"input_ids\": input_ids,\n","            \"attention_mask\": attention_mask,\n","            \"labels\": labels,\n","        }\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["ac6ebdb8e0ba45528817c6123dcb3eec","4ab93ffb178c497e9453d87440125c30","6e8f94aacad84a648bc2b65fe563fbbf","42dbaef0555545fbab3cfd2b8d7a950e","cea51bd4c09f41019a3972bee04e9c8a","b82738a627e9476a935740a38402295d","60150903d2524a9e8bdf57eedbf2fd66","2c15bcdca52b46039c6d3b7b5f6f00ea","3c6359bf23544dfa8f5a21bcfd7dd1d9","2d4e6169b6784fecb3d1d430c6c07fdd","1e1d4ed6192042d49333abf4bc55fa66","ee7d2e5be90c47db939b6eafb5c249e8","a7ad4cdb21474e97b704242cf7ed3f51","7572399cc37e4ce3b91dd188dd746502","a9ad99f264fe4fd5b806ddb6b4c375da","e4a6f2221a93475bad9ac7ebdd65ee91","ffcfcc6de1fa4a87a35ad905f0e83225","40b979c61efe44cdb6fc6038ea5cdc51","bf40fbbf0e724324adffec3776bf5ce0","1018c4cbfde54113a2bf41957cb945d3","377ce664ff6b4b6f966505a4e1f9ba51","d6de7c7074aa494186f5467da3443173","30e6cde0f5fc4382994eb01a0773e3c9","ef32de53c2874c81895296259214006b","c4e8dc41896441bb92b5b2db7590bdc2","86615f4c53b84054b5fef41718d1f946","d11480a43c1c4217ba26db7e64445aed","c1d9cbee75b1429da089b991555df344","7885d0904959451b82fed0611a58ddb7","84e711a7d5914ba68822bbca9a4bc1ee","d9550bd7bfab4369ac9cd4fe1298582b","0f0476f8d0234da6ad02acce1f6251ec","5ae0b67e33ba483e9438ef6a0dc56399","fb4d9b7123c14c1f82680f446c2c9b0b","fc99349736364bcda836c75b45c4e510","a1e423520fb844df90b117c6678d1e11","9f243c84fba34e0d97bc3e886cde95aa","311ad81bc1204beabe2ea7938bf3fbb5","8743b3489097443dbb5417cb96370847","954651a255c64fd893324585178d61ad","f6d874ab2b0b4194aca123ea8c65f414","fb96c5466862419c9a6d60b0941efc9e","9a6686236a674d279bd646f2bf9b38f1","b6de5e70d92d4f41892670a5489e8de8"]},"executionInfo":{"elapsed":36769,"status":"ok","timestamp":1690799093667,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"},"user_tz":-480},"id":"5L4KewXyfHLv","outputId":"ebd2630b-6417-4f41-9749-cebc7de7d23e"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ac6ebdb8e0ba45528817c6123dcb3eec","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/14011 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ee7d2e5be90c47db939b6eafb5c249e8","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/14011 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"30e6cde0f5fc4382994eb01a0773e3c9","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fb4d9b7123c14c1f82680f446c2c9b0b","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"name":"stdout","output_type":"stream","text":["input_ids tensor([    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","            1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29889,\n","         1678, 14350,   263,  2933,   393,  7128,  2486,  1614,  2167,   278,\n","         2009, 29889,   268,    13,    13,  2277, 29937,  2799,  4080, 29901,\n","           13,  5328,   437,   366,  1246,   278, 19444, 13353, 24961,   272,\n","         1627, 29879, 29973,    13,    13,  2277, 29937, 13291, 29901,    13,\n","         1576,  5375,  8665,   411,  1009, 10188, 16116,   287,   714,  2978,\n","        11324,   391, 29892,   322, 14205, 12020,   963,  1550,  2599, 21230,\n","        23915,   599,  1550,  5934,   376,   827,  3634,  3634,  3634, 29908,\n","          769,  2748,   596, 10188,   526,  2038,   596,  2343,   366,  1827,\n","          376, 29886,   335,   577, 29877,   347,  1213, 29871,  2538,   300,\n","          596,  6567,   304,   596, 11324,   391,   322,  1023,   901,  3064,\n","        29892,   541,   373,   278,  4654,   931, 29892,  1156,   366,  1827,\n","          376, 29886,   335,   577, 29877,   347, 29908,   366,   884,  1827,\n","          376,  9504,   272,  1627, 29879,  3850])\n","attention_mask tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n","labels tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         1576,  5375,  8665,   411,  1009, 10188, 16116,   287,   714,  2978,\n","        11324,   391, 29892,   322, 14205, 12020,   963,  1550,  2599, 21230,\n","        23915,   599,  1550,  5934,   376,   827,  3634,  3634,  3634, 29908,\n","          769,  2748,   596, 10188,   526,  2038,   596,  2343,   366,  1827,\n","          376, 29886,   335,   577, 29877,   347,  1213, 29871,  2538,   300,\n","          596,  6567,   304,   596, 11324,   391,   322,  1023,   901,  3064,\n","        29892,   541,   373,   278,  4654,   931, 29892,  1156,   366,  1827,\n","          376, 29886,   335,   577, 29877,   347, 29908,   366,   884,  1827,\n","          376,  9504,   272,  1627, 29879,  3850])\n","\n","Decoding input_ids\n"," </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> Below is an instruction that describes a task.    Write a response that appropriately completes the request.    \n","\n","### Instruction:\n","How do you call the Arkansas Razorbacks?\n","\n","### Response:\n","The individual starts with their arms stretched out near waist, and slowly raise them while doing jazz fingers all while saying \"wooooooo\" then once your arms are above your head you say \"pig sooie.\"  Reset your hands to your waist and two more times, but on the third time, after you say \"pig sooie\" you also say \"razorbacks!\"\n","\n","Decoding labels\n"," The individual starts with their arms stretched out near waist, and slowly raise them while doing jazz fingers all while saying \"wooooooo\" then once your arms are above your head you say \"pig sooie.\"  Reset your hands to your waist and two more times, but on the third time, after you say \"pig sooie\" you also say \"razorbacks!\"\n"]}],"source":["# Use the end-of-sequence token as the padding token and set mlm=False. \\\n","# This will use the inputs as labels shifted to the right by one element:\n","\n","max_length = 256\n","dataset = datasets.load_dataset(\n","    \"databricks/databricks-dolly-15k\", split='train'\n",")\n","\n","data_collator = DataCollatorForSeq2Seq(\n","    tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True)\n","\n","# data_collator = DataCollatorForLanguageModeling(\n","#     tokenizer, mlm=False, pad_to_multiple_of=8, return_tensors=\"pt\",\n","# )\n","\n","# print(f\"dataset size: {len(dataset)}\")\n","dataset = dataset.train_test_split(test_size=1000, shuffle=True, seed=42)\n","cols = [\"instruction\", \"context\", \"response\", \"category\"]\n","train_data = dataset[\"train\"].shuffle().map(generate_and_tokenize_prompt, remove_columns=cols,)\n","train_data = train_data.filter(lambda rec: len(rec[\"input_ids\"]) < max_length)\n","val_data = dataset[\"test\"].shuffle().map(generate_and_tokenize_prompt, remove_columns=cols,)\n","val_data = val_data.filter(lambda rec: len(rec[\"input_ids\"]) < max_length)\n","\n","# test collator\n","val_batch = data_collator(list(iter(val_data)))\n","n = 100\n","for k, v in val_batch.items():\n","    print(k, v[n])\n","\n","print('\\nDecoding input_ids\\n', tokenizer.decode(val_batch['input_ids'][n]))\n","print('\\nDecoding labels\\n', tokenizer.decode([x for x in val_batch['labels'][n] if x > 0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jBK9qPGw4nAV"},"outputs":[],"source":["# for c, i in enumerate(val_batch['input_ids'][n]):\n","#     if i.item() == 29902:\n","#         print(c, i)\n","#         break\n","\n","# for c, i in enumerate(val_batch['labels'][n]):\n","#     if i == 29902:\n","#         print(c, i)\n","#         break"]},{"cell_type":"markdown","metadata":{"id":"labeeEjQfLFx"},"source":["# 3) Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"V5v_04ApIgJ6","outputId":"8702c1ea-97b6-4247-fa84-0ad71c96d3d4"},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='308' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 308/1000 29:52 < 1:07:33, 0.17 it/s, Epoch 0.12/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.023500</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.746400</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.079100</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>2.189000</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>1.395400</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>1.657700</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>4.624800</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.632600</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.397100</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.310500</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>0.876000</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>1.221700</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>0.305800</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>2.982000</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>4.016700</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.720900</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>3.822300</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>0.545900</td>\n","    </tr>\n","    <tr>\n","      <td>37</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>39</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>41</td>\n","      <td>1.332700</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>43</td>\n","      <td>1.706300</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>0.508200</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>1.889600</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>0.398300</td>\n","    </tr>\n","    <tr>\n","      <td>47</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>49</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>1.516400</td>\n","    </tr>\n","    <tr>\n","      <td>51</td>\n","      <td>0.302200</td>\n","    </tr>\n","    <tr>\n","      <td>52</td>\n","      <td>0.203800</td>\n","    </tr>\n","    <tr>\n","      <td>53</td>\n","      <td>1.963000</td>\n","    </tr>\n","    <tr>\n","      <td>54</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.126200</td>\n","    </tr>\n","    <tr>\n","      <td>56</td>\n","      <td>1.435900</td>\n","    </tr>\n","    <tr>\n","      <td>57</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>58</td>\n","      <td>1.505900</td>\n","    </tr>\n","    <tr>\n","      <td>59</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>61</td>\n","      <td>1.279600</td>\n","    </tr>\n","    <tr>\n","      <td>62</td>\n","      <td>1.281900</td>\n","    </tr>\n","    <tr>\n","      <td>63</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>64</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.264600</td>\n","    </tr>\n","    <tr>\n","      <td>66</td>\n","      <td>0.133400</td>\n","    </tr>\n","    <tr>\n","      <td>67</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>68</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>69</td>\n","      <td>1.463300</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.932100</td>\n","    </tr>\n","    <tr>\n","      <td>71</td>\n","      <td>4.574100</td>\n","    </tr>\n","    <tr>\n","      <td>72</td>\n","      <td>1.906100</td>\n","    </tr>\n","    <tr>\n","      <td>73</td>\n","      <td>0.289700</td>\n","    </tr>\n","    <tr>\n","      <td>74</td>\n","      <td>1.509500</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.290500</td>\n","    </tr>\n","    <tr>\n","      <td>76</td>\n","      <td>0.146400</td>\n","    </tr>\n","    <tr>\n","      <td>77</td>\n","      <td>0.478500</td>\n","    </tr>\n","    <tr>\n","      <td>78</td>\n","      <td>1.343100</td>\n","    </tr>\n","    <tr>\n","      <td>79</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.884800</td>\n","    </tr>\n","    <tr>\n","      <td>81</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>82</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>83</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>84</td>\n","      <td>1.843900</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.581300</td>\n","    </tr>\n","    <tr>\n","      <td>86</td>\n","      <td>2.524400</td>\n","    </tr>\n","    <tr>\n","      <td>87</td>\n","      <td>0.712500</td>\n","    </tr>\n","    <tr>\n","      <td>88</td>\n","      <td>1.538400</td>\n","    </tr>\n","    <tr>\n","      <td>89</td>\n","      <td>0.547500</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>3.014600</td>\n","    </tr>\n","    <tr>\n","      <td>91</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>92</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>93</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>94</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.151200</td>\n","    </tr>\n","    <tr>\n","      <td>96</td>\n","      <td>1.242000</td>\n","    </tr>\n","    <tr>\n","      <td>97</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>98</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>99</td>\n","      <td>0.104300</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>101</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>102</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>103</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>104</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>105</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>106</td>\n","      <td>0.988200</td>\n","    </tr>\n","    <tr>\n","      <td>107</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>108</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>109</td>\n","      <td>0.291900</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>1.492500</td>\n","    </tr>\n","    <tr>\n","      <td>111</td>\n","      <td>0.020800</td>\n","    </tr>\n","    <tr>\n","      <td>112</td>\n","      <td>1.119800</td>\n","    </tr>\n","    <tr>\n","      <td>113</td>\n","      <td>1.685400</td>\n","    </tr>\n","    <tr>\n","      <td>114</td>\n","      <td>0.233100</td>\n","    </tr>\n","    <tr>\n","      <td>115</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>116</td>\n","      <td>0.119100</td>\n","    </tr>\n","    <tr>\n","      <td>117</td>\n","      <td>0.449900</td>\n","    </tr>\n","    <tr>\n","      <td>118</td>\n","      <td>0.225600</td>\n","    </tr>\n","    <tr>\n","      <td>119</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.095800</td>\n","    </tr>\n","    <tr>\n","      <td>121</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>122</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>123</td>\n","      <td>0.850000</td>\n","    </tr>\n","    <tr>\n","      <td>124</td>\n","      <td>3.175300</td>\n","    </tr>\n","    <tr>\n","      <td>125</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>126</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>127</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>128</td>\n","      <td>2.643400</td>\n","    </tr>\n","    <tr>\n","      <td>129</td>\n","      <td>0.493800</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>3.090700</td>\n","    </tr>\n","    <tr>\n","      <td>131</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>132</td>\n","      <td>0.089500</td>\n","    </tr>\n","    <tr>\n","      <td>133</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>134</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>135</td>\n","      <td>2.248800</td>\n","    </tr>\n","    <tr>\n","      <td>136</td>\n","      <td>0.365300</td>\n","    </tr>\n","    <tr>\n","      <td>137</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>138</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>139</td>\n","      <td>1.814100</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>2.082900</td>\n","    </tr>\n","    <tr>\n","      <td>141</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>142</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>143</td>\n","      <td>0.176500</td>\n","    </tr>\n","    <tr>\n","      <td>144</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>145</td>\n","      <td>4.674700</td>\n","    </tr>\n","    <tr>\n","      <td>146</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>147</td>\n","      <td>0.333500</td>\n","    </tr>\n","    <tr>\n","      <td>148</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>149</td>\n","      <td>0.606100</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>151</td>\n","      <td>0.190900</td>\n","    </tr>\n","    <tr>\n","      <td>152</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>153</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>154</td>\n","      <td>0.521300</td>\n","    </tr>\n","    <tr>\n","      <td>155</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>156</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>157</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>158</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>159</td>\n","      <td>0.430900</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>1.832500</td>\n","    </tr>\n","    <tr>\n","      <td>161</td>\n","      <td>0.421800</td>\n","    </tr>\n","    <tr>\n","      <td>162</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>163</td>\n","      <td>0.499800</td>\n","    </tr>\n","    <tr>\n","      <td>164</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>165</td>\n","      <td>2.294200</td>\n","    </tr>\n","    <tr>\n","      <td>166</td>\n","      <td>0.421100</td>\n","    </tr>\n","    <tr>\n","      <td>167</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>168</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>169</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>171</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>172</td>\n","      <td>2.135100</td>\n","    </tr>\n","    <tr>\n","      <td>173</td>\n","      <td>3.324700</td>\n","    </tr>\n","    <tr>\n","      <td>174</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>175</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>176</td>\n","      <td>0.063300</td>\n","    </tr>\n","    <tr>\n","      <td>177</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>178</td>\n","      <td>3.841000</td>\n","    </tr>\n","    <tr>\n","      <td>179</td>\n","      <td>3.532300</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>0.061000</td>\n","    </tr>\n","    <tr>\n","      <td>181</td>\n","      <td>2.287200</td>\n","    </tr>\n","    <tr>\n","      <td>182</td>\n","      <td>1.202200</td>\n","    </tr>\n","    <tr>\n","      <td>183</td>\n","      <td>0.542800</td>\n","    </tr>\n","    <tr>\n","      <td>184</td>\n","      <td>0.826400</td>\n","    </tr>\n","    <tr>\n","      <td>185</td>\n","      <td>4.030000</td>\n","    </tr>\n","    <tr>\n","      <td>186</td>\n","      <td>1.298900</td>\n","    </tr>\n","    <tr>\n","      <td>187</td>\n","      <td>0.251400</td>\n","    </tr>\n","    <tr>\n","      <td>188</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>189</td>\n","      <td>0.549300</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>0.044600</td>\n","    </tr>\n","    <tr>\n","      <td>191</td>\n","      <td>1.292400</td>\n","    </tr>\n","    <tr>\n","      <td>192</td>\n","      <td>0.226400</td>\n","    </tr>\n","    <tr>\n","      <td>193</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>194</td>\n","      <td>2.371100</td>\n","    </tr>\n","    <tr>\n","      <td>195</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>196</td>\n","      <td>1.255700</td>\n","    </tr>\n","    <tr>\n","      <td>197</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>198</td>\n","      <td>0.420200</td>\n","    </tr>\n","    <tr>\n","      <td>199</td>\n","      <td>0.459800</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.382700</td>\n","    </tr>\n","    <tr>\n","      <td>201</td>\n","      <td>0.629700</td>\n","    </tr>\n","    <tr>\n","      <td>202</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>203</td>\n","      <td>2.830200</td>\n","    </tr>\n","    <tr>\n","      <td>204</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>205</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>206</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>207</td>\n","      <td>1.558400</td>\n","    </tr>\n","    <tr>\n","      <td>208</td>\n","      <td>4.600600</td>\n","    </tr>\n","    <tr>\n","      <td>209</td>\n","      <td>0.203300</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>1.177300</td>\n","    </tr>\n","    <tr>\n","      <td>211</td>\n","      <td>0.353400</td>\n","    </tr>\n","    <tr>\n","      <td>212</td>\n","      <td>1.744800</td>\n","    </tr>\n","    <tr>\n","      <td>213</td>\n","      <td>1.135300</td>\n","    </tr>\n","    <tr>\n","      <td>214</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>215</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>216</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>217</td>\n","      <td>1.303000</td>\n","    </tr>\n","    <tr>\n","      <td>218</td>\n","      <td>0.126500</td>\n","    </tr>\n","    <tr>\n","      <td>219</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>3.478100</td>\n","    </tr>\n","    <tr>\n","      <td>221</td>\n","      <td>0.447200</td>\n","    </tr>\n","    <tr>\n","      <td>222</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>223</td>\n","      <td>2.943900</td>\n","    </tr>\n","    <tr>\n","      <td>224</td>\n","      <td>1.845600</td>\n","    </tr>\n","    <tr>\n","      <td>225</td>\n","      <td>0.240000</td>\n","    </tr>\n","    <tr>\n","      <td>226</td>\n","      <td>1.651800</td>\n","    </tr>\n","    <tr>\n","      <td>227</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>228</td>\n","      <td>1.937200</td>\n","    </tr>\n","    <tr>\n","      <td>229</td>\n","      <td>1.470700</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>0.082100</td>\n","    </tr>\n","    <tr>\n","      <td>231</td>\n","      <td>2.248600</td>\n","    </tr>\n","    <tr>\n","      <td>232</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>233</td>\n","      <td>1.362400</td>\n","    </tr>\n","    <tr>\n","      <td>234</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>235</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>236</td>\n","      <td>0.200700</td>\n","    </tr>\n","    <tr>\n","      <td>237</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>238</td>\n","      <td>0.580600</td>\n","    </tr>\n","    <tr>\n","      <td>239</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>241</td>\n","      <td>0.062300</td>\n","    </tr>\n","    <tr>\n","      <td>242</td>\n","      <td>2.799900</td>\n","    </tr>\n","    <tr>\n","      <td>243</td>\n","      <td>1.762400</td>\n","    </tr>\n","    <tr>\n","      <td>244</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>245</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>246</td>\n","      <td>0.235000</td>\n","    </tr>\n","    <tr>\n","      <td>247</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>248</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>249</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.288200</td>\n","    </tr>\n","    <tr>\n","      <td>251</td>\n","      <td>0.882400</td>\n","    </tr>\n","    <tr>\n","      <td>252</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>253</td>\n","      <td>0.951800</td>\n","    </tr>\n","    <tr>\n","      <td>254</td>\n","      <td>0.200000</td>\n","    </tr>\n","    <tr>\n","      <td>255</td>\n","      <td>0.026000</td>\n","    </tr>\n","    <tr>\n","      <td>256</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>257</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>258</td>\n","      <td>0.831500</td>\n","    </tr>\n","    <tr>\n","      <td>259</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>5.131800</td>\n","    </tr>\n","    <tr>\n","      <td>261</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>262</td>\n","      <td>0.527100</td>\n","    </tr>\n","    <tr>\n","      <td>263</td>\n","      <td>1.428800</td>\n","    </tr>\n","    <tr>\n","      <td>264</td>\n","      <td>0.730000</td>\n","    </tr>\n","    <tr>\n","      <td>265</td>\n","      <td>0.830600</td>\n","    </tr>\n","    <tr>\n","      <td>266</td>\n","      <td>0.377700</td>\n","    </tr>\n","    <tr>\n","      <td>267</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>268</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>269</td>\n","      <td>0.861900</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>1.337400</td>\n","    </tr>\n","    <tr>\n","      <td>271</td>\n","      <td>0.715800</td>\n","    </tr>\n","    <tr>\n","      <td>272</td>\n","      <td>0.187700</td>\n","    </tr>\n","    <tr>\n","      <td>273</td>\n","      <td>0.661300</td>\n","    </tr>\n","    <tr>\n","      <td>274</td>\n","      <td>1.665500</td>\n","    </tr>\n","    <tr>\n","      <td>275</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>276</td>\n","      <td>2.121500</td>\n","    </tr>\n","    <tr>\n","      <td>277</td>\n","      <td>1.139300</td>\n","    </tr>\n","    <tr>\n","      <td>278</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>279</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>281</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>282</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>283</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>284</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>285</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>286</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>287</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>288</td>\n","      <td>5.740100</td>\n","    </tr>\n","    <tr>\n","      <td>289</td>\n","      <td>0.611100</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>0.734100</td>\n","    </tr>\n","    <tr>\n","      <td>291</td>\n","      <td>0.604500</td>\n","    </tr>\n","    <tr>\n","      <td>292</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>293</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>294</td>\n","      <td>0.542100</td>\n","    </tr>\n","    <tr>\n","      <td>295</td>\n","      <td>0.185600</td>\n","    </tr>\n","    <tr>\n","      <td>296</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>297</td>\n","      <td>3.255500</td>\n","    </tr>\n","    <tr>\n","      <td>298</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>299</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>301</td>\n","      <td>1.204200</td>\n","    </tr>\n","    <tr>\n","      <td>302</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>303</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>304</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>305</td>\n","      <td>0.744800</td>\n","    </tr>\n","    <tr>\n","      <td>306</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# args = TrainingArguments(\n","#     output_dir=\"./llama-7b-int4-dolly\",\n","#     num_train_epochs=20,\n","#     max_steps=2000,\n","#     fp16=False,\n","#     tf32=False,\n","#     optim=\"paged_adamw_8bit\",\n","#     learning_rate=2e-4,\n","#     lr_scheduler_type=\"constant\",\n","#     per_device_train_batch_size=micro_batch_size,\n","#     gradient_accumulation_steps=gradient_accumulation_steps,\n","#     gradient_checkpointing=True,\n","#     ddp_find_unused_parameters=False if ddp else None,\n","#     group_by_length=False,\n","#     logging_steps=10,\n","#     save_strategy=\"epoch\",\n","#     save_total_limit=3,\n","#     # report_to=\"wandb\",\n","#     # run_name=\"llma_run_00\",\n","#     disable_tqdm=False,\n","# )\n","\n","# trainer = Trainer(\n","#     model=model,\n","#     train_dataset=train_data,\n","#     eval_dataset=val_data,\n","#     args=args,\n","#     data_collator=data_collator,\n","# )\n","\n","trainer = Trainer(\n","    model=model,\n","    train_dataset=train_data,\n","    args=TrainingArguments(\n","        per_device_train_batch_size=1,\n","        gradient_accumulation_steps=4,\n","        warmup_steps=10,\n","        max_steps=1000,\n","        learning_rate=2e-4,\n","        fp16=True,\n","        logging_steps=1,\n","        output_dir=\"./llama-7b-int4-dolly\",\n","        optim=\"paged_adamw_8bit\"\n","    ),\n","    data_collator=data_collator,\n",")\n","# silence the warnings. re-enable for inference!\n","model.config.use_cache = False\n","trainer.train()\n","model.save_pretrained(\"llama-7b-int4-dolly_1\")\n","\n","# export CUDA_VISIBLE_DEVICES=0,1,2,3\n","# export WORLD_SIZE=4 (gpu num)\n","# export LOCAL_RANK=3"]},{"cell_type":"markdown","metadata":{"id":"S5OYiJMVGa_q"},"source":["# 4) Generation\n","\n","```python\n","# standard prompt for llama\n","prompt = f'''\\\n","    [INST] <>\\n{system_message}\\n<>\\n\\nWrite a function that reverses a string. \\\n","    [/INST]\" # replace the command here with something relevant to your task \\\n","    '''\n","```\n","\n","```python\n","def build_llama2_prompt(messages):\n","    startPrompt = \"<s>[INST] \"\n","    endPrompt = \" [/INST]\"\n","    conversation = []\n","    for index, message in enumerate(messages):\n","        if message[\"role\"] == \"system\" and index == 0:\n","            conversation.append(f\"<<SYS>>\\n{message['content']}\\n<</SYS>>\\n\\n\")\n","        elif message[\"role\"] == \"user\":\n","            conversation.append(message[\"content\"].strip())\n","        else:\n","            conversation.append(f\" [/INST] {message.content}</s><s>[INST] \")\n","\n","    return startPrompt + \"\".join(conversation) + endPrompt\n","\n","messages = [\n","  {\n","    \"role\": \"system\",\n","    \"content\": '''You are a friendly and knowledgeable vacation planning assistant named Clara. \\\n","    Your goal is to have natural conversations with users to help them plan their perfect vacation. '''}\n","]\n","\n","instruction = \"What are some cool ideas to do in the summer?\"\n","messages.append({\"role\": \"user\", \"content\": instruction})\n","prompt = build_llama2_prompt(messages)\n","chat = llm.predict({\"inputs\":prompt})\n","print(chat[0][\"generated_text\"][len(prompt):])\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NXOXyxLDGcmh"},"outputs":[],"source":["# model path and weight\n","model_id = \"NousResearch/Llama-2-7b-hf\"\n","peft_path = \"llama-7b-int4-dolly_1\"\n","\n","# loading model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    quantization_config=bnb_config,\n","    use_cache=False,\n","    device_map=\"auto\"\n",")\n","model = PeftModel.from_pretrained(\n","    model,\n","    peft_path,\n","    torch_dtype=torch.float16,\n",")\n","model.config.pretraining_tp = 1\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","model.eval()\n","\n","# generation config\n","generation_config = GenerationConfig(\n","    temperature=0.1,\n","    top_p=0.75,\n","    top_k=40,\n","    num_beams=4, # beam search\n",")\n","\n","with torch.no_grad():\n","    prompt = \"Write me a poem about Singapore.\"\n","    inputs = tokenizer(prompt, return_tensors=\"pt\")\n","    generation_output = model.generate(\n","        input_ids=inputs.input_ids,\n","        generation_config=generation_config,\n","        return_dict_in_generate=True,\n","        output_scores=True,\n","        max_new_tokens=64,\n","    )\n","    print('\\nAnswer: ', tokenizer.decode(generation_output.sequences[0]))"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"jupytext":{"encoding":"# -*- coding: utf-8 -*-","formats":"ipynb,py:percent"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0f0476f8d0234da6ad02acce1f6251ec":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1018c4cbfde54113a2bf41957cb945d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1e1d4ed6192042d49333abf4bc55fa66":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2c15bcdca52b46039c6d3b7b5f6f00ea":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d4e6169b6784fecb3d1d430c6c07fdd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30e6cde0f5fc4382994eb01a0773e3c9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ef32de53c2874c81895296259214006b","IPY_MODEL_c4e8dc41896441bb92b5b2db7590bdc2","IPY_MODEL_86615f4c53b84054b5fef41718d1f946"],"layout":"IPY_MODEL_d11480a43c1c4217ba26db7e64445aed"}},"311ad81bc1204beabe2ea7938bf3fbb5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"377ce664ff6b4b6f966505a4e1f9ba51":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c6359bf23544dfa8f5a21bcfd7dd1d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"40b979c61efe44cdb6fc6038ea5cdc51":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"42dbaef0555545fbab3cfd2b8d7a950e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d4e6169b6784fecb3d1d430c6c07fdd","placeholder":"â€‹","style":"IPY_MODEL_1e1d4ed6192042d49333abf4bc55fa66","value":" 14011/14011 [00:27&lt;00:00, 591.71 examples/s]"}},"4ab93ffb178c497e9453d87440125c30":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b82738a627e9476a935740a38402295d","placeholder":"â€‹","style":"IPY_MODEL_60150903d2524a9e8bdf57eedbf2fd66","value":"Map: 100%"}},"5ae0b67e33ba483e9438ef6a0dc56399":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"60150903d2524a9e8bdf57eedbf2fd66":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6e8f94aacad84a648bc2b65fe563fbbf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c15bcdca52b46039c6d3b7b5f6f00ea","max":14011,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3c6359bf23544dfa8f5a21bcfd7dd1d9","value":14011}},"7572399cc37e4ce3b91dd188dd746502":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf40fbbf0e724324adffec3776bf5ce0","max":14011,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1018c4cbfde54113a2bf41957cb945d3","value":14011}},"7885d0904959451b82fed0611a58ddb7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"84e711a7d5914ba68822bbca9a4bc1ee":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86615f4c53b84054b5fef41718d1f946":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f0476f8d0234da6ad02acce1f6251ec","placeholder":"â€‹","style":"IPY_MODEL_5ae0b67e33ba483e9438ef6a0dc56399","value":" 1000/1000 [00:02&lt;00:00, 421.03 examples/s]"}},"8743b3489097443dbb5417cb96370847":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"954651a255c64fd893324585178d61ad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9a6686236a674d279bd646f2bf9b38f1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f243c84fba34e0d97bc3e886cde95aa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a6686236a674d279bd646f2bf9b38f1","placeholder":"â€‹","style":"IPY_MODEL_b6de5e70d92d4f41892670a5489e8de8","value":" 1000/1000 [00:00&lt;00:00, 3855.04 examples/s]"}},"a1e423520fb844df90b117c6678d1e11":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6d874ab2b0b4194aca123ea8c65f414","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb96c5466862419c9a6d60b0941efc9e","value":1000}},"a7ad4cdb21474e97b704242cf7ed3f51":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ffcfcc6de1fa4a87a35ad905f0e83225","placeholder":"â€‹","style":"IPY_MODEL_40b979c61efe44cdb6fc6038ea5cdc51","value":"Filter: 100%"}},"a9ad99f264fe4fd5b806ddb6b4c375da":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_377ce664ff6b4b6f966505a4e1f9ba51","placeholder":"â€‹","style":"IPY_MODEL_d6de7c7074aa494186f5467da3443173","value":" 14011/14011 [00:03&lt;00:00, 2837.09 examples/s]"}},"ac6ebdb8e0ba45528817c6123dcb3eec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4ab93ffb178c497e9453d87440125c30","IPY_MODEL_6e8f94aacad84a648bc2b65fe563fbbf","IPY_MODEL_42dbaef0555545fbab3cfd2b8d7a950e"],"layout":"IPY_MODEL_cea51bd4c09f41019a3972bee04e9c8a"}},"b6de5e70d92d4f41892670a5489e8de8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b82738a627e9476a935740a38402295d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf40fbbf0e724324adffec3776bf5ce0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1d9cbee75b1429da089b991555df344":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4e8dc41896441bb92b5b2db7590bdc2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_84e711a7d5914ba68822bbca9a4bc1ee","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d9550bd7bfab4369ac9cd4fe1298582b","value":1000}},"cea51bd4c09f41019a3972bee04e9c8a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d11480a43c1c4217ba26db7e64445aed":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6de7c7074aa494186f5467da3443173":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d9550bd7bfab4369ac9cd4fe1298582b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e4a6f2221a93475bad9ac7ebdd65ee91":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee7d2e5be90c47db939b6eafb5c249e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a7ad4cdb21474e97b704242cf7ed3f51","IPY_MODEL_7572399cc37e4ce3b91dd188dd746502","IPY_MODEL_a9ad99f264fe4fd5b806ddb6b4c375da"],"layout":"IPY_MODEL_e4a6f2221a93475bad9ac7ebdd65ee91"}},"ef32de53c2874c81895296259214006b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1d9cbee75b1429da089b991555df344","placeholder":"â€‹","style":"IPY_MODEL_7885d0904959451b82fed0611a58ddb7","value":"Map: 100%"}},"f6d874ab2b0b4194aca123ea8c65f414":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb4d9b7123c14c1f82680f446c2c9b0b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fc99349736364bcda836c75b45c4e510","IPY_MODEL_a1e423520fb844df90b117c6678d1e11","IPY_MODEL_9f243c84fba34e0d97bc3e886cde95aa"],"layout":"IPY_MODEL_311ad81bc1204beabe2ea7938bf3fbb5"}},"fb96c5466862419c9a6d60b0941efc9e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fc99349736364bcda836c75b45c4e510":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8743b3489097443dbb5417cb96370847","placeholder":"â€‹","style":"IPY_MODEL_954651a255c64fd893324585178d61ad","value":"Filter: 100%"}},"ffcfcc6de1fa4a87a35ad905f0e83225":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26ad8ffcf92548f4a269fba254279516":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0784e9eecfb44f668ff2c4b9480275e5","IPY_MODEL_65d08ef5fa6245718a760cd0713c1eef","IPY_MODEL_d1e97d72cf7b4e8eb28350efd077c2e4"],"layout":"IPY_MODEL_83df6dc432a7403b8ad48d0fe78e7954"}},"0784e9eecfb44f668ff2c4b9480275e5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_465a4c026f484310baca06d5dd1df7b9","placeholder":"â€‹","style":"IPY_MODEL_53d5d3c50bef4bd7b1e849caf1f19427","value":"Loading checkpoint shards: 100%"}},"65d08ef5fa6245718a760cd0713c1eef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8aaffadad0bb49169f91d53b61e6c560","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_de22628b7a6f4b28a62121f0c384ac61","value":2}},"d1e97d72cf7b4e8eb28350efd077c2e4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_21a989168dae462ba8d629dca3e43293","placeholder":"â€‹","style":"IPY_MODEL_f03faf7671c14a47bf290bd460603196","value":" 2/2 [01:12&lt;00:00, 33.09s/it]"}},"83df6dc432a7403b8ad48d0fe78e7954":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"465a4c026f484310baca06d5dd1df7b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53d5d3c50bef4bd7b1e849caf1f19427":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8aaffadad0bb49169f91d53b61e6c560":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de22628b7a6f4b28a62121f0c384ac61":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"21a989168dae462ba8d629dca3e43293":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f03faf7671c14a47bf290bd460603196":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}